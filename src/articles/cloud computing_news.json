[
    {
        "title": "Microsoft is taking its foot off the AI accelerator. What does that mean?",
        "description": "Microsoft's decision to pace data center growth indicates a shift from AI training to inference, aligning with the industry's embrace of efficiency.",
        "url": "https://www.businessinsider.com/microsoft-ai-data-center-cloud-decreased-spending-explained-why-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-14T09:02:01Z",
        "full_text": "This story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nIn the high-stakes race to dominate AI infrastructure, a tech giant has subtly shifted gears.\n\nSince ChatGPT burst onto the scene in late 2022, there's been a mad dash to build as many artificial intelligence data centers as possible. Big Tech is spending hundreds of billions of dollars on land, construction, and computing gear to support new generative-AI workloads.\n\nMicrosoft has been at the forefront of this, mostly through its partnership with OpenAI, the creator of ChatGPT.\n\nFor two years, there's been almost zero doubt in the tech industry about this AI expansion. It's been all very up and to the right.\n\nUntil recently, that is.\n\nPacing plans\n\nOn Tuesday, Noelle Walsh, the head of Microsoft cloud operations, said the company \"may strategically pace our plans.\"\n\nThis is pretty shocking news for an AI industry that's been constantly kicking and screaming for more cloud capacity and more Nvidia graphics processing units. So it's worth reading closely what Walsh wrote about how things have changed.\n\n\"In recent years, demand for our cloud and AI services grew more than we could have ever anticipated and to meet this opportunity, we began executing the largest and most ambitious infrastructure scaling project in our history,\" she wrote in a LinkedIn post.\n\n\"By nature, any significant new endeavor at this size and scale requires agility and refinement as we learn and grow with our customers. What this means is that we are slowing or pausing some early-stage projects,\" Walsh added.\n\nMicrosoft has backed off a bit lately\n\nShe didn't share more details, but the TD Cowen analyst Michael Elias has found several recent examples of what he said was Microsoft backing off.\n\nRelated stories Business Insider tells the innovative stories you want to know Business Insider tells the innovative stories you want to know\n\nThe tech giant has walked away from more than 2 gigawatts of AI cloud capacity in the US and Europe in the past six months, which was in the process of being leased, he said. In the past month or so, Microsoft has also deferred and canceled data center leases in the US and Europe, Elias wrote in a recent note to investors.\n\nThis pullback on new capacity leasing was largely driven by Microsoft's decision to not support incremental OpenAI training workloads, Elias said. A recent change to this crucial partnership allows OpenAI to work with cloud providers beyond Microsoft.\n\n\"However, we continue to believe the lease cancellations and deferrals of capacity point to data center oversupply relative to its current demand forecast,\" Elias added.\n\nThis is worrying because trillions of dollars in current and planned investments are riding on the generative-AI boom. With so much money on the line, any inkling that this rocket ship is not ascending at light speed is unnerving. (I asked a Microsoft spokesperson all about this twice and didn't get a response.)\n\nAn AI recalibration, not a retreat\n\nThe reality is more nuanced than a simple pullback, though. What we're witnessing is a recalibration — not a retreat.\n\nRaimo Lenschow, a Barclays analyst, put the situation in context. The initial wave of this industry spending spree focused a lot on securing land and buildings to house all the chips and other computing gear needed to create and run AI models and services.\n\nIn this AI \"land grab,\" it's common for large cloud companies to sign and negotiate leases that they end up walking away from later, Lenschow said.\n\nNow that Microsoft feels more comfortable with the amount of land it has on hand, the company is likely shifting some spending to the later stages that focus more on buying the GPUs and other computing gear that go inside these data centers.\n\n\"In other words, over the past few quarters, Microsoft has 'overspent' on land and buildings, but is now going back to a more normal cadence,\" Lenschow wrote in a recent note to investors.\n\nMicrosoft still plans $80 billion in capital expenditures during its 2025 fiscal year and has guided for year-over-year growth in the next fiscal year. So the company probably isn't backing away from AI much but is becoming more strategic about where and how it invests.\n\nFrom AI training to inference\n\nPart of the shift appears to be from AI training to inference. Pretraining is how models are created, and this requires loads of closely connected GPUs, along with state-of-the-art networking. Expensive stuff! Inference is how existing models are run to support services such as AI agents and Microsoft Copilot. Inference is less technically demanding but expected to be the larger market.\n\nWith inference outpacing training, the focus is shifting toward scalable, cost-effective infrastructure that maximizes return on investment.\n\nFor instance, at a recent AI conference in New York, the discussion was focused more on efficiency than attaining artificial general intelligence, a costly endeavor to make machines work better than humans.\n\nThe AI startup Cohere said that its new Command A model needs only two GPUs to run. That's a heck of a lot fewer than most models have required in recent years.\n\nMicrosoft's AI chief weighs in\n\nMustafa Suleyman, the CEO of Microsoft AI, echoed this in a recent podcast. While he acknowledged a slight slowdown in returns from massive pretraining runs, he said the company's compute consumption was still \"unbelievable\" — it's just shifting to different stages of the AI pipeline.\n\nSuleyman added that some of the canceled leases and projects were never finalized contracts but exploratory discussions — part of standard operating procedure in hyperscale cloud planning.\n\nThis strategic pivot comes as OpenAI, Microsoft's close partner, has begun sourcing capacity from other cloud providers and even hinting at developing its own data centers. Microsoft, however, maintains a right of first refusal on new OpenAI capacity, signaling continued deep integration between the two companies.\n\nWhat does this all mean?\n\nFirst, don't mistake agility for weakness. Microsoft is likely adjusting to changing market dynamics, not scaling back ambition. Second, the hyperscaler space remains incredibly competitive.\n\nWhen Microsoft walked away from capacity in overseas markets, Elias said, Google stepped in to snap up the supply. Meanwhile, Meta backfilled the capacity that Microsoft left on the table in the US.\n\n\"Both of these hyperscalers are in the midst of a material year-over-year ramp in data center demand,\" Elias wrote, referring to Google and Meta.\n\nSo Microsoft's pivot may be more a sign of maturity than retreat. As AI adoption enters its next phase, the winners won't necessarily be those who spend the most — but those who spend the smartest."
    },
    {
        "title": "Dashlane brings confidential computing to passkeys for better business security",
        "description": "The move to passkeys is happening in this way: gradually, then suddenly. Apple has gone all in on passkeys, and while they’re a huge win for consumer security, they’ve been harder to manage in an enterprise environment. That’s starting to change.\n\n\n\n more…",
        "url": "https://9to5mac.com/2025/04/29/dashlane-brings-confidential-computing-to-passkeys-for-better-business-security/",
        "source": "9to5Mac",
        "publishedAt": "2025-04-30T00:00:00Z",
        "full_text": "The move to passkeys is happening in this way: gradually, then suddenly. Apple has gone all in on passkeys, and while they’re a huge win for consumer security, they’ve been harder to manage in an enterprise environment. That’s starting to change.\n\nSome of my favorite gear Aqara Smart Lock U50 Upgrade your doors with Apple Home Key and the Aqara U50.\n\nAhead of World Password Day, Dashlane announced it’s now using confidential computing to secure synced passkeys in the cloud. Instead of storing the private key in client software memory, it’s now housed in AWS Nitro Enclaves. The result is stronger protection and more control for IT teams without giving up on a great end-user experience.\n\n“There’s a lot I could say about passkeys as a technology, but I fundamentally believe that they will create a world where account takeovers are almost eliminated.” —John Bennett, Chief Executive Officer, Dashlane\n\nFor companies managing shared accounts or sensitive access, like cloud storage logins or single-sign-on admin portals, this update unlocks new options. Admins can now build stricter policies around who can share credentials, where they can be used, and how access is revoked.\n\nWhat makes this move especially interesting is that Dashlane is not giving up on it its zero knowledge security approach. Passkeys do remain encrypted in the cloud, and only the end user can decrypt them. The secure enclave handles the authentication handshake while keeping the private key secure.\n\nThere has always been a tradeoff and tension between security keys and usability. Hardware keys are rock solid but a pain to manage at scale – especially for worldwide. companies. Software-based synced passkeys are easier to use but not quite as bulletproof from a security point of view . With this update, Dashlane aims to narrow that gap. Businesses get the convenience users expect and the security teams need.\n\nSome of my favorite gear Abode Home Security System Abode is the best home security system and includes compatibility with HomeKit.\n\nPasskeys join SSO and SIEM as part of Dashlane’s confidential computing focus. For Apple-focused IT teams looking to clean up credential management sprawl without sacrificing a great user experience, this is a step in the right direction. Passkeys, in my opinion, continue to be one of the most important technologies in cybersecurity, and the faster the world migrations fully to it, the better. It’s rare than major technology vendors will collaborate on open standards, and passkeys is one of those times."
    },
    {
        "title": "AWS preps staff for customer concerns over tariffs, data risks, and possible foreign cloud restrictions by Trump",
        "description": "Amazon's cloud business seems less exposed to tariffs than its e-commerce operation. And yet, AWS is advising staff on potential customer questions.",
        "url": "https://www.businessinsider.com/aws-preps-staff-customer-questions-tariffs-trump-cloud-restrictions-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-30T08:35:02Z",
        "full_text": "lighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nAs tariffs spark growing uncertainty across Amazon's retail operations, the company's cloud division is quietly moving to head off similar concerns from business customers.\n\nAccording to an internal document obtained by Business Insider, Amazon Web Services has issued new guidance to frontline sales and technical staff, instructing them on how to respond to customer questions about tariffs, data sovereignty, and potential restrictions tied to US government policy.\n\nAmong the talking points: If an AWS customer asks about possible price increases due to tariffs, employees are told to avoid direct answers and instead reaffirm pricing terms for those covered under existing Private Pricing Agreements (PPAs).\n\n\"In the event that AWS does increase prices, these increases will not change any agreed upon discounts, credits or service-specific rates in your PPA,\" the internal document stated.\n\nWhile AWS may be less directly impacted by tariffs than Amazon's e-commerce business, the document reveals the company is concerned enough to prep staff with answers to potential tough customer questions.\n\nThe document covers questions ranging from potential price hikes and data-privacy concerns. It even broaches the possibility that US President Donald Trump might ban foreign companies from using AWS.\n\nIn a recent CNBC interview, Amazon CEO Andy Jassy acknowledged the situation remains fluid and emphasized efforts by the company's e-commerce business to keep consumer prices low. Still, he hinted that some third-party sellers might raise prices in response to tariffs. He also noted that, despite the uncertainty, Amazon continues its data center expansion.\n\nAmazon, whose stock has dropped about 15% this year, is set to report first-quarter earnings on Thursday.\n\nA spokesperson for the company referred BI to a statement from the internal document:\n\n\"We're closely monitoring the situation, and we are working to assess the impact on our business. As we navigate the evolving trade policy landscape, our focus remains on delivering value to our customers and innovating on their behalf.\"\n\nDo not 'speculate'\n\nTariff-driven price hikes have already become a flashpoint in Amazon's retail division.\n\nAs BI previously reported, internal teams have struggled with forecasting, and vendors say Amazon has offered cost relief in exchange for strict margin guarantees. Meanwhile, third-party sellers say they're being forced to raise prices due to rising import costs.\n\nAWS CEO Matt Garman Amazon\n\nWhat this means for AWS pricing remains unclear. Internal guidance tells employees not to \"speculate,\" citing the rapidly evolving nature of trade policy.\n\nRelated stories Business Insider tells the innovative stories you want to know Business Insider tells the innovative stories you want to know\n\nSome cloud industry experts suggest tariffs could squeeze AWS more than the company lets on.\n\nAWS relies heavily on high-end computing gear, much of it manufactured in China or Taiwan. While some semiconductor components were recently exempted from tariffs, other critical data center parts may still be affected. Trump has paused most new tariffs for 90 days, but a 145% tariff on Chinese goods remains in effect.\n\n\"AWS and other hyperscalers could choose to absorb the cost or pass it on to customers,\" said Travis Rehl, CTO of cloud consultancy Innovative Solutions. \"I'm unsure which direction they'd take.\"\n\nBen Schaechter, CEO of cloud cost optimization firm Vantage, said tariffs could force AWS to tighten future discounts or slow infrastructure growth due to higher hardware costs.\n\nThe bigger threat, some say, is reduced cloud spending.\n\nRandall Hunt, CTO of cloud advisory firm Caylent, told BI that customers are already cutting back in broad spending in anticipation of slower growth and rising costs.\n\nData sovereignty and Trump-era fears\n\nThe growing uncertainty over Trump's actions has pushed Amazon to prepare for even more extreme scenarios, including potential US government demands for cloud customer data or a move to block non-US users from accessing AWS.\n\nThose concerns over privacy and data access have grown recently as Trump's tariff-driven trade war increased tensions between the US and European countries.\n\nIf asked about potential US government data requests, Amazon instructed employees to emphasize that AWS does not disclose customer information unless legally required and that all requests are thoroughly reviewed.\n\nThe guidance also clarifies AWS's position on the CLOUD Act, or Clarifying Lawful Overseas Use of Data Act. The CLOUD Act, passed in 2018, gives US law enforcement agencies the authority to access data held by US-based companies, even if stored abroad.\n\nAWS has not provided enterprise or government customer data stored outside the US since at least 2020 and it will challenge any \"over-broad\" or unlawful requests, the document stated.\n\n\"The CLOUD Act does not provide the U.S. government with unfettered access to data held by cloud providers,\" the document added.\n\nPresident Donald Trump. Chip Somodevilla/Getty Images\n\nOn the question of whether Trump could block foreign access to AWS, the document stops short of addressing whether the president has the authority, but notes there's no indication such action is imminent.\n\nIn fact, it argues that doing so would contradict the administration's stated goal of supporting US tech companies abroad.\n\n\"AWS is closely plugged into US policy and this Administration's efforts, and can confirm we have heard nothing about restricting cloud services to non-US customers in response to addressing trade imbalances or unfair trade barriers, and expect their focus to continue to be on tariffs as the 'rebalancing' mechanism,\" the document said.\n\nSanctions and 'Buy Canada'\n\nAWS also addresses fears that US sanctions could restrict access to its services in certain countries. The guidance notes that full country-wide sanctions are rare and that in the past, companies have been given time to wind down operations when sanctions do occur.\n\n\"US country-wide sanctions or services restrictions are exceedingly rare,\" the document said. \"But in the theoretical case that such sanctions ever came to pass, AWS would do everything practically possible to provide continuity of service.\"\n\nFinally, AWS is preparing for patriotic backlash in some markets, such as a potential \"Buy Canada\" movement. Employees are told to clarify that AWS's Canada office is a registered Canadian corporation headquartered in Toronto, and that customers can choose to store their data locally and encrypt it.\n\nStill, the guidance urges caution. Employees should be careful framing AWS as a \"Canadian business,\" given the complexity of the term.\n\n\"Whether AWS is a 'Canadian business' will depend on how that is defined in particular circumstances,\" the document concludes."
    },
    {
        "title": "NASCAR fraudster pleads guilty to tricking his hedge fund investors, scuttling Monday jury trial",
        "description": "Racing superfan and team owner Andrew Franzone waved the red flag on a trial set to start Monday in New York.",
        "url": "https://www.businessinsider.com/nascar-fraudster-pleads-guilty-in-hedge-fund-scam-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-14T16:28:28Z",
        "full_text": "lighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nFormer Miami hedge fund founder and NASCAR team owner Andrew Franzone has pleaded guilty to federal securities and wire fraud, scuttling a jury trial that had promised to play out against a racing backdrop.\n\nFranzone's plea was made public hours before jury selection was to begin in a Manhattan courtroom on Monday.\n\nSentencing is set for July 15. He faces up to 20 years in prison, though a lesser sentence is likely.\n\nProsecutors had said Franzone tricked more than 100 victims — including fellow drivers and racing fans — into investing $40 million in his Miami-based hedge fund.\n\nThe NASCAR superfan and vintage race car collector lied about his fund's liquidity and performance, assuring investors their money was in high-performing but safe stocks while actually locking up their cash in risky ventures.\n\nFranzone had hoped to argue at trial that his picks turned out to be extremely successful — particularly a $250,000 investment in CoreWeave from 2019 that grew to a worth exceeding $55 million.\n\nA judge's decision on Thursday barred that line of defense, while also allowing prosecutors to argue that Franzone used the NASCAR community to find victims and indulge his expensive racing hobby.\n\nVictim investors included Franzone's own race team partner, champion NASCAR truck series racer Mike \"The Gunslinger\" Skinner, according to court documents.\n\nIt was Thursday's decision barring jurors from learning of the fund's eventual gangbuster success that prompted Franzone to wave the red flag on going to trial, his lawyer, Joseph R. Corozzo, told Business Insider on Monday.\n\n\"The judge's decisions were limiting a great deal of the evidence,\" Corozzo explained of the decision to take a plea.\n\nThe success of CoreWeave, a Nvidia-backed cloud computing startup, will now be an issue at sentencing, as a victim restitution plan is ironed out, he said.\n\n\"Everybody's going to be made whole, maybe as much as 10 times over,\" Corozzo said of investors.\n\nFranzone had been trying to buy his fund out of bankruptcy in 2021, and to make investors whole using the CoreWeave windfall, when he was arrested in Fort Lauderdale, Corozzo said.\n\n\"He was right about his investments in the end,\" the lawyer added. \"That's always been his reputation — quirky, but brilliant.\""
    },
    {
        "title": "AI slowdown? Big Tech earnings are telling a different story.",
        "description": "Microsoft and Meta shrugged off concerns about their big AI investments by showing data center demand remains strong and increasing capex forecasts.",
        "url": "https://www.businessinsider.com/big-tech-earnings-ai-slowdown-microsoft-meta-google-data-centers-2025-5",
        "source": "Business Insider",
        "publishedAt": "2025-05-01T13:46:14Z",
        "full_text": "Mark Zuckerberg and Satya Nadella are all in on AI.\n\nMark Zuckerberg and Satya Nadella are all in on AI. AP Photo/Jeff Chiu\n\nlighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nIs there a let-up in the AI mania? Big Tech doesn't seem to think so.\n\nMicrosoft and Meta this week took turns explaining why they see no sign of weakening interest in the AI they've bet their futures on, reporting earnings that appeared to shrug off concerns over recent analyst notes suggesting wobbles in demand.\n\nMeta said it was updating its guidance on capital expenditure for the year to $64-72 billion, up from $60-65 billion.\n\nMeanwhile, Microsoft said its capital expenditure was up from $14 billion in the same quarter last year to $21.4 billion in the most recent quarter.\n\nThese are both signs that the two companies are willing to spend more and more on the critical infrastructure needed to host the AI services in high demand from customers.\n\nMicrosoft posted cloud unit revenue of $42.4 billion in the first quarter of 2025, beating analyst expectations. Revenue in the unit accounting for the AI data center services it provides to customers jumped 20% year over year.\n\nA Jefferies analyst note, published Thursday, said that \"AI demand is trending higher than expected\" for Microsoft, pointing to the soaring number of AI tokens the company processed in its third financial quarter.\n\nThe updates appeared to have done enough to convince investors that the AI boom is staying strong — Microsoft shares opened up more than 9% on Thursday, while Meta was up about 5% — despite recent reports suggesting a slowdown in demand for Big Tech data centers.\n\nLast week, analysts at Wells Fargo published a report claiming that AI data center giant Amazon had paused some of its data center leasing discussions. In response, an Amazon Web Services executive said there was still \"strong demand\" to provide the technology underpinning the AI boom.\n\nMicrosoft CEO Satya Nadella addressed anxieties about data center pauses in a call with investors on Wednesday, as his company had also been the subject of a report earlier this year claiming that it was canceling lease commitments.\n\nDuring the call, he said he felt \"very, very good\" about the pace at which his company's data center expansion was taking place.\n\nMicrosoft's chief financial officer, Amy Hood, said the company had a \"customer contracted backlog of $315 billion\" for server technology like graphics processing units, or GPUs.\n\nGoogle got the ball rolling\n\nGoogle helped set the stage for both Microsoft and Meta when it reported earnings last week, showing a 28% year-on-year jump in first-quarter revenue in its AI-focused cloud computing unit to $12.3 billion. Capital expenditure also rose from $12 billion year-on-year to $17.2 billion.\n\nRelated stories Business Insider tells the innovative stories you want to know Business Insider tells the innovative stories you want to know\n\nAll these suggestions that the AI hype train shows no sign of slowing come with a few asterisks, however.\n\nMeta's increase in full-year guidance on AI infrastructure spending? Some of that is down to rising costs against a backdrop of tariffs, according to Meta's chief financial officer, Susan Li, who offered a tone of caution to investors on Wednesday.\n\n\"The higher cost we expect to incur for infrastructure hardware this year really comes from suppliers who source from countries around the world, and there's just a lot of uncertainty around this given the ongoing trade discussions,\" she said.\n\nBoth Google and Microsoft, meanwhile, have shown that while revenue in their cloud units has hit higher and higher targets, there has been a slight decrease in the revenue growth rates at their cloud units over the past two quarters.\n\nSome of this may result from cyclical trends; data centre revenue can sometimes fluctuate throughout the year as companies manage waves of demand.\n\nThe other caveat is that the likes of Amazon, Apple, and Nvidia could shift the AI narrative when they report earnings this month.\n\nFor now, Big Tech is clear. Demand is still coming for AI, and so too is Big Tech's spending."
    },
    {
        "title": "What Big Tech CEOs are saying about their massive AI spending plans",
        "description": "Big Tech companies including Meta, Microsoft, Alphabet, and Amazon plan to spend more than $300 billion this year, much of which will be on AI.",
        "url": "https://www.businessinsider.com/big-tech-massive-ai-spending-spree-quarterly-earnings-reports-calls-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-05-02T17:59:54Z",
        "full_text": "lighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nA slew of Big Tech companies reported quarterly earnings this week, and with those earnings, some progress reports on the tens of billions of dollars they're funneling into AI.\n\nSome Big Tech companies are showing signs of reining in AI spending while others are plowing full steam ahead.\n\nCumulatively, Meta, Microsoft, Alphabet, and Amazon plan to spend more than $300 billion this year, much of which will be on AI. Apple has also said it plans to spend $500 billion over the next four years.\n\nHere's a look at where some of the biggest players stand:\n\nGoogle\n\nGoogle parent company Alphabet estimates $75 billion in capital expenditures for 2025, largely for data centers and server capacity for AI. This is well above the consensus estimate of $57.9 billion and represents an increase of about 43% year-over-year.\n\n\"We are confident about the opportunities ahead, and to accelerate our progress, we expect to invest approximately $75 billion in capital expenditures in 2025,\" CEO Sundar Pichai said in a February earnings release.\n\n\"We expect to increase our investments in capital expenditure for technical infrastructure, primarily for servers, followed by data centers and networking,\" CFO Anat Ashkenazi said during the company's earnings call that month.\n\nFirst-quarter capex was $17.2 billion for the company, which is breaking ground for several new data centers to support its AI work, including Google Search AI overview and the Gemini chatbot.\n\nAmazon\n\nAmazon previously said it expects increased capital expenditures this year of $100 billion, largely for AI, particularly for the company's AWS cloud computing division.\n\nIn Amazon's Q1 earnings Thursday, the company reported capex of $24.3 billion, up more than 70% year-over-year.\n\nCEO Andy Jassy added AWS has seen an \"explosion of coding agents.\"\n\nJassy said on a third-quarter earnings call last year that the massive investment was justified because AI is \"a really unusually large, maybe once-in-a-lifetime type of opportunity.\"\n\n\"I think that both our business, our customers and shareholders will be happy, medium to long-term, that we're pursuing the capital opportunity and the business opportunity in AI,\" he said.\n\nMeta\n\nIn its recent Q1 earnings, Meta raised its full-year capex estimate from a range of $60 to $65 billion to $64 to $72 billion now.\n\nThe change \"reflects additional data center investments to support our artificial intelligence efforts as well as an increase in the expected cost of infrastructure hardware,\" the company said in its earnings report.\n\nThis is a marked increase from the company's 2024 capex of $39.23 billion.\n\nCEO Mark Zuckerberg kicked off his remarks in Meta's Q1 earnings call by talking about AI being \"the major theme\" at Meta right now that's \"transforming everything we do.\"\n\nZuckerberg noted these are \"long-term investments that are downstream from us,\" but said the company \"will be wildly happy with the investments that we are making.\"\n\nMicrosoft\n\nMicrosoft is showing signs it may not remain as bullish on AI spending as some of its peers.\n\nRelated stories Business Insider tells the innovative stories you want to know Business Insider tells the innovative stories you want to know\n\nCFO Amy Hood said on the Q1 earnings call Wednesday that the company expects capex to grow in the coming fiscal year, but noted, \"it will grow at a lower rate than FY 2025 and will include a greater mix of short lived assets, which are more directly correlated to revenue than long lived assets.\"\n\nThe company has said before that it expects capex of $80 billion in fiscal year 2025 in order to \"build out AI-enabled datacenters to train AI models and deploy AI and cloud-based applications around the world.\" More than half of the investment will be in the US.\n\nEarlier this month, Noelle Walsh, the head of Microsoft cloud operations, said the company \"may strategically pace our plans.\"\n\n\"In recent years, demand for our cloud and AI services grew more than we could have ever anticipated and to meet this opportunity, we began executing the largest and most ambitious infrastructure scaling project in our history,\" she wrote in a LinkedIn post.\n\n\"By nature, any significant new endeavor at this size and scale requires agility and refinement as we learn and grow with our customers. What this means is that we are slowing or pausing some early-stage projects,\" she continued.\n\nApple\n\nOn February, Apple announced its biggest spend commitment in the company's history, for $500 billion in the US over four years toward AI initiatives, manufacturing, and silicon engineering, among other expenses.\n\n\"We're going to be expanding our teams and our facilities in several states, including Michigan, Texas, California, Arizona, Nevada, Iowa, Oregon, North Carolina, and Washington,\" CEO Tim Cook said in the company's Q1 earnings call Thursday. \"And we're going to be opening a new factory for advanced server manufacturing in Texas.\""
    },
    {
        "title": "OpenAI is on a hiring spree, and one area of investment stands out",
        "description": "OpenAI is seeking data center talent as it reduces reliance on Microsoft and ramps up its own infrastructure ambitions.",
        "url": "https://www.businessinsider.com/openai-data-center-infrastructure-hiring-push-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-16T09:30:02Z",
        "full_text": "This story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nAs OpenAI lessens its reliance on Microsoft, the AI startup is looking to add workers with data center experience to its ranks.\n\nOf the roughly 300 job listings on OpenAI's website at the time of publication, Business Insider counted more than 20 that appeared directly related to data center development. Many of the open jobs involve building out the digital infrastructure that supports OpenAI's technology as the company uses more computing power.\n\nThe infrastructure hiring push comes as the company starts to operate more independently of Microsoft, which had until recently served as OpenAI's exclusive cloud provider.\n\nOpenAI CEO Sam Altman posted Sunday on X about the company's hiring push.\n\n\"If you are interested in infrastructure and very large-scale computing systems, the scale of what's happening at OpenAI right now is insane and we have very hard/interesting challenges. Please consider joining us! We could desperately use your help,\" Altman wrote.\n\nThe open roles cover various functions in departments across the company, from strategic finance to hardware, security, and scaling. One of the job descriptions mentions Stargate, OpenAI's data center venture with Oracle and SoftBank, among others, that plans to spend up to $500 billion on building AI infrastructure. Most of the data center-related roles are in San Francisco, though there are some openings in Seattle and New York. Only two of the jobs are remote-friendly.\n\nWhen asked for comment on this story, Liz Bourgeois, the head of OpenAI's policy communications, directed BI to Altman's post.\n\n\"In particular, if you have thought about how to squeeze max performance out of a system, we'd love to talk to you,\" Altman said on X.\n\nIn October, Microsoft disclosed in a Securities and Exchange Commission filing that it had invested $13 billion in OpenAI. As of its last funding round from investors, including SoftBank, the startup was valued at $300 billion.\n\nOpenAI has been using those funds to start securing graphics processing unit capacity to power its artificial intelligence models directly from third parties and bringing in talent with data center design, construction, and capacity planning experience, the TD Cowen analyst Michael Elias wrote in March in a research note.\n\nOpenAI's data center hires\n\nSome of the jobs OpenAI is hiring for deal directly with building physical data centers. The Stargate team, for instance, is looking for a site selection and enablement leader to oversee the process of determining where to build facilities, negotiating power and land deals, and navigating state and local government hurdles.\n\nThe company is also hiring for a data center design engineer with experience in power and mechanical systems and a data center lead for physical operations and logistics to oversee on-site security and operations.\n\nBetween November and March, OpenAI hired at least three data center leaders, including a director of infrastructure, an infrastructure construction lead, and a strategy and operations leader. The new employees came from Meta, Equinix, and SandboxAQ, an AI and quantum computing startup that spun out of Alphabet.\n\nEarlier this year, Lane Dilg, who was an advisor in then-President Joe Biden's Department of Energy and had been working in global affairs for OpenAI since 2023, transitioned to a role overseeing infrastructure policy and partnerships, according to LinkedIn.\n\nElias said the hires pointed \"to the potential for OpenAI to begin self-building data centers in the medium to long term.\"\n\nMeanwhile, Microsoft has begun to scale back its data center capacity investments, pausing construction projects and pulling out of leases with third parties."
    },
    {
        "title": "WhatsApp borrowing Apple’s Private Cloud Compute approach to AI privacy",
        "description": "Meta’s encrypted messaging app WhatsApp will use an approach pioneered by Apple to add AI capabilities without compromising privacy.\n\n\n\nThe company has announced that it will use tech it calls Private Processing, which appears to exactly replicate Apple’s Pri…",
        "url": "https://9to5mac.com/2025/04/30/whatsapp-borrowing-apples-private-cloud-compute-approach-to-ai-privacy/",
        "source": "9to5Mac",
        "publishedAt": "2025-04-30T11:35:18Z",
        "full_text": "Meta’s encrypted messaging app WhatsApp will use an approach pioneered by Apple to add AI capabilities without compromising privacy.\n\nThe company has announced that it will use tech it calls Private Processing, which appears to exactly replicate Apple’s Private Cloud Compute …\n\nApple’s Private Cloud Compute\n\nApple takes a two-stage approach to ensuring user privacy for Apple Intelligence features:\n\nAs much processing as possible is done on-device, with no data sent to servers If external processing power is needed, Private Cloud Compute (PCC) servers are used\n\nAny personal data sent to PCC uses end-to-end encryption, so that not even Apple has access to it – but the company goes further than this. It uses an approach known as ‘stateless computation,’ which means that once processing is complete, the personal data is completely wiped from the system. The moment processing is complete, it’s as if it never existed in the first place.\n\nAdditionally, Apple allows anyone to check the security of the approach for themselves, meaning security researchers will be able to verify the company’s claims.\n\nWhatsApp will use Private Processing\n\nConcerns were raised when Meta added an AI chatbot to WhatsApp, with no option to remove it.\n\nSome users are seeing a new Meta AI logo in the chats screen, while others have an ‘Ask Meta AI or Search’ prompt in the search bar. There is currently no way to remove either. Many users are expressing their frustration at what they see as an unwanted intrusion, with Guardian columnist Polly Hudson among those to object. She likened it to the time Apple annoyed everyone by adding a U2 album to their devices.\n\nOne of the capabilities of the WhatsApp AI is to summarize messages, which obviously entails the ability to read them.\n\nThe company has now laid out how it will ensure this is done in a privacy-protecting way.\n\nWe’re excited to share an initial overview of Private Processing, a new technology we’ve built to support people’s needs and aspirations to leverage AI in a secure and privacy-preserving way. This confidential computing infrastructure, built on top of a Trusted Execution Environment (TEE), will make it possible for people to direct AI to process their requests — like summarizing unread WhatsApp threads or getting writing suggestions — in our secure and private cloud environment. In other words, Private Processing will allow users to leverage powerful AI features, while preserving WhatsApp’s core privacy promise, ensuring no one except you and the people you’re talking to can access or share your personal messages, not even Meta or WhatsApp.\n\nIncludes two other key PCC features\n\nLike PCC, Private Processing will use stateless processing.\n\nStateless processing and forward security: Private Processing must not retain access to user messages once the session is complete to ensure that the attacker can not gain access to historical requests or responses.\n\nFinally, Meta will also follow Apple’s lead in allowing anyone to verify its claims.\n\nUsers and security researchers must be able to audit the behavior of Private Processing to independently verify our privacy and security guarantees.\n\nYou can find more details on the company’s engineering blog.\n\n9to5Mac’s Take\n\nWhile there are always grumbles when anyone copies Apple (more so than when Apple copies others), this is an area where nobody should have any complaints. Meta appears to be precisely replicating all of Apple’s safeguards, and that’s to be entirely commended. All tech giants should do the same.\n\nThe verification part is particularly important in Meta’s case. Given the company’s, uh, casual attitude to privacy over a great many years, few would be willing to take it at its word.\n\nHighlighted accessories\n\nImage: Meta"
    },
    {
        "title": "Microsoft shares grow on FY25 Q3 earnings, beating expectations with a 13% increase year-over-year, driven by cloud, gaming, and AI",
        "description": "Microsoft cloud dominates the messaging as Redmond beats Wall Street expectations to post yet another record quarter.",
        "url": "https://www.windowscentral.com/microsoft/microsoft-shares-grow-on-fy25-q3-earnings-beating-expectations-with-a-13-percent-increase",
        "source": "Windows Central",
        "publishedAt": "2025-05-01T06:27:13Z",
        "full_text": "Yesterday evening Microsoft posted its FY25 Q3 results, and it was pretty much a clean slate of growth across the board. Microsoft's shares rose on the news, which grew 1.2+ points in after hours trading, likely to grow further when the markets open fully later today.\n\n\"We delivered a strong quarter with Microsoft Cloud revenue of $42.4 billion, up 20% year-over-year,\" CEO Satya Nadella said, \"driven by continued demand for our differentiated offerings.\"\n\nMicrosoft's revenue hit $70.1 billion in revenue, 13% up year-over-year, with an operating income of $32 billion and a net income of $25.8 billion.\n\nEarnings per share hit $3.46, beating Wall Street estimates.\n\nMicrosoft's Productivity and Businesses revenue grew 10%, driven by Microsoft 365 and Dynamics 365.\n\nIntelligent Cloud, which includes Azure, grew by a massive 21%, with Azure alone growing 33% year-over-year.\n\nMore Personal Computing revenue, which includes Windows, rose 6%.\n\nXbox and gaming grew by 6% overall, with hardware down 6% and content and services growing 8%. PC Game Pass grew by 45% year over year, and Xbox Cloud Gaming hit 150 million hours streamed for the quarter, up 10 million.\n\nMicrosoft returned $9.7 billion to shareholders through dividends and stock buybacks.\n\nMicrosoft was keen to emphasize that global tariffs and geopolitical issues threaten economic certainty may undermine its future profitability in the near term, but remained optimistic on AI and cloud services as key drivers of growth. Microsoft also mentioned opportunities around its quantum computing efforts, fostered by the Majorana-1 breakthrough.\n\nMicrosoft also hailed boosted engagement with services like Copilot and Bing, noting that Copilot usage grew 35% quarter over quarter, with Copilot on Microsoft 365 servicing \"100 million\" users. Bing engagement also increased by 22% apparently, too.\n\nUnfortunately, Microsoft made no mention of Surface itself in this earnings report, although it is thought that new Surface devices under the Copilot+ range are just around the corner."
    },
    {
        "title": "Simulating, Detecting and Responding to S3 Ransomware Attacks",
        "description": "Cloud and Application Security",
        "url": "https://raphabot.com/articles/simulating-detecting-and-responding-s3-ransomware/",
        "source": "Raphabot.com",
        "publishedAt": "2025-05-05T14:46:52Z",
        "full_text": "I am fascinated by the world of possibilities that Cloud Computing enables people and organizations to achieve. When it comes to security, tools and frameworks such as the Shared Responsibility Model make following good security practices easier than ever. I am equally fascinated by new attack vectors that Cloud Computing enables bad actors to achieve, though.\n\nNot that recently ago, Halcyon put up a really interesting article about a concerning new ransomware campaign targeting Amazon S3 buckets. This is a new kind of ransomware. One that only exists in the cloud, thanks to the cloud, since it leverages some of the many great security features that are built-in into AWS to help organizations achieve security and compliance encrypting Amazon S3 Objects, but to encrypt for ransom instead.\n\nI am not going to go over many details about the attack itself, since there are many great articles out there going over them already, like the one from Halcyon themselves or this one from SentinelOne. So why are we here, then? I believe the kind of information that these types of articles bring are priceless, but I also believe that one should be able to programmatically be able to validate if their own environment are susceptible to this kind of attack, and also validate if they can detect and respond in case they are.\n\nThis article is about understanding how S3 encryption works, how you can use the S3 Ransomware Simulator to test your own environment, how you can programmatically detect this kind of attack, respond to it, but also how to prevent it as well.\n\nThe Attack #\n\nOn its core, the attack is simple, but it requires understanding a bit of how encryption works in Amazon S3.\n\nYou might have heard that Amazon S3 automatically applies encryption to all new object uploaded at no additional cost and with no impact on performance since January 5, 2023. And that’s great news! But there are different ways to encrypt an object in AWS, so first we need to go over them.\n\nUnderstanding S3 Encryption #\n\nClient-side encryption - This is the most straightforward type of Encryption in S3. You/your application encrypt the data, before uploading it to S3, with a key that you own and manage, even if outside of AWS.\n\nServer-side encryption: Amazon S3 managed keys (SSE-S3) - This Encryption method is the one that is enabled by default since 2023. You send your objects and they are encrypted by AWS Server-side, with Amazon S3 managed keys, and each object is encrypted with a unique key.\n\nServer-side encryption: AWS KMS keys (SSE-KMS) - AWS KMS is a managed service to create and manage keys. Here you send your object and AWS uses server-side encryption leveraging these KMS keys to encrypt them, in case a compliance standard you must adhere to requires you to have full control of the encryption keys.\n\nDual-layer server-side encryption: AWS KMS keys (DSSE-KMS) - Not that different from SSE-KMS. However, some compliance standards require you to apply multilayer encryption to your data, so DSSE-KMS applies two layers of encryption to the objects.\n\nServer-side encryption: customer-provided keys (SSE-C) - It’s like Client-side and Server-Side encryptions had a baby. Like in Client-Side, AWS doesn’t host/manage your keys. Like SSE, you don’t need to worry about encrypting your objects before uploading to the bucket. Here you provide the key as part of the upload request and AWS will encrypt the object on upload, but never save the key anywhere. This is the one we care about.\n\nWhy Is it Effective? #\n\nGiven that an attacker has access to rewrite a victim’s S3 Objects, picking SSE-C as encryption method is the most effective way to guarantee that only they can recover the objects. Since the attacker can create one unique key per victim, they can leverage this key to rewrite the objects, overwrite them and the only way to recover access to these files, would be using they key that belongs only to the attacker.\n\nReplicating The Attack #\n\nIn order to programmatically detect and respond to this kind of attack, we need to first be able to programmatically replicate this kind of attack. When it comes to the S3 API, I am fairly familiar with the GetObject and PutObject actions. But in my mind it wouldn’t make much sense for an attacker to download (GetObject) every single object in a bucket in order to upload (PutObject) them back, while encrypting the data. So I started a research on the best way to encrypt existing objects in a bucket.\n\nThat research led me to, funnily enough, AWS’s own blog page, where a blog post on how to encrypt existing objects described some of the best techniques. Even though the article uses the AWS CLI to encrypt the existing objects, and my goal is to use Python’s Boto3 SDK, it led me exactly to what I was looking for, the CopyObject action. In summary, I just need to make a CopyObject request, where the source and destinations of the copy are the same object, while making sure I was making the proper encryption request as well.\n\nThe Code #\n\nFirst and foremost, you can follow along in your own environment. You can find the code in the S3 Ransomware Simulator GitHub repository.. It tries, as much as possible, to mimic the behavior of an attacker exploiting your own AWS environment.\n\nThe behavior goes as below:\n\nIt enumerate all the buckets available in that account, if the flag --all-buckets is used; It generates and saves to disk an AES-256 encryption key to be used in the attack; For each of the buckets, or just the one in case the flag --bucket-name was used it will: Check if it can PutObject in the bucket, dropping a dummy object Check if it can GetObject in the bucket, getting the previously uploaded dummy file Deletes the dummy file Considering all permissions are in place, and the flag --encrypt-objects was provided, it will: List and encrypt all objects Drop a fake ransom note\n\nAn example of the execution can be seen below:\n\n$ python3 attacker.py --bucket-name raphabot-no-ransomware --encrypt-objects S3 Bucket Encryption Tool with SSE-C Processing specified bucket: raphabot-no-ransomware Generated AES-256 encryption key for SSE-C: M+a4reQycj3pBBZyYs1KE9XpOcdyT7kGq1Mu+q5u+vM = Key MD5: S2k8nSe8W9C7A2JO+Nr4mw == Checking bucket: raphabot-no-ransomware GetObject permission: Yes PutObject permission: Yes Processing bucket: raphabot-no-ransomware Encrypting: regular-file.txt Encrypted 1 files in raphabot-no-ransomware using SSE-C Ransom note dropped in raphabot-no-ransomware. Encryption key saved to encryption_key.bin WARNING: This key is required to decrypt your files. Store it securely! Encryption complete. Total files encrypted: 1 Warning: Without the encryption key, your files cannot be recovered!\n\nIn order to respond, we need first to detect. If you ever read about logging and monitoring in Amazon S3, you know there are many different options to do so. To understand if our buckets are being targets of a Ransomware attack, however, some options are better than others.\n\nSo I created a criteria for how I’d listen to events. Whatever method that was picked, had to:\n\nBe cheap/free Be scalable Be simple Be fast on notifying of the event\n\nIf you’ve been around for a while, you might know that the most traditional way to listen to events in an S3 bucket is to use Event Notification. At first, this looked like a great option, since it is built to have event notifications delivered in seconds, it is free and, although originally only supporting SNS and SQS, since November of 2021 it supports EventBridge. If you are not familiar with Amazon EventBridge, the gist is that it is a serverless service that makes it easier to build scalable event-driven applications.\n\nScalability doesn’t end with the performance of detection, though, and I also want to be able to deploy this detection across many buckets at scale, as code. This is where this solution starts to fall apart. Despite CloudFormation obviously supporting S3 Buckets, the S3::Bucket resource type creates an Amazon S3 bucket, it doesn’t update one. An alternative would be using Custom Resources, but this would come with its own set of challenges when it comes to scale (applying the event notification across multiple buckets before the maximum Lambda timeout, for instance), complexity (writing code to take in consideration any exceptions) and security (maintaining the code dependencies up to date).\n\nAnother great option would be using CloudTrail. CloudTrail comes enabled by default, logs management events across AWS services also by default, and it is free. So, chances are that you are already using CloudTrail. It is fast, with AWS suggesting that CloudTrail publishes log files about every 5 minutes, but real world testing shows that the delay is considerably lower than that. It doesn’t come without its own set of challenges, though.\n\nYes, CloudTrail is enabled by default and it’s free… for management events. Events that happen within buckets, like CopyObjects, are called Data events, which are not enabled by default and they cost $0.10 per 100,000 data events delivered. However, when it comes to scalability and simplicity of deployment, it couldn’t be a better match. Either through the Console or via CloudFormation, one can create a new CloudTrail Trail (you gotta love AWS naming!) to listen to one, some or all Buckets. As you can imagine, even if you filter to listen to data events of only the most critical S3 Buckets, listening to all data events can get pretty expensive pretty quickly. The good news is that CloudTrail enables us to use advanced event selectors to filter which events we are listening to.\n\nCreating an Advanced Selector like the one below, enables us to listen only to the CopyObject event for the selected Buckets:\n\n[ { \"Name\" : \"CopyObject\" , \"FieldSelectors\" : [ { \"Field\" : \"eventCategory\" , \"Equals\" : [ \"Data\" ] }, { \"Field\" : \"resources.type\" , \"Equals\" : [ \"AWS::S3::Object\" ] }, { \"Field\" : \"eventName\" , \"Equals\" : [ \"CopyObject\" ] } ] } ]\n\nIn summary, using CloudTrail, we are able not just to deploy a “detector” at scale easily, but also to cheaply run it at scale as well. The proof is that you can find this Detection defined as CloudFormation code that you can apply today in the same S3 Ransomware Simulator repository.\n\nOnce we detect an attempt, we need to be able to respond to it. The response can and will look different based on different organization preferences. Some would not ever dream of making a change in their AWS environment automatically, others would like to have humans (like you!) notified so they can take action, but some would be fine to take at least some proactive action automatically based on these events, while further investigation is ongoing. For the purposes of this blog post, we will follow AWS’ own best practices on how to remediate if there are unauthorized activity in an AWS account.\n\nThe first step to remediate the compromise of an AWS identity is to, first, understand what kind of identity it is, since the remediation steps to deal with each kind of identity is different. The type of identity used in an API call can be determined by checking the type attribute of the userIdentity object in the CloudTrail event. There are three types of identity in AWS:\n\nIAM User : When one creates an IAM User and wants to make a request against an AWS service, they need to generate a long lived pair of access key id and secret. Shows up as IAMUser in CloudTrail.\n\n: When one creates an IAM User and wants to make a request against an AWS service, they need to generate a long lived pair of access key id and secret. Shows up as in CloudTrail. Assumed Role : This is generally used when an application/service, not a person, needs to access AWS resources. Assuming a role leads to AWS Security Token Service generating a short-lived pair. Shows up as AssumedRole in CloudTrail.\n\n: This is generally used when an application/service, not a person, needs to access AWS resources. Assuming a role leads to AWS Security Token Service generating a short-lived pair. Shows up as in CloudTrail. Identity Center User: AWS IAM Identity Center streamlines and simplifies workforce user access to applications or AWS accounts. For a request made on behalf of an IAM Identity Center user, it will show up as userIdentity in CloudTrail.\n\nNow let’s talk about the actual remediation: blocking this identity from making further requests in the AWS account.\n\nFor an IAM User, for instance, you could disable the user’s Access Keys. It’s good to remember that AWS recommends that, as best practice, to use temporary security credentials (such as IAM roles) instead of creating long-term credentials like access keys. So that’s probably a good idea anyway 😅\n\nFor an Assumed Role, you have options. You could update the role to remove this access, you could attach a policy denying CopyObject actions to S3… the options are close to limitless! If you want to be precise, you can restrict requests from the attacker’s IP address.\n\nExample workflow that I setup for this Response: #\n\nThe workflow below will guarantee that, in case of an attack where the identity is either an IAM User or an Assumed Role, that the identity will be invalidated automatically.\n\nYou can find this sample Response defined as CloudFormation code in the same S3 Ransomware Simulator repository.\n\nOf course, better than remediating this kind of attack, is to prevent it in the first place. Perfect security is a pipe dream that we all chase, but there are some actions that you can take today to make your environment safer against this kind of threat. Here’s a list of some of them.\n\nRestrict SSE-C Usage #\n\nThe most effective action that you can take, in case your organization isn’t using SSE-C, is to block its usage at least in the most critical S3 Buckets. Using Amazon S3 condition keys, you can update your Bucket Policy adding something like the following:\n\n{ \"Version\" : \"2012-10-17\" , \"Id\" : \"PutObjectPolicy\" , \"Statement\" : [ { \"Sid\" : \"RestrictSSECObjectUploads\" , \"Effect\" : \"Deny\" , \"Principal\" : \"*\" , \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3:::my-important-bucket/*\" , \"Condition\" : { \"Null\" : { \"s3:x-amz-server-side-encryption-customer-algorithm\" : \"false\" } } } ] }\n\nRestrict CopyObject #\n\nIf your applications are not using the CopyObject action, it might be a good idea to block it in your most critical S3 Buckets. However, as pointed out by Jason Kao, one can’t simply block the CopyAction. But if you look closely to the CopyObject API, it is the same PUT http verb as the PutObject. One of the main differences is the collection of x-amz-copy-source headers. So, if we craft our bucket policy to block any PutObject request that contains the x-amz-copy-source header, we are effectively blocking any CopyObject request.\n\nExample of BucketPolicy:\n\n{ \"Version\" : \"2012-10-17\" , \"Id\" : \"CopyObjectPolicy\" , \"Statement\" : [ { \"Sid\" : \"RestrictCopyObject\" , \"Effect\" : \"Deny\" , \"Principal\" : \"*\" , \"Action\" : \"s3:PutObject\" , \"Resource\" : \"arn:aws:s3:::my-important-bucket/*\" , \"Condition\" : { \"Null\" : { \"s3:x-amz-copy-source\" : \"false\" } } } ] }\n\nObject Versioning #\n\nThis is by far the easiest to implement, with the best result. It is, as well, the most expensive. Enabling Object Versioning in your must critical buckets will guarantee that, in case of a Ransomware attack, or even an accidental overwrite or deletion, you can still recover the original Object. Example on how to enable it using the AWS CLI:\n\naws s3api put-bucket-versioning --bucket my-important-bucket --versioning-configuration Status = Enabled\n\nNow, in case the identity that the attacker is assuming has full access to the objects in the bucket, they can still delete the older versions using s3:DeleteObjectVersion. You might want to deny this action as well:\n\n{ \"Version\" : \"2012-10-17\" , \"Id\" : \"DeleteObjectVersionPolicy\" , \"Statement\" : [ { \"Sid\" : \"RestrictDeleteObjectVersion\" , \"Effect\" : \"Deny\" , \"Principal\" : \"*\" , \"Action\" : \"s3:DeleteObjectVersion\" , \"Resource\" : \"arn:aws:s3:::my-important-bucket/*\" } ] }\n\nPretty please, avoid using hardcoded secrets #\n\nThis one is self explanatory and a core principle of proper cloud and application security: avoid using hardcoded secrets. Sooner or later, one way or another, hardcoded secrets always find a way to get in the wrong hands. Secret Scanning should be part of your CI/CD pipelines.\n\nAlthough Ransomware is far from being a novel kind of attack, even in Cloud environments such as AWS, as the cloud usage ramps up, so does this kind of attack. As organizations move more and more of critical customer data to the cloud, while adoption techniques such as Workload Isolation to reduce blast radius, it’s more important than ever to have a full grasp of the environment’s posture.\n\nPosture isn’t enough, however. Different teams will have different levels of maturity and you must be ready to detect and respond to this kind of event in near real time. Make sure you are listening to, and reacting to, your CloudTrail events. A good way to understand if you organization is ready for this, but also to get ready as well, is using the provided code to simulate, detect and respond to this kind of attack in your own AWS accounts."
    },
    {
        "title": "Sam Altman says OpenAI is no longer \"compute-constrained\" — after Microsoft lost its exclusive cloud provider status",
        "description": "OpenAI CEO Sam Altman recently indicated that the company is no longer compute-constrained, further claiming that the company can comfortably foster the development of advanced AI models.",
        "url": "https://www.windowscentral.com/software-apps/sam-altman-says-openai-is-no-longer-compute-constrained",
        "source": "Windows Central",
        "publishedAt": "2025-04-15T10:00:26Z",
        "full_text": "Sam Altman recently indicated that OpenAI is no longer compute-constrained, making it easier for the company to develop sophisticated and advanced AI models without any limitations and restrictions.\n\nAs part of its multi-billion dollar partnership with Microsoft, OpenAI had exclusive access to the tech giant's vast computing power. However, multiple reports suggested that Microsoft didn't meet the computing threshold required by OpenAI to facilitate its computing needs.\n\nThe AI lab expressed fears of other rival firms hitting the coveted AGI benchmark before it did, further indicating that it would be Microsoft's fault. Consequently, OpenAI managed to find some wiggle room in its agreement with Microsoft, following its $500 billion Stargate project announcement to build data centers across the United States to facilitate its AI efforts.\n\nMicrosoft ended up losing its exclusive cloud provider and largest investor status as OpenAI seemingly tightened it partnership with SoftBank, which recently led its latest round of funding, raising $40 billion from key investors, pushing its market cap to $300 billion.\n\nIt'd only take 5 to 10 people to build GPT-4 from scratch\n\nGPT-4 would, apparently, no longer require an army of people to build. (Image credit: Getty Images | SOPA Images)\n\nAs you may know, OpenAI is getting ready to discontinue GPT-4 and replace it with GPT-4o in ChatGPT. However, the company indicated that users can continue accessing the model via its API.\n\nOpenAI CEO Sam Altman has openly expressed his disappointment with the model, citing that it \"kind of sucks\" and is mildly embarrassing at best. He further indicated that GPT-4 would be the dumbest model that users would have to interact with ever, potentially suggesting an upward trajectory in terms of capabilities for OpenAI's flagship models.\n\nPerhaps more interestingly, while speaking to engineers behind the development of GPT-4.5, the executive asked them what it would take to build and develop GPT-4 again.\n\nGet the Windows Central Newsletter All the latest news, reviews, and guides for Windows and Xbox diehards. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nFor context, the CEO revealed that when GPT-4 was being developed, it took \"hundreds of people, almost all of OpenAI's effort.\" Interestingly, things have seemingly gotten easier as generative AI has scaled and advanced to greater feats.\n\nInterestingly, Alex Paino, lead for GPT-4.5 pretraining machine learning, indicated that the GPT-4's development process would probably only need 5 to ten people (via Business Insider).\n\nAccording to the engineer:\n\n\"We trained GPT-4o, which was a GPT-4-caliber model that we retrained using a lot of the same stuff coming out of the GPT-4.5 research program. Doing that run itself actually took a much smaller number of people.\"\n\nOpenAI researcher Daniel Selsam seemingly reiterated Paino's sentiments, indicating that building GPT-4 from scratch would be much easier now. \"Just finding out someone else did something — it becomes immensely easier,\" the researcher added. \"I feel like just the fact that something is possible is a huge cheat code.\""
    },
    {
        "title": "CNCF tells main NATS contributor Synadia that it's free to fork off",
        "description": "But what it can't do is 'unilaterally claw back a community project and its infrastructure, assets, and branding'\nThe Cloud Native Computing Foundation (CNCF) has filed a petition with the US Patent and Trademark Office to prevent Synadia from using the logo …",
        "url": "https://www.theregister.com/2025/04/28/cncf_synadia_nats_dispute/",
        "source": "Theregister.com",
        "publishedAt": "2025-04-28T18:32:05Z",
        "full_text": "The Cloud Native Computing Foundation (CNCF) has filed a petition with the US Patent and Trademark Office to prevent Synadia from using the logo and domain for NATS, the open source messaging system.\n\nThe move follows a decision by Synadia, the vendor behind most of the contributions to the server software, to change the licensing terms of the NATS server, moving it from Apache 2.0 to Business Source License (BSL) in future releases.\n\nAt the same time, the company's lawyers wrote to the Linux Foundation [PDF], which runs CNCF, to end its relationship with CNCF and receive full control of the NATS.io domain and GitHub repository.\n\n\"The NATS.io project has failed to thrive as a CNCF project, with essentially all growth of the project to date arising from Synadia's efforts and Synadia's expense,\" the letter said.\n\nSynadia's counsel also argued in the letter that the \"law is well settled that ownership of a domain name does not establish trademark rights; rather, it is the content of an associated website that may create trademark rights,\" going on to claim that Synadia and predecessor Apcera \"at all times\" controlled the content of the website.\"\n\nIn a blog post last week, CNCF said Synadia had a number of options in moving to a less permissive software license, but the company was not allowed to keep the logo and the domain for the open source project, noting its charter states \"Any project that is added to the CNCF must have ownership of its trademark and logo assets transferred to the Linux Foundation.\"\n\nIt went on to claim: \"CNCF has offered multiple paths that would allow Synadia to pursue its goals while respecting open source principles and community governance,\" adding: \"Synadia is free to walk away from contributing to the existing NATS project. They're also free to fork NATS and build a proprietary offering under a new name. What they can't do is unilaterally claw back a community project and its infrastructure, assets, and branding.\"\n\nCNCF said it has also filed petitions with the US Patent and Trademark Office to prevent Synadia from continuing to use the logo [PDF] or domain name [PDF].\n\n\"The Linux Foundation and the CNCF have protected the licensing integrity of open source projects before,\" the CNCF said in its blog. \"There are proper ways for companies to fork projects and take another direction based on business needs. For example, the vendor Grafana forked the CNCF Cortex project under the new name Mimir while the original Cortex project continues to be maintained by the community within CNCF. Synadia's actions here are markedly different. Rather than creating a fork of NATS under a new name, Synadia wants to unilaterally seize control of the project's community-owned assets.\n\n\"Synadia is attempting to convert a successful open source project into a closed, commercial product – and take the NATS community's name, trust, and infrastructure with it. Imagine if Google tried to take back Kubernetes after all these years of it being a neutral open source and community-driven project.\"\n\nHowever, Derek Collison, creator of NATS.io and Synadia founder and CEO, replied that BSL would continue to offer code transparency while the server software would revert to the Apache 2.0 license after two to four years, a model similar to other mixed source software companies.\n\nHe said: \"NATS is primarily funded, supported, and maintained by Synadia. This does not align neatly with the CNCF's model. Over recent years, it has become apparent that the CNCF may no longer be the best strategic fit for NATS. Rather than face forced archival, Synadia proactively initiated internal discussions with CNCF about a joint announcement regarding a departure, ensuring NATS' continued health and development.\n\n\"To sustain long-term company and project viability, we explored excluding some advanced features and enhancements from the NATS server and licensing them separately instead. Ultimately, we considered a more community-minded approach: to include them in the NATS server while exploring a BSL license model for future versions. While the BSL is not OSI-approved, it ensures source code remains transparent and publicly accessible, reverting to Apache 2.0 after a defined period (typically 2-4 years). An Apache 2.0 licensed server version will always remain available and supported.\"\n\nThe Register has contacted Synadia for additional comment.\n\nCommenting on LinkedIn, William Morgan, CEO of Buoyant, creators of the service mesh Linkerd, said: \"The announcement that Synadia wants to withdraw NATS from the CNCF feels like it is going to end in a messy legal fight. This is not your run-of-the-mill OSS relicensing story.\" ®"
    },
    {
        "title": "NATS custody battle ends with CNCF and Synadia sharing nicely",
        "description": "Trademark, domain name, GitHub repos stay under control of open source foundation – no word on any forking off for now\nThe Cloud Native Computing Foundation (CNCF) has reached an agreement with Synadia, the company behind open source messaging system NATS, wh…",
        "url": "https://www.theregister.com/2025/05/02/cncf_synadia_nats/",
        "source": "Theregister.com",
        "publishedAt": "2025-05-02T17:03:07Z",
        "full_text": "The Cloud Native Computing Foundation (CNCF) has reached an agreement with Synadia, the company behind open source messaging system NATS, which will see it control the trademark, domain name, and GitHub repositories.\n\nThe agreement follows a dispute over Synadia's decision to move its NATS code from the Apache 2.0 to Business Source License (BSL) in future releases.\n\nThe dispute saw CNCF file a petition with the US Patent and Trademark Office to prevent Synadia from using the logo and domain for NATS. At the same time, Synadia's lawyers told the Linux Foundation, which runs CNCF, that it intended to end its relationship with CNCF and expected to receive full control of the NATS.io domain and GitHub repositories.\n\nFollowing productive discussions, the two organizations have agreed to \"align expectations and reinforce open governance.\" Synadia will assign its two NATS trademark registrations to the Linux Foundation to secure the project's neutral stewardship, a statement said.\n\nMeanwhile, CNCF will continue to hold the NATS infrastructure and assets, including the NATS.io domain name and GitHub repositories, under the Apache 2.0 license.\n\nIn a prepared statement, Todd Moore, SVP of Community Operations at the Linux Foundation, said: \"As steward of the NATS project, CNCF is committed to upholding open collaboration, neutral governance, and shared ownership so NATS can continue to grow and thrive as a community-driven project. We value all of Synadia's efforts in developing and contributing to NATS – including their investment in defense of the NATS trademark – and appreciate their continued support of the project.\"\n\nThe joint statement also said Synadia would be free to pursue its own commercial interests by building on top of the open source NATS project. \"As with any open source codebase, if Synadia chooses to fork the NATS server code for a proprietary offering in the future, it will do so under a new name,\" it said.\n\nSynadia has not indicated whether it will fork the code and use a Business Source License (BSL) in future releases, as it had previously stated.\n\nDerek Collison, CEO of Synadia and the creator of NATS, said: \"NATS has been a labor of love of mine for nearly 15 years now. The entire Synadia team and I deeply care about NATS and have devoted significant resources to its growth and adoption. Our commitment remains unwavering – to provide impactful, accessible technology that benefits the global community. We genuinely look forward to deepening our collaboration with the Linux Foundation and CNCF, ensuring all mature and successful projects within the CNCF receive the robust support they need to thrive.\n\n\"The outpouring of support from the global community over the past week for NATS to remain a thriving open source project with Synadia's continued involvement has been extraordinary and deeply appreciated.\" ®"
    },
    {
        "title": "Beyond Elk: Lightweight and Scalable Cloud-Native Log Monitoring",
        "description": "This article explores the growing limitations of the ELK stack in modern log storage scenarios and introduces GreptimeDB as a next-generation log database with advantages in both architecture and user experience. By combining Vector with GreptimeDB, we demons…",
        "url": "https://greptime.com/blogs/2025-04-24-elasticsearch-greptimedb-comparison-performance",
        "source": "Greptime.com",
        "publishedAt": "2025-04-28T20:34:56Z",
        "full_text": "ELK Overview ​\n\nBack in the days when options were limited, Elasticsearch became the preferred solution for log storage and query due to its excellent full-text search capabilities. Later on, elastic.co built the ELK ecosystem around Elasticsearch, providing a complete collection, storage, and analysis ecosystem. The later addition of the beats components further improved the shortcomings in data collection, making data collection more flexible and lightweight. As a result, ELK has become the de facto answer for log collection solutions.\n\nAs we enter 2025, various new programming methods and deployment paradigms are being widely adopted. Compared to more modern solutions, Elasticsearch's relatively old technical and architectural design gradually reveals some drawbacks:\n\n1. Soaring Storage Costs: The More Logs, The Higher the Cost ​\n\nWith the increasing complexity of software systems, the volume of logs produced by applications is growing exponentially. To troubleshoot root causes, we often need to retain all logs as long as possible. However, Elasticsearch builds an index upon each line of logs to speed up full-text search, which means HUGE storage overhead.\n\nOur tests found that ingesting 10GB of log data resulted in Elasticsearch generating more than 10GB of storage files. Long-term storage size of log data has made storage costs the primary reason for replacing ELK.\n\n2. Coupled Storage and Computation, Severe Resource Waste ​\n\nAs a system born before cloud computing, Elasticsearch naturally uses local disk storage and has a built-in data replication mechanism. Combined with the large amount of data mentioned above, this means a rising dependence on high-performance SSDs.\n\nWhat's worse is that storage and computing resources are bound together: if we want to scale out the CPU to handle high concurrency, we scale out the disk as well; and vice versa. The result is that we pay for resources we don't use.\n\n3. Thirst for Resources and Prone to OOM ​\n\nRunning on the JVM, Elasticsearch is thirsty for hardware resources, especially memory. Discussions about \"OOM being killed\" are common in various forums. In production environments, Elasticsearch generally requires very high-spec machines to run.\n\nComparing multiple log databases, we found that Elasticsearch's hardware resource consumption is the highest under the same write request pressure (assuming it can handle the volume without crashing).\n\nElasticsearch is also notorious for its high maintenance difficulty. Starting a single node might be easy, but when we face scenarios like upgrades, scaling out, fault recovery, and backups, every step can be daunting. Trying to automate Elasticsearch clusters with Kubernetes is almost impossible.\n\nAlthough ELK was once popular and is still the choice for many users' log monitoring solutions, the above analysis clearly shows that ELK is gradually falling behind in meeting the needs of high ingestion volumes and long-term storage for real-time log monitoring and data analysis.\n\nWe need a more modern, low-cost, and easy-to-operate log monitoring solution.\n\nGreptimeDB as the Log Monitoring Storage Solution ​\n\nGreptimeDB is a cloud-native database designed for observability data, making it well-suited for metric collection, log storage, and real-time monitoring. Its architecture is optimized for high-frequency time-stamped data ingestion and querying, such as metrics, logs and events.\n\nAs a cloud-native database, GreptimeDB employs a storage-compute separation architecture. Native on Kubernetes, it enables seamless elastic scaling, making it ideal for cloud environments. Independent resource scaling ensures cost efficiency and stable performance under high-demand workloads—requiring minimal manual intervention.\n\nCompared to traditional solutions, GreptimeDB's storage-compute separation architecture and cloud-native design make it a highly suitable modern time-series database for log storage. Here are its core advantages in actual use:\n\n1. High Compression Rate, Saving Storage Means Saving Money ​\n\nGreptimeDB is a columnar database that achieves high data compression using methods such as run-length encoding and dictionary encoding.\n\nOur tests found that under the same log ingestion volume, GreptimeDB's storage file size is about 1/10 of Elasticsearch's—store more, occupy less.\n\n2. Compute-Storage Separation, Further Reducing Storage Costs ​\n\nGreptimeDB adopts the storage-compute separation architecture from the start, storing data in object storage, which is both cost-effective and reliable:\n\nIn cloud services like AWS, the price of object storage is usually less than half of block storage.\n\nof block storage. Storage reliability is guaranteed by the underlying object storage, preventing the complexity of the database itself for implementing data replication and backup.\n\nis guaranteed by the underlying object storage, preventing the complexity of the database itself for implementing data replication and backup. No capacity limit, and no coupled resources. No need to scale up the disk when scaling up the CPU, avoiding resource waste.\n\n3. Lightweight, Less Hardware Requirements ​\n\nWritten in Rust, GreptimeDB consumes fewer system resources and runs stably, even on low-end hardware.\n\nIn our tests, under the same ingestion volume, Elasticsearch's CPU and memory usage are several times that of GreptimeDB. For log systems, this means fewer OOMs and better stability.\n\nThanks to its cloud-native architecture, GreptimeDB's deployment and maintenance experience on Kubernetes is very smooth and simple. Once deployed, it automates maintenance operations such as rolling updates, shutdowns, restarts, resource changes, and load balancing, ensuring stable operation with minimal to no technician intervention. For ops teams, this provides genuine convenience and ease of use.\n\n5. Multiple Indexing Mechanisms for Accelerating Query ​\n\nGreptimeDB provides various index types to adapt to different query needs:\n\nFor low cardinality data (such as k8s_pod_ip ), an inverted index can be used to speed up filtering operations.\n\n), an inverted index can be used to speed up filtering operations. For high cardinality text (such as trace_id ), a skipping index can be set to improve the efficiency of precise queries.\n\n), a skipping index can be set to improve the efficiency of precise queries. For fuzzy text searches, a full-text index can be set to support flexible search.\n\nBy flexibly combining indexes, query speed is significantly improved while reducing the overhead of building and storing indexes. More detailed index introductions can be found in this document.\n\nUsing Vector + GreptimeDB as a Log Monitoring Solution ​\n\nNext, we will demonstrate how to quickly build a log collection and storage solution using Vector and GreptimeDB.\n\n（Figure 1: Vector + GreptimeDB: Log Monitoring Solutions）\n\nIn this simple solution introduction, we use:\n\nUse Flog to mock local log files; Use Vector to collect local log files and ingest them into the GreptimeDB instance; Use GreptimeDB's built-in logview or other tools to view the ingested log rows.\n\nMock Log Files ​\n\nWe use flog to mock generating a continuously outputting log file. The following command can quickly generate a log file log.txt that writes one line of logs per second:\n\nIn production environments, applications will use the logging library to print logs to log files. Log files may have time suffixes and may be automatically split and rotated according to rules. For some popular middlewares, Vector also integrates log collection components, such as kubernetes logs:\n\nExecute head -1 log.txt to view a sample of the log data:\n\nshell 17.61.197.240 - nikolaus3107 [14/Apr/2025:21:11:44 +0800] \" HEAD /envisioneer/efficient HTTP/1.0 \" 406 5946\n\nDeploy GreptimeDB Instance ​\n\nWe need to deploy a GreptimeDB instance. Although this article uses a standalone instance as an example, the data ingestion API is the same for both standalone and cluster instances. In production environments, cluster deployment is preferred, which can be referred to in this document.\n\nInstalling GreptimeDB is relatively simple, and different installation methods can be referred to in this document. To facilitate reproduction in various environments, we use Docker to start the GreptimeDB database:\n\nshell docker run -p 127.0.0.1:4000-4003:4000-4003 \\ --name greptime --rm \\ greptime/greptimedb:v0.14.0-nightly-20250407 standalone start \\ --http-addr 0.0.0.0:4000 \\ --rpc-bind-addr 0.0.0.0:4001 \\ --mysql-addr 0.0.0.0:4002 \\ --postgres-addr 0.0.0.0:4003\n\nThen verify that the database has been successfully started by executing curl 127.0.0.1:4000/health; we can also use the MySQL-compatible client to connect to the database for confirmation by executing mysql -h127.0.0.1 -P4002 .\n\nCollect Logs Using Vector ​\n\nWe need to install Vector first, which can be referred to in this official document. It is recommended to use the system's package manager to install for convenience. After installation, we can verify the installation by running vector --version . In this article, we will collect log data by starting Vector locally. For Vector deployment in production environments, we can refer to this blog.\n\nVector Configuration Introduction ​\n\nNext, we need to write the Vector configuration required for collecting logs. The example is as follows:\n\ntoml # config.toml [ sources . file_input ] type = \" file \" include = [ \" <path_to_log_dir>/log.txt \" ] data_dir = \" <data_dir> \" [ sinks . greptime_sink ] type = \" greptimedb_logs \" inputs = [ \" file_input \" ] compression = \" gzip \" dbname = \" public \" endpoint = \" http://127.0.0.1:4000 \" pipeline_name = \" greptime_identity \" table = \" app_log \"\n\nLet's take a closer look at this configuration:\n\ntoml [ sources . file_input ] type = \" file \" include = [ \" <path_to_log_dir>/log.txt \" ] data_dir = \" <data_dir> \" ignore_checkpoints = true\n\nThis is Vector's file source. Source is Vector's data input, which can collect data from various adapted sources. We specify using File Source through type = \"file\" , and the include option is used to configure the file location (include can use wildcards to configure multiple log files, see the document). Finally, we need to configure data_dir to specify a metadata directory. Vector's default path for data_dir is /var/lib/vector/ :\n\ntoml [ sinks . greptime_sink ] type = \" greptimedb_logs \" inputs = [ \" file_input \" ] compression = \" gzip \" dbname = \" public \" endpoint = \" http://127.0.0.1:4000 \" pipeline_name = \" greptime_identity \" table = \" app_log \"\n\nThis is the GreptimeDB logs sink. Sink is Vector's data output, sending data to the adapted output end. We specify using GreptimeDB log output through type = \"greptimedb_logs\" , and the inputs specify using the above file source. The configuration here merely specifies the connection parameters of GreptimeDB:\n\ncompression specifies the compression option for sending data;\n\nspecifies the compression option for sending data; dbname specifies the database to ingest to; here we ingest to the default public database;\n\nspecifies the database to ingest to; here we ingest to the default database; endpoint specifies the HTTP address of the GreptimeDB instance. In the above, we started the GreptimeDB instance using docker and bound the database's 4000 port to the 127.0.0.1 ;\n\nspecifies the HTTP address of the GreptimeDB instance. In the above, we started the GreptimeDB instance using and bound the database's 4000 port to the ; pipeline_name specifies the pipeline for log ingestion. Pipeline is a built-in mechanism of GreptimeDB for preprocessing text data. Here we use greptime_identity , which does not process the raw data and directly stores the input JSON data in separate columns in the database;\n\nspecifies the pipeline for log ingestion. Pipeline is a built-in mechanism of GreptimeDB for preprocessing text data. Here we use , which does not process the raw data and directly stores the input JSON data in separate columns in the database; table specifies which database table to ingest the data to. If this table does not exist, it will be automatically created.\n\nIngest Logs to GreptimeDB ​\n\nAfter configuration, we can ingest the log data from the file to GreptimeDB. Run the following command to start Vector using the configuration:\n\nshell vector -c < path_to_config_fil e >\n\nIf we see the following log and do not see any ERROR , we can confirm that Vector has started ingesting data to GreptimeDB:\n\nshell 2025-04-15T06:53:17.864603Z INFO vector::topology::builder: Healthcheck passed.\n\nQuery Data in GreptimeDB ​\n\nNext, we can verify the data has been ingested by querying GreptimeDB. Use the MySQL-compatible client to connect to the database by running mysql -h127.0.0.1 -P4002 .\n\nFirst, execute show tables to check whether the table was successfully created:\n\nshell +---------+ | Tables | +---------+ | app_log | | numbers | +---------+ 2 rows in set (0.018 sec )\n\nThen execute select * from app_log limit 2 ; to observe the data:\n\nshell +----------------------------+------------------------+-----------+------------------------------------------------------------------------------------------------------------------+-------------+--------------------------------+ | greptime_timestamp | file | host | message | source_type | timestamp | +----------------------------+------------------------+-----------+------------------------------------------------------------------------------------------------------------------+-------------+--------------------------------+ | 2025-04-15 07:02:37.136667 | < path_to_file > /log.txt | some_host | 17.61.197.240 - nikolaus3107 [14/Apr/2025:21:11:44 +0800] \" HEAD /envisioneer/efficient HTTP/1.0 \" 406 5946 | file | 2025-04-15T07:02:36.116847265Z | | 2025-04-15 07:02:37.136674 | < path_to_file > /log.txt | some_host | 56.87.252.7 - - [14/Apr/2025:21:11:45 +0800] \" PATCH /bricks-and-clicks/transition/interfaces HTTP/1.1 \" 416 15579 | file | 2025-04-15T07:02:36.116864203Z | +----------------------------+------------------------+-----------+------------------------------------------------------------------------------------------------------------------+-------------+--------------------------------+ 2 rows in set (0.032 sec )\n\nwe can see that in addition to the message column storing the mocked logs, there are several additional columns:\n\ngreptime_timestamp : Since we used greptime_identity and did not specify a time index column (we can manually specify the index column in greptime_identity mode, see the document), GreptimeDB uses the timestamp the log reaches the server and sets it as the time index column. Usually, we prefer the time index to be set to the actual log generation time to more accurately restore the event sequence;\n\n: Since we used and did not specify a time index column (we can manually specify the index column in mode, see the document), GreptimeDB uses the timestamp the log reaches the server and sets it as the time index column. Usually, we prefer the time index to be set to the actual log generation time to more accurately restore the event sequence; file , host , source_type , timestamp : Careful readers may have noticed that these are runtime context information automatically attached by Vector. If not manually deleted, Vector includes this context information by default when outputting.\n\nView Data Through log-query ​\n\nGreptimeDB comes with a dashboard console that allows we to quickly execute data queries and operations. Enter http://127.0.0.1:4000/dashboard/#/dashboard/log-query in the browser to quickly open the log-query page:\n\n（Figure 2: Visiting Log-Query Page）\n\nClick the Table dropdown menu, and we can see the app_log table just created. Click to select the app_log table, then click the run button above to view the log data just ingested in log-query :\n\n（Figure 3: Viewing Ingested Data）\n\nParse Logs Using Pipeline ​\n\nIn the previous section, we successfully ran Vector to collect log data from the file and ingest it into GreptimeDB. However, we ingested the entire log row as a single column in the database. In this way, if we need to filter a certain type of log, we can only rely on LIKE fuzzy queries, which is not only inefficient but also not conducive to subsequent analysis.\n\nIn production environments, logs are generally preprocessed through ETL steps, breaking down the text into structured fields before entering the database. The above-mentioned GreptimeDB built-in Pipeline mechanism is here to help.\n\nNext, we will demonstrate how to parse log text through the Pipeline to improve log usage efficiency.\n\nPipeline Configuration ​\n\nFirst, before using Pipeline, we need to write the configuration and save it to the database for GreptimeDB to invoke at runtime. Pipeline is written based on the input text data. Taking the input mock log text as an example, the following Pipeline configuration is written:\n\nyaml # pipeline.yaml processors : - dissect : fields : - message patterns : - ' %{client_ip} - %{user_identifier} [%{timestamp}] \"%{http_method} %{request_uri} %{http_version}\" %{status_code} %{response_size} ' ignore_missing : true - date : fields : - timestamp formats : - ' %d/%b/%Y:%H:%M:%S %z ' timezone : ' Asia/Shanghai ' ignore_missing : true transform : - fields : - client_ip - user_identifier type : string index : skipping - fields : - http_method - http_version type : string index : inverted - fields : - request_uri type : string index : fulltext - fields : - status_code type : int32 - fields : - response_size type : int64 - fields : - timestamp type : time index : timestamp\n\nPipeline mainly consists of processors and transform . The former processes the data, and the latter converts the processed fields into data types recognized by the database. Below we introduce each part of the Pipeline configuration:\n\nyaml processors : - dissect : fields : - message patterns : - ' %{client_ip} - %{user_identifier} [%{timestamp}] \"%{http_method} %{request_uri} %{http_version}\" %{status_code} %{response_size} ' ignore_missing : true - date : fields : - timestamp formats : - ' %d/%b/%Y:%H:%M:%S %z ' timezone : ' Asia/Shanghai ' ignore_missing : true\n\nFirst, use the dissect processor to extract fields from the long text. Fields such as client_ip and user_identifier can be extracted based on spaces. Then use the date processor to convert the time text into timestamp data type, using a parsing format and timezone:\n\nyaml transform : - fields : - client_ip - user_identifier type : string index : skipping - fields : - http_method - http_version type : string index : inverted - fields : - request_uri type : string index : fulltext - fields : - status_code type : int32 - fields : - response_size type : int64 - fields : - timestamp type : time index : timestamp\n\nNext, use transform to save the parsed fields to the database.\n\nThe syntax of transform is straightforward, just combining the corresponding fields and types;\n\nis straightforward, just combining the corresponding fields and types; Note that we add indexes to each field through index: ;\n\n; Similarly, specify index: timestamp on the timestamp field, meaning this field is set as the time index column.\n\nWe use the following command to upload the Pipeline configuration to the database, name it app_log , and save it:\n\nAfter successful execution, the following HTTP response will be returned:\n\njson { \" pipelines \" : [ { \" name \" : \" app_log \" , \" version \" : \" 2025-04-15 07:53:51.914557113 \" } ], \" execution_time_ms \" : 8 }\n\nWe can also confirm by executing select name from greptime_private.pipelines ; in the MySQL-compatible client:\n\nMore detailed Pipeline configuration options can be found in the official document.\n\nNote: GreptimeDB dashboard also provides a Pipeline online debugging tool, and we can use this page to test and debug. We can also try to leverage AI with Pipeline configuration 😛 (the above Pipeline configuration was generated by AI)\n\nTo use our custom Pipeline configuration, we need to slightly modify the Vector configuration:\n\ntoml # config.toml [ sources . file_input ] type = \" file \" include = [ \" <path_to_log_dir>/log.txt \" ] data_dir = \" <data_dir> \" ignore_checkpoints = true [ sinks . greptime_sink ] type = \" greptimedb_logs \" inputs = [ \" file_input \" ] compression = \" gzip \" dbname = \" public \" endpoint = \" http://127.0.0.1:4000 \" pipeline_name = \" app_log \" table = \" app_log_2 \"\n\nAdd ignore_checkpoints = true in the File Source to allow Vector to re-read the same file each time it runs (this is just for debugging examples);\n\nin the File Source to allow Vector to re-read the same file each time it runs (this is just for debugging examples); Modify pipeline_name = \"app_log\" to use the Pipeline configuration we just uploaded;\n\nto use the Pipeline configuration we just uploaded; Modify table = \"app_log_2\" to save the new log data to another table, avoiding conflicts with the previous app_log table .\n\nThen run vector -c <path_to_config_file> to rewrite the log data.\n\nQuery Data in GreptimeDB ​\n\nRun show tables in the MySQL-compatible client to see that the app_log_2 table has been successfully created:\n\ntoml +-----------+ | Tables | +-----------+ | app_log | | app_log_2 | | numbers | +-----------+ 3 rows in set (0.008 sec)\n\nWe can run show index from app_log_2 to observe the fields and indexes:\n\nsql + -----------+------------+----------------+--------------+-----------------+-----------+-------------+----------+--------+------+----------------------------+---------+---------------+---------+------------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression | + -----------+------------+----------------+--------------+-----------------+-----------+-------------+----------+--------+------+----------------------------+---------+---------------+---------+------------+ | app_log_2 | 1 | SKIPPING INDEX | 1 | client_ip | A | NULL | NULL | NULL | YES | greptime - bloom - filter - v1 | | | YES | NULL | | app_log_2 | 1 | INVERTED INDEX | 3 | http_method | A | NULL | NULL | NULL | YES | greptime - inverted - index - v1 | | | YES | NULL | | app_log_2 | 1 | INVERTED INDEX | 4 | http_version | A | NULL | NULL | NULL | YES | greptime - inverted - index - v1 | | | YES | NULL | | app_log_2 | 1 | FULLTEXT INDEX | 5 | request_uri | A | NULL | NULL | NULL | YES | greptime - fulltext - index - v1 | | | YES | NULL | | app_log_2 | 1 | TIME INDEX | 1 | timestamp | A | NULL | NULL | NULL | NO | | | | YES | NULL | | app_log_2 | 1 | SKIPPING INDEX | 2 | user_identifier | A | NULL | NULL | NULL | YES | greptime - bloom - filter - v1 | | | YES | NULL | + -----------+------------+----------------+--------------+-----------------+-----------+-------------+----------+--------+------+----------------------------+---------+---------------+---------+------------+ 6 rows in set ( 0 . 05 sec)\n\nWe can see that the indexes for each field have also been correctly created.\n\nThen use select * from app_log_2 limit 2 to see the following results:\n\ntoml +---------------+-----------------+-------------+------------------------------------------+--------------+-------------+---------------+---------------------+ | client_ip | user_identifier | http_method | request_uri | http_version | status_code | response_size | timestamp | +---------------+-----------------+-------------+------------------------------------------+--------------+-------------+---------------+---------------------+ | 17.61.197.240 | nikolaus3107 | HEAD | /envisioneer/efficient | HTTP/1.0 | 406 | 5946 | 2025-04-14 13:11:44 | | 56.87.252.7 | - | PATCH | /bricks-and-clicks/transition/interfaces | HTTP/1.1 | 416 | 15579 | 2025-04-14 13:11:45 | +---------------+-----------------+-------------+------------------------------------------+--------------+-------------+---------------+---------------------+ 2 rows in set (0.008 sec)\n\nAs shown here, compared to the original log line being a single column, the logs are now split into different fields, saved as separate columns with semantic data types. Now we can perform more precise searching queries on logs through conditions like where client_id = '17.61.197.240' , which not only improves query accuracy but also query efficiency.\n\nView Data Through log-query ​\n\nSimilarly, we can view the data in the app_log_2 table at http://127.0.0.1:4000/dashboard/#/dashboard/log-query, as shown below:\n\n（Figure 4: Viewing Data in app_log_2 Chart）\n\nNow that the fields are split, we can quickly filter data through the Where condition in log-query , for example:\n\n（Figure 5: Quickly Filtering Data）\n\nThis article explores log storage solutions, explaining the drawbacks of the ELK stack in current situations, and introducing GreptimeDB as a next-gen log storage solution with its advantages in architecture and user experience. Through the combination of Vector + GreptimeDB, we demonstrated the complete process from log collection to storage, parsing, and querying.\n\nIn the query section, we demonstrated data retrieval using the MySQL-compatible client and GreptimeDB's built-in log-query tool. As an open-source, highly compatible time-series database, GreptimeDB also supports seamless integration with visualization tools like Grafana, enabling users to easily visualize and analyze their data.\n\nIf you are interested in log processing and storage, feel free to modify the configuration provided in this article to explore more flexible and efficient log storage solutions.\n\nAbout Greptime ​\n\nGreptimeDB is an open-source, cloud-native database purpose-built for real-time observability. Built in Rust and optimized for cloud-native environments, it provides unified storage and processing for metrics, logs, and traces—delivering sub-second insights from edge to cloud —at any scale.\n\nGreptimeDB OSS – The open-sourced database for small to medium-scale observability and IoT use cases, ideal for personal projects or dev/test environments.\n\nGreptimeDB Enterprise – A robust observability database with enhanced security, high availability, and enterprise-grade support.\n\nGreptimeCloud – A fully managed, serverless DBaaS with elastic scaling and zero operational overhead. Built for teams that need speed, flexibility, and ease of use out of the box.\n\n🚀 We’re open to contributors—get started with issues labeled good first issue and connect with our community.\n\n⭐ GitHub | 🌐 Website | 📚 Docs\n\n💬 Slack | 🐦 Twitter | 💼 LinkedIn"
    },
    {
        "title": "With a limited supply of GPUs, how do you prioritize AI projects? Amazon uses these 8 tenets to decide who gets access.",
        "description": "Amazon has been hacking away at GPU shortages for over a year. It developed internal rules to streamline distribution, lift ROI, and enhance access.",
        "url": "https://www.businessinsider.com/amazon-tenets-ai-projects-prioritize-gpus-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-23T17:15:58Z",
        "full_text": "lighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nAmazon, like many other tech companies, has grappled with significant GPU shortages in recent years.\n\nTo address the problem, it created eight \"tenets,\" or guiding principles, for approving employee graphics processing unit requests, according to an internal document seen by Business Insider.\n\nThese tenets are part of a broader effort to streamline Amazon's internal GPU distribution process. Last year, Amazon launched \"Project Greenland,\" which one document called a \"centralized GPU orchestration platform,\" to more efficiently allocate capacity across the company. It also pushed for tighter controls by prioritizing return on investment for each AI chip.\n\nAs a result, Amazon is no longer facing a GPU crunch, which strained the company last year.\n\n\"Amazon has ample GPU capacity to continue innovating for our retail business and other customers across the company,\" an Amazon spokesperson told BI. \"AWS recognized early on that generative AI innovations are fueling rapid adoption of cloud computing services for all our customers, including Amazon, and we quickly evaluated our customers' growing GPU needs and took steps to deliver the capacity they need to drive innovation.\"\n\nHow Amazon decides who gets GPUs\n\nHere are the eight tenets for GPU allocation, according to the internal Amazon document:\n\nROI + High Judgment thinking is required for GPU usage prioritization. GPUs are too valuable to be given out on a first-come, first-served basis. Instead, distribution should be determined based on ROI layered with common sense considerations, and provide for the long-term growth of the Company's free cash flow. Distribution can happen in bespoke infrastructure or in hours of a sharing/pooling tool. Continuously learn, assess, and improve: We solicit new ideas based on continuous review and are willing to improve our approach as we learn more. Avoid silo decisions: Avoid making decisions in isolation; instead, centralize the tracking of GPUs and GPU related initiatives in one place. Time is critical: Scalable tooling is a key to moving fast when making distribution decisions which, in turn, allows more time for innovation and learning from our experiences. Efficiency feeds innovation: Efficiency paves the way for innovation by encouraging optimal resource utilization, fostering collaboration and resource sharing. Embrace risk in the pursuit of innovation: Acceptable level of risk tolerance will allow to embrace the idea of 'failing fast' and maintain an environment conducive to Research and Development. Transparency and confidentiality: We encourage transparency around the GPU allocation methodology through education and updates on the wiki's while applying confidentiality around sensitive information on R&D and ROI sharable with only limited stakeholders. We celebrate wins and share lessons learned broadly. GPUs previously given to fleets may be recalled if other initiatives show more value. Having a GPU doesn't mean you'll get to keep it."
    },
    {
        "title": "Microsoft beats quarterly revenue estimates as AI shift bolsters cloud demand",
        "description": "The results are likely to ease concerns about a potential slowdown in AI demand, after some analysts pointed to canceled data center leases at Microsoft as a...",
        "url": "https://finance.yahoo.com/news/microsoft-beats-quarterly-revenue-estimates-200606536.html",
        "source": "Yahoo Entertainment",
        "publishedAt": "2025-04-30T20:06:06Z",
        "full_text": "By Deborah Mary Sophia, Stephen Nellis and Aditya Soni\n\n(Reuters) - Microsoft forecast on Wednesday stronger-than-expected quarterly growth for its cloud-computing business Azure after blowout results in the latest quarter, calming investor worries in an uncertain economy and lifting its shares 7% after hours.\n\nMicrosoft's results, which follow similar outcomes from Google last week, could ease concerns about a potential slowdown in AI demand, after some analysts pointed to canceled data-center leases at Microsoft as a sign of excess capacity.\n\nInvestors had also been worried about the fallout from sweeping U.S. tariffs that are prompting businesses to rein in spending, but robust advertising sales at Meta Platforms suggested that is so far not happening.\n\nThe rise in Microsoft's shares set it on course to add more than $200 billion to its value.\n\nMicrosoft said revenue at its Azure cloud division rose 33% in the third quarter ended March 31, exceeding estimates of 29.7%, according to Visible Alpha. AI contributed 16 percentage points to the growth, up from 13 points in the previous quarter.\n\nThe company also forecast cloud-computing revenue growth of 34% to 35% on a constant currency basis for the fiscal fourth quarter to between $28.75 billion and $29.05 billion, well above analyst estimates, according to data from Visible Alpha.\n\nCommercial bookings growth - which reflects new infrastructure and software contracts signed by business customers - rose 18% in the fiscal third quarter, driven in part by a new Azure contract with ChatGPT creator OpenAI. Microsoft declined to comment on the size of the deal or what role it played in overall Azure sales growth.\n\nHowever, Amy Hood, Microsoft's chief financial officer, told investors on a conference call that the AI contribution to the cloud computing business was in line with the company's expectations, while \"the real outperformance in Azure this quarter was in our non-AI business.\"\n\n\"So the only real upside we saw on the AI side of the business was that we were able to deliver supply early to a number of customers,\" Hood said.\n\nThe company's Azure results came after a number of Wall Street analysts had lowered expectations as research reports said Microsoft had ended some data center lease obligations.\n\nCEO Satya Nadella said on a conference call that Microsoft has a long history of constantly adjusting its data center plans, but only in recent quarters had analysts started closely scrutinizing those moves.\n\n\"The numbers were skeptical going in, giving them the room to beat pretty heavy. The beat wouldn't have been this big if we didn't have all these problems,\" said Dan Morgan, senior portfolio manager at Synovus Trust, referring to tariff uncertainty."
    },
    {
        "title": "Axiom Space to launch its 1st orbiting data centers this year",
        "description": "Axiom Space will launch two orbiting data center nodes into low Earth orbit by the end of this year, as the first step in the development of off-Earth computing infrastructure.",
        "url": "https://www.space.com/space-exploration/private-spaceflight/axiom-space-to-launch-its-1st-orbiting-data-centers-this-year",
        "source": "Space.com",
        "publishedAt": "2025-04-14T10:00:00Z",
        "full_text": "Axiom Space will launch two orbiting data center nodes into low Earth orbit by the end of this year, as the first step in the development of off-planet computing infrastructure.\n\nThe two satellites will be part of the upcoming optical relay constellation by Canada-headquartered Kepler Communications, which is expected to begin launching in late 2025.\n\nThe pioneering satellites will be used to process data from Earth -observation satellites, using complex AI and machine-learning algorithms to speed up the delivery of valuable insights to users on the ground.\n\nCurrently, satellites need to beam their images to Earth for processing, which introduces delays. Analyzing data directly in orbit will also make the use of available bandwidth more efficient, as only images containing the requested information will be sent down. Bypassing the need to downlink data to ground stations scattered around the world will also mitigate security concerns, such as possible interception of data by adversary actors.\n\nRelated: Axiom Space: Building the off-Earth economy\n\n\"Our Orbital Data Center (OCD) nodes will soon be open for business,\" Kam Ghaffarian, CEO, executive chairman and co-founder of Houston-based Axiom Space , said in a statement . \"We have agreements in place with users around the world to deploy initial, space-based cloud services, not just demonstrations of capabilities.\"\n\nIn 2023, Axiom Space announced plans to deploy an orbital data center on board its planned space station , which is expected to launch in 2027. The company has now revealed a vision for a more distributed orbital computing infrastructure, which will take advantage of Kepler Communications satellites to move computing to space.\n\nGet the Space.com Newsletter Breaking space news, the latest updates on rocket launches, skywatching events and more! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nAlthough the first orbiting data centers will exclusively crunch data collected in space, some technologists think that power-hungry computing infrastructure would in the future do better in space, where solar power is constant and cooling is easy. On top of that, the growing need for data centers on Earth is putting pressure on the real estate market and energy grids, affecting local communities\n\nThe first two nodes of Axiom Space's planned orbital cloud-computing system, dubbed ODC 1 and 2, will be placed on two of the 10 upcoming Kepler Communications satellites, each of which weighs 573 pounds (260 kilograms).\n\nSatellites connected to the data centers will be able to operate more autonomously, without the need of constant oversight by Earth-based ground controllers.\n\nAxiom Space has previously tested a demonstration payload based on Amazon Web Services' mini edge computer Snowcone at the International Space Station . In March, Axiom announced it would launch a larger edge computer, the Data Center Unit-1 (AxDCU-1), to the space station in the coming months. The AxDCU-1 payload will help the company test hardware and software before the deployment of the ODCs.\n\n\"ODC Nodes 1 and 2 are an acceleration of our plans to deploy a multitude of free-flyer ODC nodes in low Earth orbit to meet the rapidly emerging demand from users around the world,\" Jason Aspiotis, global director of in-space data and security at Axiom Space, said in the statement.\n\nThe ODCs will be connected via high-speed 2.5 Gbps laser links to the other satellites of the Kepler Communications constellations, as well to data centers on Earth.\n\nBefore orbital data centers can take off in a big way, technologists must make sure that the technology can withstand the harsh environment of space, especially the strong radiation that frequently shortens the lifespan of electronic devices."
    },
    {
        "title": "'I paid for the whole GPU, I am going to use the whole GPU'",
        "description": "A guide to maximizing the utilization of GPUs, from cloud allocations to FLOP/s.",
        "url": "https://modal.com/blog/gpu-utilization-guide",
        "source": "Modal.com",
        "publishedAt": "2025-05-07T21:04:39Z",
        "full_text": "Typical attire of a GPU utilization maximizer.\n\nGraphics Processing Units, or GPUs, are the hottest mathematical co-processor since the FM synthesis chips that shaped the sounds of the late 1900s .\n\nLike all co-processors, they are chosen when the performance of more flexible commodity hardware, like an x86 Central Processing Unit (CPU), is insufficient. GPUs are in particular designed for problems where CPUs cannot achieve the desired throughput of mathematical operations (in particular, matrix multiplications).\n\nBut GPUs are not cheap: high performance can command a high price.\n\nCombined together, the high price, performance sensitivity, and throughput-orientation of GPU applications mean that a large number of engineers and technical leaders find themselves concerned with GPU utilization of some form or another — “we’re paying a lot, so we’d better be using what we’re paying for”.\n\nAt Modal, we have our own GPU utilization challenges to solve and we help our users solve theirs. We’ve noticed that the term “GPU utilization” gets used to mean very different things by people solving problems at different parts of the stack. So we put together this article to share our framework for thinking about GPU utilization across the stack and the tips and tricks we’ve learned along the way.\n\nIn particular, we’ll talk about three very different things that all get called “GPU utilization”:\n\nGPU Allocation Utilization , the fraction of your GPUs that are running application code,\n\nGPU Kernel Utilization , the fraction of time your application is running code on GPUs, and\n\nModel FLOP/s Utilization , the fraction of the GPUs’ theoretical arithmetic bandwidth your application is using to run models.\n\nWe’ll specifically focus on neural network inference workloads — neural networks because they are workload generating the most demand right now and inference because, unlike training, inference is a revenue center not a cost center. We’re betting on the revenue center.\n\nWhat is utilization?\n\nUtilization = Output achieved ÷ Capacity paid for\n\nUtilization relates the available capacity of a system to that system’s output.\n\nIn throughput-oriented systems like GPU applications, the capacity paid for is often a bandwidth (e.g. the arithmetic bandwidth) and the output achieved is then a throughput (e.g. floating point operations per second, FLOP/s).\n\nBecause it is a ratio, utilization is unitless. That means there are actually many GPU-related quantities you might call “GPU utilization”, leaving off the implicit units of the capacity and output. These different quantities range across orders of magnitude of time and across different organizational capacities (e.g. procurement, DevOps, and low-level performance engineering).\n\nWhat is GPU Allocation Utilization?\n\nGPU Allocation Utilization = GPU-seconds running application code ÷ GPU-seconds paid for\n\nFirst, consider the number of GPUs that you have allocated — whether that is fixed GPU capacity on-premise in your basement (or data center) or it is rented capacity in a cloud data center (or many people’s basements) — across a period of time.\n\nWe use the term GPU Allocation Utilization for the fraction of those GPU-seconds during which you were running application code. This is the highest-level notion of “GPU utilization”.\n\nThere are two key limits on GPU Allocation Utilization: economic and developer-operational.\n\nThe economic limits on GPU Allocation Utilization rise from combined technical and market limitations. Purchasing, commissioning, decomissioning, and selling GPUs cannot be done as quickly as the output demanded by the application changes (on the scale of seconds or minutes).\n\nOf course, as for other hardware we are blessed with highly-virtualized data center platforms (“clouds”) where we can virtually allocate and de-allocate GPU capacity. Even there, however, existing pricing models and demand that exceeds supply leave providers dictating terms, like multi-month or multi-year commitments, which limit achievable utilization for a given quality-of-service.\n\nWith a fixed, over-provisioned GPU allocation, utilization is low Application Demand Provisioned\n\nModal helps organizations solve this problem. We aggregate GPU demand across consumers and GPU supply across providers to improve GPU allocation efficiency.\n\nBut GPU Allocation Utilization isn’t just about the GPU-seconds paid for, it’s about the GPU-seconds spent running application code.\n\nThat’s where the DevOps limits on GPU Allocation Utilization come in. Even in a fully liquid GPU market, there is latency between the time at which a GPU is purchased or rented and the time at which the GPU is running useful work — time to configure operating systems, perform health checks, copy over application code, etc. Absent the ability to precisely predict future demand at timescales greater than that latency, this leads to reduced GPU Allocation Utilization, reduced quality-of-service, or both!\n\nIf allocation is slow, utilization and QoS suffer Application Demand Provisioned\n\nTo achieve high GPU Allocation Utilization and meet quality-of-service goals, allocation and spin-up to application code needs to be fast enough to respond to increases in demand.\n\nWith fast, automatic allocation, utilization and QoS can both be high Application Demand Provisioned\n\nThis is one of the core problems solved by Modal. We manage a large multi-cloud GPU fleet, benefitting from economies of scale to unlock better engineering solutions and concentration of measure to improve predictability of demand. We built a custom container stack (in Rust btw) to reduce the latency from non-application code and system configuration. And users’ workloads spin up faster because the serverless runtime for that container execution system frames user workloads in terms of application code, not virtual machine maintenance. That allows us to skip the repetitive, undifferentiated work required to create virtual machines. That unlocks novel engineering optimizations for us, like memory snapshotting and restoration , and it just-so-happens to make application engineering easier for our users.\n\nWhat level of GPU Allocation Utilization can I expect to achieve?\n\nThe existing numbers are sobering. According to the State of AI Infrastructure at Scale 2024 report , the majority of organizations achieve less than 70% GPU Allocation Utilization when running at peak demand — to say nothing of aggregate utilization. This is true even of sophisticated players, like the former Banana serverless GPU platform , which operated at an aggregate utilization of around 20%.\n\nWith Modal, users can achieve GPU Allocation Utilization in excess of 90% — in aggregate, not just at peak.\n\nIf that interests you, check out our docs and our pricing page .\n\nIf it doesn’t, read on for more about the software engineering required to get the most out of your GPUs — on Modal or elsewhere.\n\nWhat is GPU Kernel Utilization?\n\nGPU Kernel Utilization = GPU-seconds running kernels ÷ GPU-seconds paid for\n\nJust because an allocated GPU is running application code doesn’t mean it is running code on the GPU. The term of art for “code that runs on the GPU” in the popular CUDA programming model for GPUs is “kernel”, and so we call the fraction of time we spend running code on the GPU the GPU Kernel Utilization.\n\nThis utilization metric is reported by, among others, the beloved nvidia-smi command line tool wrapping NVIDIA’s Management Library for their GPU hardware, and so it is commonly checked and cited. We expose it to our users under the name that library uses, “GPU utilization”. Note that this name can be slightly misleading, since this metric does not care whether the code we’re running on the GPU is exercising the hardware’s actual capacity.\n\nAn application that is achieving low GPU Allocation Utilization is necessarily going to achieve low GPU Kernel Utilization, so long as you consider all GPU-seconds being paid for: a unit not running application code can’t run kernels.\n\nWhy else might you achieve low GPU Kernel Utilization? In particular, what patterns will show up as low kernel utilization per GPU?\n\nFirst, there might be lots of work to do that supports your application but doesn’t use the GPU, like moving input or output data via network or disk, downloading the many gigabytes of weights of a foundation model, or writing logs.\n\nThese tasks can be sped up by usual means — judicious application of lazy and eager loading, parallelization, increased bandwidth for non-GPU components like networks, and deleting more code YAGN .\n\nSecond, the CPU might not be providing work to the GPU quickly enough. A typical GPU-accelerated program is, like a high-performance network application, a dance of concurrency between the CPU executing logic about what work must be done and specialized, but dumb, hardware that can actually do the work. For example, when multiplying two matrices, the popular PyTorch library needs to determine the shapes and types of those two matrices and then lookup the appropriate kernel — somewhat akin to a JIT database query optimizer selecting a physical operator mid-execution. If you are unable to complete this work before the GPU finishes its previous task, the GPU will idle. We’ll call this class of issue “host overhead”.\n\nOften, resolving host overhead is a matter of re-writing the host logic — preventing slow host work (like logging in Python) from blocking the host work that drives the GPU. But at the scale of milliseconds per task step, Python starts to become incapable of keeping up, and at the scale of microseconds per task step, the latency required to schedule kernels onto the GPU via the CUDA C++ APIs and driver begins to bottleneck.\n\nIn both cases, there are two basic optimizations. First, multiple kernels can be launched at once using CUDA Graphs , which essentially convert a sequence of kernel launches into a DAG that only needs to be launched once. Second, the application can aggregate more work for the GPU to complete for a given unit of host work — for example by batching requests together — to improve utilization with a possible penalty to latency.\n\nCode regions with low GPU Kernel Utilization can be identified from application traces, like those produced by the PyTorch Profiler . Specifically, any period of time where all CUDA streams are empty is a period of zero GPU Kernel Utilization, and so applications with low GPU Kernel Utilization have largely empty CUDA streams in their traces, like the one below. These periods of quiescence need to be correlated to activity on the host to determine which parts of the application code are leading to the bottleneck. GPU application profilers and trace viewers generally support this, e.g. by showing kernel launch dependencies, like the arrow in the trace below.\n\nIn traces of GPU applications, periods where no kernels are running appear as empty strips in the timelines of CUDA streams (e.g. Stream 7 7 in the trace above). For details, see our documentation .\n\nWhat level of GPU Kernel Utilization can I hope to achieve?\n\nGPU Kernel Utilization is the closest metric in this article to the better-known CPU utilization. CPU utilization tracks the fraction of CPU cycles during which instructions were being executed on behalf of your program (as opposed to the CPU idling or running other programs).\n\nHowever, for CPU utilization, hitting 90%+ is often bad, even a trigger for alerts. But we want to and can achieve that level of GPU Kernel Utilization!\n\nFundamentally, this is downstream of the greater predictability of many GPU applications. Running a transactional database replica at 90% CPU utilization baseline risks degraded quality-of-service if query patterns or quantity change. Typical GPU applications have much less variability — for a database analogue, imagine repeatedly running only one basic sequential scan aggregation query, but with slightly different parameters each time — and so have more controllable quality-of-service.\n\nWhat is Model FLOP/s Utilization (MFU)?\n\nModel FLOP/s Utilization = Model FLOP/s throughput achieved ÷ FLOP/s bandwidth paid for\n\nAt some galaxy-brained, CEO-math level, expenditures on GPUs are really expenditures on floating point operation bandwidth, and so the deepest and most fundamental utilization metric to measure is the ratio of that bandwidth to the throughput achieved.\n\nThis metric is known as MFU, which either means “Maximum” or “Model” FLOP/s Utilization, depending on who you ask. We go with “Model”, since it’s more common.\n\nInstances that aren’t running application code or that aren’t running GPU kernels cannot achieve a high MFU, so low GPU Allocation Utilization or low GPU Kernel Utilization imply low Model FLOP/s Utilization.\n\nHowever, high utilization at these more abstract levels does not imply high MFU.\n\nFirst, as an implementation detail, communication between GPUs is frequently implemented via GPU kernels. This communication, like most communication in distributed systems, is subject to faults (hardware fault, programmer fault, shark attack fault ), which frequently manifest as deadlock. From the perspective of GPU Kernel Utilization, a system that is deadlocked in the middle of running a communication kernel is fully utilized (!), but it is completing no useful work. We like to catch this particular issue by monitoring GPU power draw and heat . More generally, optimizing communication is critical for achieving high MFU, especially for workloads that spread a single task across multiple nodes.\n\nSecond, floating point computation is just one of the things a GPU must do to complete a task. The most important other task is moving data. Computation can only occur on data stored inside of the register file of the GPU’s streaming multiprocessors , which each store less than a megabyte, while foundation models are measured in gigabytes. The data to which a computation applies must generally be moved from a slower, larger area of the memory hierarchy . The bandwidth of this memory is generally many times lower than the device’s FLOP/s bandwidth, especially in recent generations. The ratio of an algorithm’s FLOP/s throughput to its byte/s throughput is called the arithmetic intensity.\n\nBottlenecking on memory is a particular challenge in latency-sensitive foundation model inference workloads, where the arithmetic intensity is low (perhaps a few FLOPs per byte). Besides algorithmic rewrites to increase arithmetic intensity, like the online softmax in Flash Attention , the primary generic strategy is batching more work together, which increases FLOPs executed more than memory bytes moved for most neural network inference workloads, but generally adds per-task latency.\n\nFinally, GPU kernels must be carefully written to achieve high MFU. This public worklog by Si Boehm gives a flavor for the effort required to reach state-of-the-art for a single kernel. Even that worklog stops short of truly maximizing MFU, since it tackles a problem that can’t make use of the fastest elements of contemporary GPUs, the Tensor Cores , and writing kernels that can saturate Tensor Cores is even more challenging — see this worklog from Pranjal Shankhdhar . For this reason, most teams use high-quality open source kernels through libraries like CuBLAS or frameworks like PyTorch and vLLM.\n\nThe achieved FLOP/s and memory throughput of a GPU application can be monitored using the NVIDIA Data Center GPU Management tool , dcgm . The metrics prefixed with DCGM_FI_PROF are generally relevant. In particular, the DCGM_FI_PROF_DRAM_ACTIVE metric measures the utilization of the DRAM-to-SRAM memory bandwidth. The DCGM_FI_PROF_PIPE_TENSOR_ACTIVE metric measures the utilization of the Tensor Cores that provide the maximum FLOP/s bandwidth. This isn’t identical to MFU for subtle reasons covered well in Stas Bekman’s guide here .\n\nWhat level of Model FLOP/s Utilization can I hope to achieve?\n\nFirst, let’s note that measuring Model FLOP/s Utilization is tricky. The theoretical bandwidth can be read from manufacturer datasheets — but watch for asterisks like “with sparsity”. The achieved model throughput, on the other hand, can be hard to measure, in particular since some FLOPs might be spent on other computations, like activation recomputation in training. For that reason, it is often done based on pen-and-paper analysis of the algorithm and with approximate, “napkin” math.\n\nThe state-of-the-art for MFU in training is achieved by the foundation model teams at leading organizations like OpenAI, Google, and Meta. Of these, Meta is the most open and reports an MFU of 38 - 41% when training the LLaMA 3 405B model . The more recent DeepSeek-v3 training run by DeepSeek achieved around 20-30% MFU (there’s no official number) using GPUs with tighter communication bottlenecks .\n\nMuch of the shortfall is due to the need for inter-node communication in large training jobs, which creates bandwidth constraints that aren’t present in inference applications. For inference workloads, MFU might reach higher, closer to the 70% - 80% MFU achieved by raw matrix multiplications , but we aren’t aware of any published results from large-scale deployments. Let us know if we missed them!\n\nFor context, it’s also helpful to consider the equivalent of MFU for a job running on a CPU. For concreteness, consider the One Billion Row Challenge , which led teams around the world to competitively optimize a large-scale aggregation problem on CPUs. This problem requires three floating point operations per row on one billion rows, and so has a total FLOP count of 3 billion. The leading results finished in about one second, and so achieved a FLOP/s throughput of about 3 billion. If we assume that the hardware used for the challenge, eight cores out of a 32 core AMD EPYC 7502P machine which can run at 3.35 GHz, is capable of issuing one FLOP per cycle, then the FLOP/s bandwidth is ~26 billion, for an MFU of ~10%. However, that CPU has AVX2 SIMD vector instructions with a lane width of 256 and so, assuming it can issue 16 FLOPs/cycle per core, the FLOP/s bandwidth is actually ~420 billion, leading to an MFU of under 1%.\n\nHow can I improve my GPU utilization?\n\nIf you’re not using Modal , that’s a great place to start! Especially for GPU Allocation Utilization.\n\nBesides that, we recommend that if you want to improve your GPU utilization, you dive deeper into GPU-based computing.\n\nWe wrote a GPU Glossary to collect together our definitions of the most important terms in one place, complete with links to some of our favorite resources for learning more. Try starting there!\n\nAmong those resources, a few stand out, like this talk by Horace He , of the PyTorch team, and this dense blog post by Abhinav Upadhyay of Coding Confessions. We also highly recommend the ML Engineering Open Book by Stas Bekman for deep dives and useful snippets all across the stack.\n\nWe’d like to thank Mark Saroufim of PyTorch & the GPU_MODE Discord (join it!) and Erik Dunteman of Pig for comments on a draft of this post."
    },
    {
        "title": "'Project Greenland': How Amazon overcame a GPU crunch",
        "description": "How Amazon's big retail business navigated GPU shortages by emphasizing ROI and tightening its approval process to balance internal supply and demand.",
        "url": "https://www.businessinsider.com/amazon-strategy-overcome-gpu-shortages-nvidia-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-22T09:00:02Z",
        "full_text": "Your browser does not support the video element.\n\nlighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nLast year, Amazon's huge retail business had a big problem: It couldn't get enough AI chips to get crucial work done.\n\nWith projects getting delayed, the Western world's largest e-commerce operation launched a radical revamp of internal processes and technology to tackle the issue, according to a trove of Amazon documents obtained by Business Insider.\n\nThe initiative offers a rare inside look at how a tech giant balances internal demand for these graphics processing units with supply from Nvidia and other industry sources.\n\nEarly last year, the generative-AI boom was in full swing, with thousands of companies vying for access to the infrastructure needed to apply this powerful new technology.\n\nInside Amazon, some employees went months without securing GPUs, leading to delays that disrupted timely project launches across the company's retail division, a sector that spans its e-commerce platform and expansive logistics operations, the internal documents said.\n\nIn July, Amazon launched \"Project Greenland,\" a \"centralized GPU capacity pool\" to better manage and allocate its limited GPU supply, one document said. The company also tightened approval protocols for internal GPU use, the documents said.\n\n\"GPUs are too valuable to be given out on a first-come, first-served basis,\" one of the Amazon guidelines said. \"Instead, distribution should be determined based on ROI layered with common sense considerations, and provide for the long-term growth of the Company's free cash flow.\"\n\nTwo years into a global shortage, GPUs remain a scarce commodity —even for some of the largest artificial intelligence companies. OpenAI CEO Sam Altman, for example, said in February that the ChatGPT maker was \"out of GPUs\" following a model launch. Nvidia, the dominant GPU provider, has said it will be supply-constrained this year.\n\nHowever, Amazon's efforts to tackle this problem may be paying off. By December, internal forecasts suggested the crunch would ease this year, with chip availability expected to improve, the documents showed.\n\nIn an email to BI, an Amazon spokesperson said the company's retail arm, which sources GPUs through Amazon Web Services, now has full access to the AI processors.\n\n\"Amazon has ample GPU capacity to continue innovating for our retail business and other customers across the company,\" the spokesperson said. \"AWS recognized early on that generative AI innovations are fueling rapid adoption of cloud computing services for all our customers, including Amazon, and we quickly evaluated our customers' growing GPU needs and took steps to deliver the capacity they need to drive innovation.\"\n\n'Shovel-ready'\n\nAmazon CEO Andy Jassy. Amazon\n\nAmazon now demands hard data and return-on-investment proof for every internal GPU request, the documents obtained by BI said.\n\nInitiatives are \"prioritized and ranked\" for GPU allocation based on several factors, including the completeness of data provided and the financial benefit per GPU, a document said. Projects must be \"shovel-ready,\" or approved for development, and prove they are in a competitive \"race to market,\" it added. They also have to provide a timeline for when benefits are expected to be realized.\n\nOne internal document from late 2024 said Amazon's retail unit planned to distribute GPUs to the \"next highest priority initiatives\" as more supply became available in the first quarter of 2025.\n\nThe broader priority for Amazon's retail business is to ensure its cloud infrastructure spending generates the \"highest return on investment through revenue growth or cost-to-serve reduction,\" one of the documents added.\n\nAmazon's new GPU 'tenets'\n\nAmazon's retail team codified its approach into official \"tenets\" — internal guidelines that individual teams or projects create for faster decision-making. The tenets emphasize a strong return on investment, selective approvals, and a push for speed and efficiency.\n\nRelated stories Business Insider tells the innovative stories you want to know Business Insider tells the innovative stories you want to know\n\nAnd if a greenlighted project underdelivers, its GPUs can be pulled back.\n\nHere are the eight tenets for GPU allocation, according to one of the Amazon documents:\n\nROl + High Judgment thinking is required for GPU usage prioritization. GPUs are too valuable to be given out on a first-come, first-served basis. Instead, distribution should be determined based on ROl layered with common sense considerations, and provide for the long-term growth of the Company's free cash flow. Distribution can happen in bespoke infrastructure or in hours of a sharing/pooling tool. Continuously learn, assess, and improve: We solicit new ideas based on continuous review and are willing to improve our approach as we learn more. Avoid silo decisions: Avoid making decisions in isolation; instead, centralize the tracking of GPUs and GPU related initiatives in one place. Time is critical: Scalable tooling is a key to moving fast when making distribution decisions which, in turn, allows more time for innovation and learning from our experiences. Efficiency feeds innovation: Efficiency paves the way for innovation by encouraging optimal resource utilization, fostering collaboration and resource sharing. Embrace risk in the pursuit of innovation: Acceptable level of risk tolerance will allow to embrace the idea of 'failing fast' and maintain an environment conducive to Research and Development. Transparency and confidentiality: We encourage transparency around the GPU allocation methodology through education and updates on the wiki's while applying confidentiality around sensitive information on R&D and ROI sharable with only limited stakeholders. We celebrate wins and share lessons learned broadly. GPUs previously given to fleets may be recalled if other initiatives show more value. Having a GPU doesn't mean you'll get to keep it.\n\nProject Greenland\n\nAmazon Web Services CEO Matt Garman. Amazon\n\nTo address the complexity of managing GPU supply and demand, Amazon launched a project called Greenland last year.\n\nGreenland is described in one of the documents as a \"centralized GPU orchestration platform to share GPU capacity across teams and maximize utilization.\"\n\nIt can track GPU usage per initiative, share idle servers, and implement \"clawbacks\" to reallocate chips to more urgent projects, the documents said. The system also offers a simplified networking setup and security updates, while alerting employees and leaders to projects with low GPU usage.\n\nThis year, Amazon employees are \"mandated\" to go through Greenland to obtain GPU capacity for \"all future demands,\" the company said, and Amazon expects this to increase efficiency by \"reducing idle capacity and optimizing cluster utilization.\"\n\n$1 billion investment in AI-related projects\n\nAmazon's retail business is wasting no time putting its GPUs to work. One document listed more than 160 AI-powered initiatives, including the Rufus shopping assistant and Theia product-image generator.\n\nOther AI projects in the works include, per the document:\n\nA vision-assisted package retrieval (VAPR) service that uses computer-vision technology to help drivers quickly identify and pick the correct packages from vans at delivery stops.\n\nA service that automatically pulls in data from external websites to create consistent product information.\n\nA new AI model that optimizes driver routing and package handling to reduce delivery times and improve efficiency.\n\nAn improved customer service agent that uses natural language to address customer return inquiries.\n\nA service that automates seller fraud investigations and verifies document compliance.\n\nLast year, Amazon estimated that AI investments by its retail business indirectly contributed $2.5 billion in operating profits, the documents showed. Those investments also resulted in about $670 million in variable cost savings, they said.\n\nThe 2025 estimates for those metrics are unclear. But Amazon plans to continue spending heavily on AI.\n\nAs of early this year, Amazon's retail arm anticipated about $1 billion in investments for GPU-powered AI projects. Overall, the retail division expects to spend about $5.7 billion on AWS cloud infrastructure in 2025, up from $4.5 billion in 2024, the internal documents showed.\n\nImproving capacity\n\nLast year, Amazon's heavy slate of AI projects put pressure on its GPU supply.\n\nThroughout the second half of 2024, Amazon's retail unit suffered a supply shortage of more than 1,000 P5 instances, AWS's cloud server that contains up to eight Nvidia H100 GPUs, one of the documents from December said. The P5 shortage was expected to slightly improve by early this year and turn to a surplus later in 2025, according to those December estimates.\n\nAmazon's spokesperson told BI those estimates were now \"outdated\" and there's no GPU shortage.\n\nAWS's in-house AI chip Trainium was also projected to satisfy the retail division's demand by the end of 2025 but \"not sooner,\" one of the documents said.\n\nAmazon's improving capacity aligns with CEO Andy Jassy's remarks from February, when he said the GPU and server constraints would \"relax\" by the second half of this year.\n\nBut even with these efforts, there are signs that Amazon still worries about GPU supply.\n\nA recent job listing from the Greenland team acknowledged that explosive growth in GPU demand has become this generation's defining challenge: \"How do we get more GPU capacity?\""
    },
    {
        "title": "This Samsung Chromebook Is Now Just $82, Down 63% From Its Original Refurbished Price",
        "description": "This deal ranks among the best offers ever available on a renewed Chromebook.",
        "url": "https://gizmodo.com/this-samsung-chromebook-is-now-just-82-down-63-from-its-original-refurbished-price-2000594670",
        "source": "Gizmodo.com",
        "publishedAt": "2025-04-26T11:45:24Z",
        "full_text": ""
    },
    {
        "title": "The Sunday Papers",
        "description": "Sundays are for ousting your editor from his throne while he's on holiday. Here's a roundup of some good writing from the internet last week.\nThis list of Games Writing Words I Hate by Riley MacLeod at Aftermath sets out some words games journalists would do …",
        "url": "https://www.rockpapershotgun.com/the-sunday-papers-759",
        "source": "Rock Paper Shotgun",
        "publishedAt": "2025-04-13T10:00:00Z",
        "full_text": "The Sunday Papers is our weekly roundup of great writing about (mostly) videogames from across the web.\n\nSundays are for ousting your editor from his throne while he's on holiday. Here's a roundup of some good writing from the internet last week.\n\nThis list of Games Writing Words I Hate by Riley MacLeod at Aftermath sets out some words games journalists would do well to avoid. I wrote a similar list over ten years ago, and it's interesting to see many of the examples (\"immersive\", \"IP\", \"franchise\") are still used by enough writers to merit continuing complaints from professional editors like MacLeod.\n\n\"A lot of video games writing words veer close to marketing words; while I don’t consider journalists the enemies of PR per se, we’re doing very different jobs, and the words we choose can make that clear. When marketing words creep into our vocabulary, we blur a boundary fundamental to journalists’ independence, and we also miss out on the opportunity to say the things that we as journalists are uniquely situated to say. Leave the press release talk in your inbox.\"\n\n(As long as we're being pedantic, I'd advise people not to use \"games writing\" to refer to journalism, because it can be confused with writing for games. But never mind, MacLeod's points on usage are sound.)\n\nPacific Drive Feels Like Storm Chasing, writes Kaile Hultner of No Escape. I like this interpretation of the driving roguelike. My go-to analogy would have been \"really shitty road trip in a busted-ass vehicle\" but a storm chase feels true to the spirit of the game. (Disclosure: Paul Dean, one of Pacific Drive's writers, has written for RPS).\n\n\"Sometimes – most of the time – storm chasing is hours of driving, navigating unfamiliar dirt paths and poorly-maintained pavement, constantly checking your route map and the GPS and Doppler Radar to make sure you’re not about to run into any, hmm, instability. Storm chasing can be incredibly dangerous, which is why preparation is so vital and why there are rules about how you do it. Nothing is stopping any yahoo from hopping in their 1996 Honda Civic and barging into the Oklahoma City metro area every late spring and mid-fall for a hoot, but smart chasers train first.\"\n\nDigital affairs writer Gerry McGovern sets out a case that our hoarding of data is an increasingly wasteful compulsion that benefits few and has a real and detrimental impact on the planet.\n\n\"We’re destroying our environment to store copies of copies of copies of stuff we have no intention of ever looking at again. We’re destroying our environment to take 1.9 trillion photos every year. That’s more photos taken in one single year in the 2020s than were taken in the entire 20th century. That more than 200 photos taken for every child, woman and man alive. Every year. 12 trillion photos and growing, stored in the Cloud, the vast majority of which will never be viewed again. Mind boggling and exactly how Big Tech wants it.\"\n\nWhat else lives in the cloud? Yup, it's AI. In Bubble Trouble tech writer Bryan McMahon makes the case that generative AI is a market bubble and points out that one of the technology's biggest companies, OpenAI, is incinerating money at an astonishing rate, with no prospect of a return, while China's own versions of the technology are much cheaper to run and use. Some points in this piece are familiar and obvious to anyone who knows snake oil when they smell it, but it's nice to have an instinct backed up by figures.\n\nTo start, OpenAI is burning money at an impressive but unsustainable pace. The latest funding round is its third in the last two years, atypical for a startup, that also included a $4 billion revolving line of credit—a loan on tap, essentially—on top of the $6.6 billion of equity, revealing an insatiable need for investor cash to survive. Despite $3.7 billion in sales this year, OpenAI expects to lose $5 billion due to the stratospheric costs of building and running generative AI models, which includes $4 billion in cloud computing to run their AI models, $3 billion in computing to train the next generation of models, and $1.5 billion for its staff. According to its own numbers, OpenAI loses $2 for every $1 it makes, a red flag for the sustainability of any business.\n\nLook at these poor ants. Is their endlessly circling pheromonal deathloop a metaphor for something we humans also experience? It might be, but damned if I'm the one making the connection. It's Sunday, I'm not working today.\n\nMusic this week is If U C My Enemies by Rubblebucket. Not video games enough for ya? Okay, have the entire soundtrack of South By Midnight. Oh aye. That'll do."
    },
    {
        "title": "Google wants Gemini AI deal with Apple by mid-2025",
        "description": "Google CEO Sundar Pichai revealed in court testimony that the company hopes to strike a deal with Apple by mid-2025 to bring its Gemini AI technology to Apple Intelligence.Apple IntelligenceDuring a federal antitrust trial on April 29, Pichai said Google is i…",
        "url": "https://appleinsider.com/articles/25/04/30/google-wants-gemini-ai-deal-with-apple-by-mid-2025",
        "source": "AppleInsider",
        "publishedAt": "2025-04-30T16:39:24Z",
        "full_text": "Google CEO Sundar Pichai revealed in court testimony that the company hopes to strike a deal with Apple by mid-2025 to bring its Gemini AI technology to Apple Intelligence.\n\nDuring a federal antitrust trial on April 29, Pichai said Google is in discussions with Apple to integrate Gemini into Apple Intelligence, the AI system expected to power upcoming versions of iOS, iPadOS, and macOS. The testimony was first reported by Reuters.\n\nGemini is Google's family of AI models, developed by Alphabet's DeepMind unit. It includes Gemini Nano, optimized for mobile devices, and Gemini Ultra for high-performance computing. The system is capable of summarizing content, generating code, and performing complex language tasks.\n\nCompared to Apple's current on-device models, Gemini promises broader reasoning and more fluid interactions in both text and voice, though the extent of any integration remains unclear.\n\nWhat Gemini AI could do for iPhone users\n\nFor iPhone users, Gemini could enable smarter everyday experiences. A Gemini-enhanced Siri might respond with more context, maintain more natural conversations, and offer suggestions based on user behavior.\n\nThat might include suggesting calendar changes based on incoming emails or summarizing documents on command.\n\nHowever, Apple is unlikely to give Google deep system access. The company is known for tightly controlling privacy-sensitive features like Mail, Calendar, and on-device automation. While ChatGPT is available on the App Store, it operates in a sandbox and can't tap into core system services.\n\nGemini, if approved, would likely face similar restrictions. Any integration would need to be opt-in and privacy-focused, consistent with Apple's current approach to third-party AI.\n\nApple's current AI system, Apple Intelligence, combines internal models with access to OpenAI's ChatGPT for certain tasks. That includes fallback support in Siri and writing assistance tools. Pichai said he personally discussed Gemini's potential with Apple CEO Tim Cook in multiple meetings throughout 2024.\n\nA deal with Google could accelerate Apple's AI roadmap, delivering immediate capabilities while it continues developing its own technology. For Google, embedding Gemini into iPhones would dramatically expand the reach of its AI tools.\n\nCompetitive and regulatory pressures\n\nThe potential partnership comes amid a broader industry push toward generative AI. Microsoft has embedded OpenAI's models across Windows and its Copilot platform. Samsung is adding AI features to its Galaxy lineup.\n\nApple risks falling behind\n\nRegulators are also paying close attention. The U.S. Department of Justice has already scrutinized past Apple-Google agreements, including Google's role as the default search engine on iPhones.\n\nA new deal involving Gemini could trigger further antitrust review.\n\nApple has built its brand around user privacy, so any agreement with Google is likely to face public scrutiny. Users may question how much data Gemini requires, whether it operates locally or in the cloud, and how Apple will maintain control over sensitive user data.\n\nIn its OpenAI partnership, Apple routes requests through its own servers, strips identifiers, and prevents logged data from being used to train models. ChatGPT is limited to answering questions and helping with writing, not controlling system features.\n\nA Gemini partnership would likely follow similar rules.\n\nThe road ahead\n\nIt's not yet known which Gemini models Apple might adopt. Gemini Nano seems suited for on-device tasks, while Gemini Pro could offer more powerful cloud-based functions.\n\nIf finalized, the partnership could be announced during Apple's Worldwide Developers Conference, which begins the week of June 9, 2025. That's when Apple is expected to unveil iOS 19, iPadOS 19, and the next version of macOS.\n\nGemini-powered features may debut in late 2025, potentially alongside the iPhone 17. For Apple, it could be the fastest way to narrow the AI gap with rivals — directly benefiting millions of iPhone users."
    },
    {
        "title": "Cloud-First Computing, How Real Are Virtual Desktops?",
        "description": "Desktops are diversifying. Long after we have all moved off of traditional wooden desktops, our “desktop” is now the space where an OS allows us to drop folder & files.",
        "url": "https://www.forbes.com/sites/adrianbridgwater/2025/04/23/cloud-first-computing-how-real-are-virtual-desktops/",
        "source": "Forbes",
        "publishedAt": "2025-04-23T13:47:04Z",
        "full_text": ""
    },
    {
        "title": "Nutanix Visualizes ‘Truly Portable’ Cloud Applications",
        "description": "Because data exists in so many places, we need flexible cloud computing applications that are capable of operating across bare metal, virtualized cloud & Kubernetes.",
        "url": "https://www.forbes.com/sites/adrianbridgwater/2025/05/08/nutanix-visualizes-truly-portable-cloud-applications/",
        "source": "Forbes",
        "publishedAt": "2025-05-08T16:28:32Z",
        "full_text": ""
    },
    {
        "title": "When Microsoft retired Clippy",
        "description": "I didn't find Clippy helpful but maybe I'm just cranky",
        "url": "https://dfarq.homeip.net/when-microsoft-retired-clippy/",
        "source": "Homeip.net",
        "publishedAt": "2025-04-11T17:50:10Z",
        "full_text": "Clippy was the unofficial nickname of the office assistant, a feature present in Microsoft Office 97 and Microsoft Office 2000. His proper name was Clippit, but nobody I knew called him that. Clippit, or Clippy, was inspired by Microsoft Bob, a misguided attempt to make Microsoft Office friendlier, more helpful, and easier to use. But most frequently, it was more annoying than any of those other things. On April 11, 2001, Microsoft announced the Office Assistant would no longer be enabled by default in future versions. Clippy was retiring. And there was much rejoicing.\n\nWhy Clippy was the feature we loved to hate\n\nWhen you used Microsoft Office, Clippy was always appearing at the most inopportune times. Clippy probably meant well, but gave lowest common denominator advice. Someone who already knew how to type a letter didn’t find Clippy’s suggestions about typing a letter helpful. And maybe it was just me, but I found Clippy’s gratiutous animation distracting. When I was trying to write a book chapter, the last thing I needed was a paper clip in the corner of my screen twisting itself into shapes and trying to be cute.\n\nThis meme seems pretty rare now. At least it took me a long time to find it. But it circulated widely in 1998, and I certainly shared it.\n\nI didn’t want Microsoft Office to annoy me with that obnoxious paper clip constantly or when I least expected it. I just wanted it to go away. Was it too much to ask for a word processor to show words on the screen when I typed them and let me go back and correct them when I typed the wrong ones?\n\nI wanted Office 95 back, frankly. Office 95 was 32-bit and stable. It didn’t have an office assistant and it didn’t do realtime spelling or grammar checking, so it was fast too. It wasn’t as fast as Word for Windows 2.0, which ran nicely even on a 386. But Office 95 ran fine on a 486 and really nicely on Pentium-grade systems.\n\nThe problems Microsoft tried to solve with Clippy\n\nClippy was trying to solve two problems. Arguably, Clippy meant well and so did the developers who created Clippy.\n\nComputers were too hard to use\n\nThe first problem was the perception that computers were hard to use. Clippy provided a virtual coach who could help you figure out how to use Word and Excel and Powerpoint.\n\nWhat to do with all that computing power\n\nThe second was the embarrassment of riches in 1990s computing. We don’t remember it now, but when the 486 came out, and again when the Pentium came out, analysts questioned what we would do with all that power. The answer with the Pentium was division errors (FDIV FTW!), and with the 486, Clippy.\n\nWe never ask those kinds of questions anymore. Either we learned our lesson, or we got used to software just growing into whatever computing resources we have available to it. Not only that, we’ve gotten used to software using what we have available locally and having to supplement it with power from the cloud.\n\nBut when the 486 processor came out in April 1989, two respected analysts, Michael Miller and Michael Slater, both questioned what we were going to use that power for. Both noted that the most popular applications at the time didn’t need the 486’s power. And in the mid 90s, after the Pentium processor came out, and software packages started including words like “Better on Pentium” on the box, my friends and I took that as coded language for “bloated programming.”\n\nOne of those pieces of software that really needed a Pentium was Microsoft Bob. And after Microsoft Bob flopped, Microsoft didn’t get the message, and incorporated elements of Bob into other products. Including future versions of Microsoft Office.\n\nI suppose if you’d never used Microsoft Office before, Clippy might have helped you learn it. But we also had books for that.\n\nBut Clippy did indeed help to answer the question of what we’d do with that CPU power. Office 95 ran pretty nicely on a 486 processor, but that wasn’t state of the art anymore in 1995. State of the art by then was a Pentium with a clock rate of 120 or 133 MHz, not a wimpy 486 running at a wimpy 66 MHz. And that 486 wasn’t up to the task of checking your spelling and grammar in realtime and drawing squiggly red lines under your mistakes while Clippy celebrated your efforts by twisting itself into odd shapes in the lower right corner of the screen. At least not without noticeable lag as you typed.\n\nNostalgia 25 years after Clippy retired\n\nToday, Clippy arguably seems less obnoxious and it seems some people may even have nostalgia for him. But I, for one, celebrated when Clippy retired. Maybe I’m just cranky.\n\nIf you found this post informative or helpful, please share it! share\n\nshare\n\nsave\n\nshare\n\nshare\n\npocket\n\nshare\n\nemail\n\nRSS feed\n\nDavid Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.\n\nLike this: Like Loading..."
    },
    {
        "title": "Palantir, Google Expand Cloud Computing Partnership To Target Federal Agencies",
        "description": "Palantir stock rose amid an expanded cloud computing partnership with Google that targets federal government agencies. \nThe post Palantir, Google Expand Cloud Computing Partnership To Target Federal Agencies appeared first on Investor's Business Daily.",
        "url": "https://biztoc.com/x/357480710e58b6d2",
        "source": "Biztoc.com",
        "publishedAt": "2025-04-24T14:18:45Z",
        "full_text": "Buttigieg on what battles Dems SHOULD be fighting now"
    },
    {
        "title": "Applied Digital Tumbles 30% on Revenue Miss; Plans Selling Cloud Computing Unit",
        "description": "The Texas company, which pivoted from crypto mining to high-performance computing, said it will sell its cloud computing business to struggling cloud computing business.",
        "url": "https://www.coindesk.com/markets/2025/04/15/applied-digital-tumbles-30-on-revenue-miss-plans-sale-of-cloud-computing-unit",
        "source": "CoinDesk",
        "publishedAt": "2025-04-15T15:11:22Z",
        "full_text": "Shares of Applied Digital (APLD), a Texas bitcoin mining and data center firm, dropped sharply on Tuesday after the digital infrastructure provider reported quarterly results that fell short of Wall Street expectations.\n\nThe company, which has pivoted from its crypto mining roots to focus on high-performance computing (HPC) and AI-focused data centers, reported revenue of $52.9 million for the quarter ending February 28, 2025—a 22% increase from a year earlier, but well below analysts' consensus estimate of $64.5 million, a nearly 18% miss.\n\nSTORY CONTINUES BELOW Don't miss another story. Subscribe to the Crypto Long & Short Newsletter today . See all newsletters Sign me up By signing up, you will receive emails about CoinDesk products and you agree to our terms of use and privacy policy .\n\nDespite the top-line miss, Applied Digital reported a non-GAAP net loss of $0.08 per share, beating analysts' expectations of a $0.10 per-share loss. However, adjusted EBITDA came in at $10 million, a 41% miss compared to the expected $16.9 million, signaling continued margin pressure amid heavy infrastructure investments.\n\n\n\nAPLD shares plunged as much as 30% from the Monday close, and were trading around $3.90 in the early hours of the session.\n\n\n\nA significant drag came from the company's Cloud Services unit, which posted a sharp sequential revenue decline of 36%, falling from $27.7 million in the prior quarter to $17.8 million. Applied Digital attributed the drop to a shift from single-tenant contracts to a multi-tenant, on-demand GPU model—a transition that faced initial technical challenges.\n\nNotably, the company's board of directors approved on April 10 a plan to sell the Cloud Services business entirely, aiming to refocus on its core HPC data center operations and potentially position itself as a real estate investment trust (REIT) in the future.\n\n“We believe separating the Cloud Services business from our data center operations better serves the long-term interests of our shareholders,” said CEO Wes Cummins on the company’s earnings call.\n\nDisclaimer: Parts of this article were generated with the assistance from AI tools and reviewed by our editorial team to ensure accuracy and adherence to our standards. For more information, see CoinDesk’s full AI Policy."
    },
    {
        "title": "The Technology Trio: AI, Blockchain And Cloud Are Driving The Next Tech Revolution",
        "description": "The convergence of AI, blockchain and cloud computing represents an unparalleled synergy that can amplify the strengths of each technology.",
        "url": "https://www.forbes.com/councils/forbestechcouncil/2025/05/02/the-technology-trio-ai-blockchain-and-cloud-are-driving-the-next-tech-revolution/",
        "source": "Forbes",
        "publishedAt": "2025-05-02T11:30:00Z",
        "full_text": ""
    },
    {
        "title": "DeepSeek transferred data without consent, South Korean watchdog says",
        "description": "Personal Information Protection Commission says AI model sent personal data to Beijing-based cloud service.",
        "url": "https://www.aljazeera.com/economy/2025/4/24/deepseek-transferred-data-without-consent-south-korean-watchdog-says",
        "source": "Al Jazeera English",
        "publishedAt": "2025-04-24T09:48:44Z",
        "full_text": "South Korea’s data protection watchdog has accused DeepSeek, the Chinese start-up whose artificial intelligence-powered chatbot took the tech scene by storm earlier this year, of transferring personal data without users’ consent.\n\nThe Personal Information Protection Commission said on Thursday that DeepSeek had been transferring information to several companies in China and the United States before its ChatGPT-like AI model was removed from app stores in February, pending a privacy review.\n\nNam Seok, director of the commission’s investigation bureau, said during a news conference that the app had sent user prompts and device and network information to a Beijing-based cloud service called Volcano Engine.\n\nDeepSeek “acknowledged it had insufficiently considered Korea’s data protection laws” and “expressed its willingness to cooperate with the commission, and voluntarily suspended new downloads”, Nam said.\n\nDeepSeek did not immediately respond to a request for comment.\n\nFollowing the South Korean watchdog’s announcement, China’s Ministry of Foreign Affairs said it placed a high level of importance on data privacy and security.\n\nAdvertisement\n\n“We have never – and will never – require companies or individuals to collect or store data through illegal means,” ministry spokesperson Guo Jiakun said during a regular news conference.\n\nDeepSeek’s R1 caused a sensation in January after its developers released a research paper claiming they spent less than $6m on computing power to train the model – a fraction of the multibillion-dollar AI budgets of US tech giants such as OpenAI and Google.\n\nThe emergence of a Chinese startup capable of rivalling Silicon Valley’s leading players challenged assumptions about US dominance in AI and prompted scrutiny of the sky-high market valuations of companies such as Nvidia and Meta.\n\nMarc Andreessen, one of the most influential tech venture capitalists in Silicon Valley, hailed DeepSeek’s model as “AI’s Sputnik moment”."
    },
    {
        "title": "The AI data center race is slowing down as Amazon and Microsoft catch their breath",
        "description": "For the past two years, tech giants have been racing to build the digital backbone of the artificial intelligence boom, spending tens of billions of dollars ...",
        "url": "https://finance.yahoo.com/news/ai-data-center-race-slowing-145800649.html",
        "source": "Yahoo Entertainment",
        "publishedAt": "2025-04-28T14:58:00Z",
        "full_text": "An Amazon Web Services data center in Stone Ridge, Virginia. - Photo: Nathan Howard (Getty Images)\n\nFor the past two years, tech giants have been racing to build the digital backbone of the artificial intelligence boom, spending tens of billions of dollars on chips and new data centers. Now, as construction cranes swing and server racks pile up, early signs of restraint are emerging.\n\nAmazon Web Services (AMZN) has paused negotiations on some new leases for data centers, particularly overseas, according to a Wells Fargo report published last week. Microsoft (MSFT) made a similar move in February, canceling plans for approximately two facilities’ worth of computing power, according to TD Cowen.\n\nBoth companies have emphasized that signed deals remain in place and described the moves as normal capacity management. Microsoft said it was still set to spend $80 billion in its fiscal year that ends in June. AWS’ vice president of global data centers wrote on LinkedIn that “there haven’t been any recent fundamental changes in our expansion plans.”\n\nThe AI buildout is real, but the pace may be shifting. While cloud providers publicly maintain that expansion plans are unchanged, the recent lease pauses suggest a more cautious recalibration behind the scenes — a signal that the AI boom may not be advancing at the unrelenting speed companies like Amazon and Microsoft have projected.\n\nOne explanation for the shift is simple overcommitment. A UBS (UBS) report last week concluded that Microsoft’s pullback likely stems from overcommitting during the initial AI rush, according to CNBC. Microsoft’s leased capital expenditures jumped 6.7 times in two years, with lease obligations now totaling roughly $175 billion, UBS wrote. With a better idea of how the tech is actually being used and what the power needs are, Microsoft is now cutting early-stage projects that no longer make immediate sense. UBS said it found little support for the idea that a sudden demand lull was responsible for the change in strategy.\n\nCost pressures inside the AI ecosystem are adding up. A single query to OpenAI’s most advanced models can cost up to $1,000 in computing power alone. Despite charging $200 per month for premium access to ChatGPT, OpenAI CEO Sam Altman said in January that the subscription service is not yet profitable.\n\nEven tech executives are acknowledging the gap between hype and outcomes.\n\nMicrosoft CEO Satya Nadella recently conceded that, so far, AI has not yet produced much measurable value. His comments reflect broader skepticism about whether generative AI can deliver sustainable returns — or whether infrastructure spending is getting too far ahead of real-world demand."
    },
    {
        "title": "The Future Of AI And Its Infrastructure Must Be Open",
        "description": "AI is now the dominant driver of infrastructure innovation, forcing a rethink of everything from cloud architectures to edge computing strategies.",
        "url": "https://www.forbes.com/councils/forbesbusinesscouncil/2025/04/18/the-future-of-ai-and-its-infrastructure-must-be-open/",
        "source": "Forbes",
        "publishedAt": "2025-04-18T11:45:00Z",
        "full_text": ""
    },
    {
        "title": "Where Chinese Or American Tech Is Used In Cloud Data Storage",
        "description": "American and Chinese firms dominate the cloud computing and data center space, raising questions around data security and independence for third countries.",
        "url": "https://www.forbes.com/sites/katharinabuchholz/2025/04/17/where-chinese-or-american-tech-is-used-in-cloud-data-storage/",
        "source": "Forbes",
        "publishedAt": "2025-04-17T13:45:45Z",
        "full_text": ""
    },
    {
        "title": "I attended LlamaCon, Meta's first event for AI developers. It was 'kinda mid.'",
        "description": "Meta's LlamaCon 2025 showcased AI ambitions and gained Wall Street's praise, but developers found it lacking compared to competitors.",
        "url": "https://www.businessinsider.com/llamacon-2025-meta-strived-reassert-ai-leadership-against-rivals-2025-5",
        "source": "Business Insider",
        "publishedAt": "2025-05-02T09:00:02Z",
        "full_text": "This story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nOn the manicured lawns outside Building 21 on Meta's sprawling Menlo Park headquarters, live llamas meandered with languid indifference, drawing clusters of developers who momentarily abandoned technical discussions for selfies with the stoic, woolly ambassadors of Meta's family of large language models.\n\nInside Building 21, I shivered. The cavernous auditorium's air conditioning was cranked up high. Mood lighting bathed the space in Meta's signature blue shade, and dance music blasted from speakers, lending a nightclub ambiance to the event that clashed oddly with the earnest, tech-focused agenda.\n\n\"Rise and shine!\" a Meta PR person chirped as I took a seat.\n\nThis was LlamaCon, Meta's first-ever conference for AI developers. Its timing felt oddly defensive. Earlier this year, DeepSeek, an open-source AI model from China that delivered groundbreaking performance with computational efficiency, had much of Silicon Valley, including Meta's AI division, panicked.\n\nAround the same time, Meta announced that it would spend $65 billion in 2025 to build out AI infrastructure. Weeks after that, the company released Llama 4, the latest version of its LLM family. Mark Zuckerberg called it \"the beginning of a new era for the Llama ecosystem.\" Almost immediately after, Meta was accused of artificially inflating Llama models' performance benchmarks, a claim that executives pushed back against.\n\nLlamaCon, I thought, was Meta's moment to reclaim trust and clarify its AI strategy.\n\nA culinary spectacle: Meta culinary line cook Ricardo J. Borjas Rodriguez's artistic talents were unexpectedly conscripted into the company's AI branding efforts. Pranav Dixit\n\nOnstage, Meta's Chief Product Officer Chris Cox framed the company's open source strategy as principled rather than reactive: \"We were a startup once, too,\" he said in the keynote. \"We built this place on open source.\"\n\nThe subtext was clear: Meta wants developers to see Llama as their path to autonomy and flexibility in an increasingly closed AI ecosystem dominated by offerings from OpenAI, Microsoft, and Google.\n\nMeta Chief Product Officer Chris Cox speaks at LlamaCon 2025 AP Photo/Jeff Chiu\n\nLlama knelt to competitors\n\nLlamaCon featured several announcements, including the launch of a new Llama API that Meta says will make it easy for developers to integrate its models using familiar tools and interfaces. Some tasks will be possible with just a few lines of code.\n\nMeta also announced partnerships with companies to make AI run faster; a security program with AT&T and others to fight AI-generated scams; and $1.5 million in grants to startups and universities around the world using Llama.\n\nConspicuously absent, however, was what many developers had actually come hoping to see: a new reasoning model to compete with what has rapidly become table stakes in the AI industry, including in Chinese open-source alternatives like DeepSeek and Alibaba's Qwen.\n\nIn a conversation with Databricks CEO Ali Ghodsi, Zuckerberg seemed to tacitly acknowledge these shortcomings.\n\nAt LlamaCon, mood lighting bathed the space in Meta's signature shade of blue, and dance music blasted through the air, lending the event a nightclub ambiance. Pranav Dixit\n\n\"Part of the value around open source is that you can mix and match,\" he said. \"If another model, like DeepSeek, is better, or if Qwen is better at something, then, as developers, you have the ability to take the best parts of the intelligence from different models. This is part of how I think open source basically passes in quality all the closed source [models]…[It] feels like sort of an unstoppable force.\"\n\nRelated stories Business Insider tells the innovative stories you want to know Business Insider tells the innovative stories you want to know\n\nVineeth Sai Varikuntla, a developer working on medical AI applications, echoed this sentiment when I spoke with him after the keynote.\n\n\"It would be exciting if they were beating Qwen and DeepSeek,\" he said. \"I think they will come out with a model soon. But right now the model that they have should be on par—\" he paused, reconsidering, \"Qwen is ahead, way ahead of what they are doing in general use cases and reasoning.\"\n\nMissing model improvements\n\nThe online reaction to LlamaCon reflected similar disappointment across developer communities.\n\nOn Reddit's r/LocalLLaMA, the top post was titled \"No new models in LlamaCon announced.\" Users compared Meta unfavorably to Qwen 3, which Alibaba strategically released just one day before Meta's event.\n\n\"Good lord. Llama went from competitively good Open Source to just so far behind the race that I'm beginning to think Qwen and DeepSeek can't even see it in their rear view mirror anymore,\" wrote one user. Others debated whether Meta had planned to release a reasoning model but pulled back after seeing Qwen's performance.\n\nOn Hacker News, a popular forum for developers and tech industry professionals, some criticized the event's focus on API services and partnerships rather than model improvements as \"super shallow.\" And one user on Threads summed up the event simply as \"kinda mid.\"\n\nWhen I asked Meta how they measured the success of the event, they declined to comment.\n\n\"It did seem like a bit of a marketing push for Llama,\" Mahesh Sathiamoorthy, cofounder of Bespoke Labs, a Mountain View-based startup that creates AI tools for data curation and training LLMs, told me. \"They wanted to cast a wider net and appeal to enterprises, but I think the technical community was looking for more substantial model improvements.\"\n\nStill, LlamaCon won praise from Wall Street analysts tracking the company's AI strategy. \"LlamaCon was one giant flex of Meta's ambitions and successes with AI,\" Mike Proulx of Forrester told me.\n\nJefferies analyst Brent Thill called Meta's announcement at the event \"a big step forward\" to becoming a \"hyperscaler, a term referring to large cloud serve providers that offer computing resources and infrastructure to businesses.\n\nSome developers using Llama models were equally enthusiastic about the technology's benefits. For Yevhenii Petrenko of Tavus, which creates AI-powered conversational videos, Llama's speed was crucial. \"We really care about very low latency, like very fast response, and Llama helps us use other LLMs,\" he told me after the event.\n\nHanzla Ramey, CTO of WriteSea, an AI-powered career services platform that helps job seekers prepare résumés and practice interviews, highlighted Llama's cost-effectiveness: \"For us, cost is huge,\" he told me. \"We are a startup, so controlling expenses is really important. If we go with closed source, we can't process millions of jobs. No way.\"\n\nThe future's form and function\n\nToward the end of the day, Zuckerberg joined Microsoft CEO Satya Nadella onstage for a wide-ranging chat about AI's future. One comment stood out.\n\nLlama 4, Zuckerberg explained, had been designed around Meta's preferred infrastructure — the H100 GPU, which shaped its architecture and scale. But he acknowledged that \"a lot of the open source community wants even smaller models.\" Developers \"just need things in different shapes,\" he said.\n\nMeta Founder and CEO Mark Zuckerberg, left, speaks with Microsoft Chairman and CEO Satya Nadella at LlamaCon 2025 AP Photo/Jeff Chiu\n\n\"To be able to basically take whatever intelligence you have from bigger models,\" he added, \"and distill them into whatever form factor you want — to be able to run on your laptop, on your phone, on whatever the thing is…to me, this is one of the most important things,\" he said.\n\nIt was a candid admission. For all the pageantry, LlamaCon wasn't a coronation. It was Meta still mid-pivot, trying to convince developers — and maybe itself — that it can build not just models, but momentum.​​​​​​​​​​​​​​​​"
    },
    {
        "title": "The top companies you want on your résumé if you work in cybersecurity, according to recruiters",
        "description": "Business Insider spoke to six cybersecurity recruiting experts to hear which companies are considered the top names to have on your résumé.",
        "url": "https://www.businessinsider.com/top-companies-cybersecurity-resume-experience-recruiters-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-13T10:37:02Z",
        "full_text": "Cybersecurity recruiting execs told us the best companies to have on your résumé if you want a job in the field.\n\nCybersecurity recruiting execs told us the best companies to have on your résumé if you want a job in the field. Irene Puzankova/Getty Images/iStockphoto\n\nlighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nIf you work in cybersecurity, having the right skill set is a top priority — but having a strong name on your résumé doesn't hurt.\n\nDomini Clark, founder and CEO of Blackmere Consulting, told Business Insider that \"it's not terribly important\" for entry-level candidates to have the most well-known names on their résumés. In fact, working for small companies can sometimes help provide a broader understanding of the space.\n\nBut as you climb the career ladder, Clark said a well-known company can provide \"credibility\" and indicate strong leadership experience exposure to handling enterprise problems.\n\nWhile success in some tech career paths can be closely tied to the major tech giants or FAANG companies, cybersecurity can offer more versatility since it's integrated across almost every industry. Many companies have their own internal cybersecurity teams, for example, which means professionals in the field aren't limited to working at firms specializing in cybersecurity.\n\nBusiness Insider spoke to six cybersecurity recruiting experts to hear which companies are considered the most powerful names to have on your résumé. Several mentioned that while there are big names in the industry, the right skills and experience are the top indicators of talent.\n\nRecruiters also told BI that what is considered a top name depends largely on the sector of cybersecurity.\n\n\"A chemical company would be less interested in someone coming from Meta,\" said Brent Stokes, director of recruiting at staff company Blue Signal Search. In that case, an industry-specific company like Westlake would be of more interest.\n\nHere's what recruiters said."
    },
    {
        "title": "Drivers, start your evidence: NY jury to hear roaring tale of a NASCAR superfan and his allegedly crooked hedge fund",
        "description": "Prosecutors say Andrew Franzone went from stock cars to stock fraud. He says his investment in Nvidia-backed CoreWeave shows his risk-taking paid off.",
        "url": "https://www.businessinsider.com/hedge-fund-founder-nascar-superfan-trial-for-fraud-andrew-franzone-2025-4",
        "source": "Business Insider",
        "publishedAt": "2025-04-13T09:03:02Z",
        "full_text": "lighning bolt icon An icon in the shape of a lightning bolt.\n\nlighning bolt icon An icon in the shape of a lightning bolt. Impact Link\n\nThis story is available exclusively to Business Insider subscribers. Become an Insider and start reading now.\n\nA federal judge in Manhattan is about to wave the green starting flag on a fraud trial mixing NASCAR, a former hedge funder, and a Nvidia-backed tech stock that proved to be a come-from-behind winner.\n\nJury selection is scheduled for Monday in the trial, in which racing superfan and car collector Andrew Franzone, 48, will fight securities and wire fraud charges.\n\nFederal prosecutors say Franzone tricked more than 100 victims — including fellow drivers and racing fans — into investing a total of $40 million in his Miami hedge fund.\n\nNASCAR is central to the case. Prosecutors say Franzone found his victims and spent his money at its speedways and vintage car shows.\n\n\"Franzone committed his crime by exploiting the networks and connections in the NASCAR community,\" from coast to coast and in England, prosecutors wrote last month.\n\nProsecutors say Franzone's victims include racing legend and 1995 NASCAR truck series champion Mike \"The Gunslinger\" Skinner. And they say Franzone used the fund to finance a flashy, high-octane racing lifestyle.\n\nIn 2015, Franzone illegally spent $565,000 in fund assets to purchase an airplane hangar just outside Daytona Beach, prosecutors say.\n\nThe hangar housed Franzone's prized vintage race car collection, including Skinner's series-winning Chevy truck and the Ford Galaxie that Fred \"Golden Boy\" Lorenzen drove to victory in the 1965 Daytona 500, according to a 2016 profile in the Wall Street Journal.\n\n\"It was the coolest sound I'd ever heard,\" Franzone told the Journal, describing being in his 20s and hearing the engine roar of \"an old 1960s big block stock car\" for the first time.\n\nProsecutors say the hangar was also home to Franzone's racing team, ATF & Gunslinger. Skinner was a celebrity driver for the team. So was five-time pro-wrestling world champion Bill Goldberg.\n\nATF & Gunslinger proved a success, winning races in the US and UK. In 2017, Hard Rock International, the global café and hotel chain, signed on as a sponsor.\n\nThen, in 2019, investors became jittery, questioning Franzone's alleged lies about his fund's liquidity and performance, and — in the words of prosecutors — his \"house of cards\" collapsed.\n\nFrom stock cars to stock fraud\n\nNow a criminal defendant — who used the New York City subway to travel to his most recent court date — Franzone has fought, without luck, to keep any reference to NASCAR and race cars out of his trial.\n\nRelated stories Business Insider tells the innovative stories you want to know Business Insider tells the innovative stories you want to know\n\nAt Franzone's last day in court, on April 7, defense attorney Joseph R. Corozzo argued that references to his client's spending on his racing hobby would be \"unduly prejudicial.\"\n\nIt would be like telling jurors, \"His spending is on race cars, so you should be offended,\" Corozzo argued.\n\nIn a ruling Thursday, US District Judge Vernon Broderick disagreed. The judge wrote that evidence of Franzone's spending and lifestyle \"is probative of his motive to commit the fraud,\" and therefore fair game for jurors to hear about.\n\nA \"romantic partner\" and luxury vehicles\n\nFranzone was so obsessed with maintaining his NASCAR lifestyle that even after his fund went bankrupt in 2019, he pocketed a $200,000 investment from one of his closest racing buddies, prosecutors allege.\n\nHe immediately spent $50,000 of the money on \"luxury vehicles,\" prosecutors say.\n\nAnother $15,000 was allegedly diverted to \"a romantic partner—\" a Manhattan woman who, according to court filings, was \"purportedly engaged in the business of delivering luxury pet gift baskets.\"\n\nFranzone wired the now-former girlfriend $289,000 in fund assets between 2017 and 2019, prosecutors say.\n\nIt wasn't until nearly two years later, in 2021, that he was arrested at a palm-tree-shaded beachfront Westin in Fort Lauderdale, where he'd been living for the previous year. His mother would end up paying his overdue $2,270 hotel bill, using his father's credit card, according to court documents.\n\nSoon after his arrest, the hedge fund — which Franzone had been fighting for two years to keep afloat amid investor mutinees, bankruptcy proceedings, an SEC investigation, and a sea of litigation — was liquidated without Franzone's approval by the bankruptcy trustee.\n\nA twist in the track\n\nThere's a twist in the track of this story, though, involving the NASCAR-worthy comeback of one of Franzone's investments.\n\nJust why Franzone's \"FF Fund\" filed for bankruptcy in 2019 remains in dispute.\n\nProsecutors say it was because Franzone realized he couldn't pay back his investors — the vast majority of his fund's assets were risky, failing, or \"illiquid\" investments that could not quickly converted into cash. The defense says Franzone had hoped bankruptcy would protect the still-viable fund from a litigious former investor who was trying to dissolve it entirely.\n\nEither way, a little over a year ago, the bankruptcy trustee liquidated — sold off — one of its investments and hit the jackpot.\n\nFranzone had purchased 250,000 shares of a Nvidia-backed cloud computing company called CoreWeave for $250,000 shortly before the 2019 bankruptcy.\n\nThe shares liquidated by the trustee two years later sold for over $55 million.\n\n\"The investors will get more than their investments back,\" Franzone's Miami bankruptcy attorney told the judge in that case last year.\n\n\"The fund seems not to be insolvent. To the contrary, it's done amazing because of the investment that was made in the company CoreWeave.\"\n\nIn Franzone's defense, his lawyers argue that investors were warned in writing that any investment comes with risk. They were also told the fund would use any investment techniques Franzone felt were appropriate.\n\nAnd in the case of CoreWeave, all of the investors who stuck with the fund — instead of writing their investment off on their taxes as a capital loss — have made money. That includes Franzone and his own parents and brother, who remain investors, according to court records.\n\nProsecutors countered in a filing last month that it matters not a whit if CoreWeave did well.\n\n\"Whether those investors ultimately recouped their losses years later, and only after FF Fund filed for bankruptcy, does not alter the fact that Franzone lied to them, deceived them, and misused their funds as part of his scheme,\" prosecutors wrote the judge.\n\nFranzone had hoped to argue that the success of his investment in CoreWeave proves that he never intended to defraud anyone and that investors will ultimately recoup any losses.\n\nOn Thursday, the judge sided with prosecutors and barred the defense from making this argument.\n\nAn attorney for Franzone and a spokesperson for the US Attorney's Office did not respond to requests for comment left Friday afternoon.\n\nThe trial is expected to last three weeks. If convicted, Franzone faces a maximum sentence of 20 years in prison."
    },
    {
        "title": "Microsoft Warns Default Helm Charts Could Leave Kubernetes Apps Exposed to Data Leaks",
        "description": "Microsoft has warned that using pre-made templates, such as out-of-the-box Helm charts, during Kubernetes deployments could open the door to misconfigurations and leak valuable data.\n\"While these 'plug-and-play' options greatly simplify the setup process, the…",
        "url": "https://thehackernews.com/2025/05/microsoft-warns-default-helm-charts-for.html",
        "source": "Internet",
        "publishedAt": "2025-05-06T11:05:00Z",
        "full_text": ""
    },
    {
        "title": "Google earnings are coming Thursday. Here's what to watch",
        "description": "Google (GOOGL) will report first-quarter 2025 earnings Thursday after the bell in what could be a key test for the Nasdaq, given the company’s heavyweight status on the index.Read more...",
        "url": "https://qz.com/google-q1-2025-earnings-report-cloud-ai-monopoly-1851777560",
        "source": "Quartz India",
        "publishedAt": "2025-04-23T19:38:00Z",
        "full_text": "Google (GOOGL-1.57% ) parent Alphabet will report first-quarter 2025 earnings Thursday after the bell in what could be a key test for the Nasdaq, given the company’s heavyweight status on the index.\n\nHow Trump’s tariffs could make your iPhone cost $3,500 CC Share Subtitles Off\n\nEnglish view video How Trump’s tariffs could make your iPhone cost $3,500\n\nHow Trump’s tariffs could make your iPhone cost $3,500 CC Share Subtitles Off\n\nEnglish How Trump’s tariffs could make your iPhone cost $3,500\n\nThe company’s stock has fallen about 16% so far this year, so investors will be tuning in to Thursday’s earnings call to gauge how exactly how confident Google’s leadership is about the company’s future, particularly in the key areas of AI and cloud computing, and amid a host of legal concerns related to antitrust cases.\n\nAdvertisement\n\nThe shares were up slightly in Thursday morning trading, about 1.3%.\n\nGoogle is expected to report Q1 revenue growth of $89.2 billion (an 11% year-over-year increase) and earnings per share of $2.02 (7% ). Despite the company’s dip in stock price over the past year, Wall Street analysts largely remain optimistic about Google’s future. The consensus? Its core business fundamentals are strong.\n\nAdvertisement\n\nStill, lingering uncertainties have led some big firms to lower price targets ahead of Thursday’s call.\n\n​Top analysts, such as those at TD Cowen (TD+0.29% ), UBS (UBS-1.61% ), and Scotiabank (BNS+0.48% ), have adjusted their 12-month price targets for Alphabet. TD Cowen reduced its target from $210 to $195, while UBS lowered its from $209 to $173, and Scotiabank dropped its from $232 to $200. But despite these more cautious outlooks, all three firms maintain a “buy” rating on the stock.\n\nAdvertisement\n\nRBC Capital (RY-0.75% ) analyst Brad Erickson said in a report that institutional investors have lowered their expectations: “We believe buy side is expecting a (2%-3%) miss, respectively, relative to sell-side estimates of 9% search growth and 8% overall ad growth in Q1.”\n\nGoogle is dealing with a host of swirling questions that analysts are taking into consideration: rising legal risks, trade war and tariff uncertainty, and increasing competition in AI and cloud computing.\n\nAdvertisement\n\nFor investors, the first-quarter earnings report could be a moment of truth: a chance to reset expectations and refocus on the long-term potential of a $1.9 trillion Silicon Valley giant navigating turbulent waters.\n\nLegal risks remain a big concern\n\nGoogle has gotten plenty of headlines lately as it wades its way through a variety of antitrust lawsuits.\n\nAdvertisement\n\nThe search engine giant was back in court this week, related to a ruling that found the company has an illegal monopoly in online search — in part by paying web browsers and smartphone manufacturers to feature its search engine. And the DOJ told the judge it had an eyebrow-raising solution: forcing the company to sell its popular Chrome browser.\n\nGoogle faces other mounting issues that have put the company in an especially vulnerable position.\n\nAdvertisement\n\nIn a separate antitrust case, a federal judge ruled that the company had illegally maintained a monopoly in some of its online advertising technology, which could result in regulatory fines or structural changes to how Google’s ad business operates — and maybe even a break-up. And Japan’s FTC recently sent the company a cease-and-desist order after it said Google’s search practices were monopolistic.\n\nOn Thursday, analysts will watch closely for any mention of these legal challenges and for what the company’s strategy is for dealing with them beyond what the company has already said — that they’ll appeal.\n\nAdvertisement\n\nPotential for Cloud and AI growth — but when?\n\nOne key feature of Google’s earnings report will likely be how the company has integrated and improved its AI offerings, especially among increased competition in the realm. Investors will look at how the company’s Gemini AI model has been integrated across Google Search, YouTube, and Google Cloud — and at how the AI will start generating substantial revenue.\n\nAdvertisement\n\nJeffries analysts said that Google Cloud is “well positioned to benefit from Gemini AI advances” as the cloud computing suite remains a bit of a highlight for the company. Analysts expect Q1 Google Cloud revenue of almost $12 billion, up 25% year-over-year and slightly below both the Street’s expectation ($12.3 billion) and Q4’s growth rate (30%).\n\nHowever, Google Cloud is still playing catch-up to rivals such as Amazon Web Services (AMZN-2.87% ) and Microsoft Azure (MSFT-0.92% ).\n\nAdvertisement\n\nGoogle has positioned Google Cloud as an AI-first platform, so as more businesses adopt AI technologies, Google hopes to position its software as the go-to provider. If this quarter’s earnings show significant cloud growth, it could signal a breakthrough moment. Analysts will be closely watching for any signs that this strategy has begun to pay off.\n\nAnd analysts will be looking at AI search competition, too, which JPMorgan Chase (JPM-1.03% ) analyst Doug Anmuth said in a report “remains front and center given the rapidly growing scale of OpenAI’s ChatGPT and Google’s more measured pace in disrupting its own search ecosystem.”\n\nAdvertisement\n\nHow long can advertising remain the backbone?\n\nGoogle’s advertising business remains the biggest component of the company’s revenue, so investors will look at those numbers to shed light on the current state of the digital advertising market.\n\nAdvertisement\n\nYouTube and Google Search, the pillars of the company’s ad revenue, are expected to show growth. Analysts predict YouTube’s ad revenue will rise nearly 11% to almost $8 billion. But the company has big hopes for expansion in YouTube Shorts monetization. If it becomes a significant revenue stream, it could help offset any potential slowdown in traditional ad revenue.\n\nAnalysts remain cautious. Legal challenges, including the recent ruling in federal court that found Google guilty of illegally dominating the market in ad servers and exchanges, could have big implications."
    },
    {
        "title": "Salesforce CEO says Microsoft did \"pretty nasty\" things to Slack and its OpenAI partnership may be a recipe for disaster",
        "description": "Marc Benioff warns OpenAI could be in trouble with its connection to Microsoft after venting frustrations with how the company previously competed with Slack.",
        "url": "https://www.windowscentral.com/microsoft/salesforce-ceo-microsoft-did-pretty-nasty-things-to-slack",
        "source": "Windows Central",
        "publishedAt": "2025-05-08T09:19:55Z",
        "full_text": "Salesforce CEO Marc Benioff is well-known for not mincing his words, especially when criticizing Microsoft and throwing jabs at the company's AI efforts. Over the past few months, the executive has referred to Copilot as the new Microsoft Clippy, claiming it doesn't deliver value.\n\nBenioff has also branded Microsoft as an \"OpenAI reseller,\" claiming the tech giant repackaged ChatGPT as Copilot and presented it to consumers as a different product. Recently, the executive appeared in a podcast interview with SaaStr CEO Jason Lemkin.\n\nAI, M&A, and the Future of SaaS: Lessons from Marc Benioff, Salesforce CEO, Co-Founder and Chair - YouTube Watch On\n\nMarc Benioff claimed that Microsoft did \"horrible things\" to Slack before Salesforce acquired the platform between 2020 and 2021 for approximately $27 billion. Interestingly, Microsoft had also shown interest in acquiring Slack for $8 billion, but later pulled the plug in a bit to focus more on Skype (RIP) and the launch of Teams.\n\nAccording to Salesforce CEO Marc Benioff:\n\n\"You can see the horrible things that Microsoft did to Slack before we bought it. That was pretty bad and they were running their playbook and did a lot of dark stuff. And it's all gotten written up in an EU complaint that Slack made before we bought them.\"\n\nMicrosoft has a long-standing rivalry with Slack. The messaging platform accused Microsoft of using anti-competitive techniques to maintain its dominance across organizations, including bundling Teams into its Microsoft Office 365 suite.\n\nHowever, Microsoft was forced to unbundle Teams from Office 365 globally after Slack filed a complaint with the EU Commission. Teams is now available for new customers as a standalone app for $5.25, whereas Office packages without Teams range between $7.75 and $54.75.\n\nGet the Windows Central Newsletter All the latest news, reviews, and guides for Windows and Xbox diehards. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nMarc Benioff further indicated that Microsoft's treatment of Slack was \"pretty nasty.\" He claimed that the company often employs a similar playbook to gain a competitive advantage over its rivals while referencing \"browser wars\" with Netscape and Internet Explorer in the late 1990s.\n\n\"That playbook should get ripped up and thrown away,\" continued Benioff.\n\nMarc Benioff doesn't think Microsoft will be in OpenAI's future\n\nOpenAI now plans to reduce its revenue share with Microsoft. (Image credit: Getty Images | NurPhoto)\n\nOver the past few months, multiple reports and speculations have surfaced online suggesting that Microsoft's multi-billion-dollar partnership with OpenAI might be fraying. It all started when OpenAI unveiled its $500 billion Stargate project alongside SoftBank, designed to facilitate the construction of data centers across the United States.\n\nThe ChatGPT maker had previously been spotted complaining that Microsoft doesn't meet its cloud computing needs, shifting blame to the tech giant if one of its rivals hit the AGI benchmark first. Consequently, Microsoft lost its exclusive cloud provider status but retains the right of refusal to OpenAI's projects.\n\nAt the same time, Salesforce CEO Marc Benioff predicted that Microsoft won't use OpenAI in the future. Interestingly, Microsoft has seemingly taken a back seat in its partnership with OpenAI and even started developing in-house AI models, emancipating itself from an overdependence and reliance on OpenAI.\n\nThat was extremely interesting and you can see how Microsoft is really starting to run a separate playbook against OpenAI. I think that's now how Microsoft thinks. Microsoft is a company that wants to own it all, control it all, if they see a hot company or hot startup, they ask themselves: ‘Hey why is that not in our world’. Salesforce CEO, Marc Benioff\n\nWhile speaking to IT Pro, Microsoft confirmed that OpenAI “continues to be our partner on frontier models.” It will be interesting to see how Microsoft and OpenAI's partnership pans out, especially after the former officially listed the ChatGPT maker as a competitor in search and AI."
    },
    {
        "title": "Microsoft doesn't want to support ChatGPT training anymore — but OpenAI isn't \"compute-constrained,\" according to Sam Altman",
        "description": "Microsoft is slowing its role in its multi-billion dollar partnership with OpenAI, and has reportedly pulled out of two big data center deals that would have provided additional support for ChatGPT's training.",
        "url": "https://www.windowscentral.com/microsoft/microsoft-doesnt-want-to-support-chatgpt-training-anymore",
        "source": "Windows Central",
        "publishedAt": "2025-04-16T18:03:51Z",
        "full_text": "Microsoft and OpenAI arguably shared the best tech bromance in history. The former made a multi-billion-dollar investment to integrate the ChatGPT maker's AI smarts across its tech stack. However, recent reports suggest that their partnership is seemingly fraying.\n\nIn February, OpenAI unveiled its $500 billion Stargate project to facilitate the construction of data centers across the United States for its AI efforts. Consequently, Microsoft lost its exclusive cloud provider status as the ChatGPT maker seemingly started establishing more independence, emancipating itself from an overreliance on the tech giant for cloud computing.\n\nSalesforce CEO Marc Benioff predicted that Microsoft wouldn't use OpenAI in the future, and as it now seems, the executive's prediction is starting to take shape.\n\nHowever, Microsoft CEO Satya Nadella indicated that the change in the structure of its partnership with OpenAI won't stall the company's AI advances. The company will stick to its plan to invest $80 billion in AI advances and data centers in 2025.\n\nAccording to a report from our friends at PC Gamer, Microsoft recently slowed down AI investments by pulling out of two data center deals that would facilitate additional training for OpenAI's ChatGPT.\n\nInterestingly, a separate report by Reuters seemingly corroborates PC Gamer's report, suggesting that Microsoft's decision to pull out of the data center deals with OpenAI was by design. Microsoft no longer wants to offer additional support to OpenAI for ChatGPT's training.\n\n(Image credit: Getty Images | SOPA)\n\nThis news comes at a crucial time in the AI era. Multiple reports suggest that top AI labs, including OpenAI, Anthropic, and Google, have hit a wall and are unable to develop advanced AI models because of a lack of high-quality content for model training.\n\nGet the Windows Central Newsletter All the latest news, reviews, and guides for Windows and Xbox diehards. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nOpenAI CEO Sam Altman and former Google CEO Eric Schmidt dismissed the claims, indicating that scaling laws haven't begun stunting AI progression. \"There is no wall,\" added Altman.\n\nPerhaps more interestingly, in an interview, Sam Altman revealed that OpenAI is no longer \"compute-constrained.\" Coincidentally, the executive made this statement shortly after OpenAI concluded its latest round of funding, led by SoftBank, where it raised $40 billion, pushing its market cap to $300 billion.\n\nDespite the emergence of Chinese startup DeepSeek with a more cost-effective model that surpasses OpenAI's o1 reasoning model's capabilities at a fraction of the cost, OpenAI is seemingly determined to move forward with its $500 billion Stargate project.\n\nIt'll be interesting to see how Microsoft's partnership with OpenAI evolves as AI advances and scales greater heights, and ultimately, who'll win the AI race by hitting the AGI benchmark first."
    },
    {
        "title": "Hands-On Large Language Models",
        "description": "Official code repo for the O'Reilly Book - \"Hands-On Large Language Models\" - HandsOnLLM/Hands-On-Large-Language-Models",
        "url": "https://github.com/HandsOnLLM/Hands-On-Large-Language-Models",
        "source": "Github.com",
        "publishedAt": "2025-04-19T01:52:55Z",
        "full_text": "Hands-On Large Language Models\n\nWelcome! In this repository you will find the code for all examples throughout the book Hands-On Large Language Models written by Jay Alammar and Maarten Grootendorst which we playfully dubbed:\n\n\n\n\"The Illustrated LLM Book\"\n\nThrough the visually educational nature of this book and with almost 300 custom made figures, learn the practical tools and concepts you need to use Large Language Models today!\n\nThe book is available on:\n\nTable of Contents\n\nWe advise to run all examples through Google Colab for the easiest setup. Google Colab allows you to use a T4 GPU with 16GB of VRAM for free. All examples were mainly built and tested using Google Colab, so it should be the most stable platform. However, any other cloud provider should work.\n\nChapter Notebook Chapter 1: Introduction to Language Models Chapter 2: Tokens and Embeddings Chapter 3: Looking Inside Transformer LLMs Chapter 4: Text Classification Chapter 5: Text Clustering and Topic Modeling Chapter 6: Prompt Engineering Chapter 7: Advanced Text Generation Techniques and Tools Chapter 8: Semantic Search and Retrieval-Augmented Generation Chapter 9: Multimodal Large Language Models Chapter 10: Creating Text Embedding Models Chapter 11: Fine-tuning Representation Models for Classification Chapter 12: Fine-tuning Generation Models\n\nTip You can check the setup folder for a quick-start guide to install all packages locally and you can check the conda folder for a complete guide on how to setup your environment, including conda and PyTorch installation. Note that the depending on your OS, Python version, and dependencies your results might be slightly differ. However, they should this be similar to the examples in the book.\n\nReviews\n\n\"Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built.\" Andrew Ng - founder of DeepLearning.AI\n\n\"This is an exceptional guide to the world of language models and their practical applications in industry. Its highly-visual coverage of generative, representational, and retrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended!\" Nils Reimers - Director of Machine Learning at Cohere | creator of sentence-transformers\n\n\"I can’t think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.\" Josh Starmer - StatQuest\n\n\"If you’re looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology!\" Luis Serrano, PhD - Founder and CEO of Serrano Academy\n\n\"Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books leave abstract. The book starts with simple introductory beginnings, and steadily builds in scope. By the final chapters, you will be fine-tuning and building your own large language models with confidence.\" Leland McInnes - Researcher at the Tutte Institute for Mathematics and Computing | creator of UMAP and HDBSCAN\n\nWe attempted to put as much information into the book without it being overwhelming. However, even with a 400-page book there is still much to discover!\n\nWe continue to create more guides that compliment the book and go more in-depth into new and exciting topics:\n\nCitation\n\nPlease consider citing the book if you consider it useful for your research:"
    },
    {
        "title": "Microsoft smashes Wall Street's earnings expectations — and the stock soars",
        "description": "Microsoft (MSFT) blew past Wall Street expectations in its fiscal third quarter, racking up $70.1 billion in revenue, up 13%, and $25.8 billion in net income, up 18%, fueled by relentless demand for cloud and AI. Read more...",
        "url": "https://qz.com/msft-earnings-nasdaq-april-30-1851778563",
        "source": "Quartz India",
        "publishedAt": "2025-04-30T20:26:00Z",
        "full_text": "Microsoft (MSFT-0.92% ) blew past Wall Street’s expectations in its fiscal third quarter, racking up $70.1 billion in revenue, up 13%, and $25.8 billion in net income, up 18%, fueled by relentless demand for cloud and AI.\n\nHow Trump’s tariffs could make your iPhone cost $3,500 CC Share Subtitles Off\n\nEnglish view video How Trump’s tariffs could make your iPhone cost $3,500\n\nHow Trump’s tariffs could make your iPhone cost $3,500 CC Share Subtitles Off\n\nEnglish How Trump’s tariffs could make your iPhone cost $3,500\n\nEarnings came in at $3.46 per share, easily topping consensus estimates of $3.22.\n\nAdvertisement\n\nCloud stays hot\n\nMicrosoft’s cloud engine kept roaring in Q3, with Azure and other cloud services surging 33% year-over-year, powering a 21% increase in the all-important Intelligent Cloud segment, which hit $26.8 billion. Server product revenue rose 22% as demand for AI infrastructure stayed hot.\n\nAdvertisement\n\nMicrosoft 365 and Xbox deliver, LinkedIn grows\n\nThe Productivity and Business Processes segment brought in $29.9 billion, up 10% from last year, driven by steady growth in Microsoft 365 and Dynamics. Microsoft 365 Commercial revenue rose 11%, with its cloud-based suite climbing 12%, while consumer revenue grew 10%. Dynamics 365 jumped 16%, lifting Dynamics overall by 11%.\n\nAdvertisement\n\nLinkedIn notched a 7% gain, which may look unimpressive to some but feels like a feat in this sluggish job market. (For context, it’s down from 10% revenue growth last year, and 9% revenue growth in Q2 2025.)\n\nMeanwhile, More Personal Computing generated $13.4 billion, a 6% increase. Xbox content and services rose 8%, search and news advertising surged 21%, and Windows OEM and Devices grew 3% — a rare bright spot in the otherwise sleepy PC category.\n\nAdvertisement\n\nMicrosoft also returned $9.7 billion to shareholders via dividends and buybacks.\n\nEarnings call focused on demand and hyperscaling\n\nOn the earnings call, Microsoft executives emphasized the sheer pace of AI-related demand, saying they can’t build data centers fast enough. Hyperscaling — the rapid expansion of cloud infrastructure — has become systemic, driven not just by digital-native startups but by major enterprise customers across industries.\n\nAdvertisement\n\nMicrosoft cited Abercrombie & Fitch (ANF-0.62% ), Coca-Cola (KO-0.01% ), and BNY Mellon (BK-0.81% ) as examples of large-scale customers leaning heavily into both AI and non-AI workloads. Management noted that digital-native clients increasingly run both workloads in the same cloud, deepening the stickiness of the relationship.\n\nAI vs. Non-AI Growth\n\nAlthough analysts zeroed in on AI during the call, management was quick to point out that the bulk of revenue growth this quarter came from non-AI workloads.\n\nAdvertisement\n\nStill, the AI story remains compelling: Microsoft has effectively turned its capital expenditures — on GPUs, CPUs, storage, and infrastructure — into revenue, creating what one analyst called an “inspiring” Azure performance. Executives described a blurred line between AI and non-AI usage, reflecting how integrated AI has become across Microsoft’s cloud offerings.\n\nForward guidance\n\nMicrosoft is guiding for a strong finish to its fiscal year, forecasting Q4 revenue of $28.75 to $29.05 billion for its Intelligent Cloud segment, reflecting 20% to 22% growth in constant currency, and $32.05 to $32.35 billion for Productivity and Business Processes, or 11% to 12% growth.\n\nAdvertisement\n\nRevenue from the More Personal Computing segment is expected to come in between $12.35 and $12.85 billion. Despite heavy AI infrastructure investment, the company still expects Microsoft Cloud gross margins to land around 67%, down from the prior year. Overall operating expenses are set to grow modestly to $18 to $18.1 billion, while COGS will rise more sharply, reaching $23.6 to $23.8 billion, reflecting a 20% increase.\n\nAzure remains the central growth engine, with revenue expected to climb 34% to 35% in constant currency. Microsoft anticipates 365 Commercial cloud revenue will rise around 14%, while projecting the consumer version to grow in the mid-teens. The tech giant expects LinkedIn to deliver high single-digit growth, and Dynamics 365 is set to rise in the mid-to-high teens.\n\nAdvertisement\n\nIn More Personal Computing, Xbox content and services are expected to grow in the high single digits, and search and news advertising is projected to jump in the high teens.\n\nFlashback to Q2\n\nMicrosoft turned in a similarly strong showing last quarter, posting $69.6 billion in revenue, up 12%, and $24.1 billion in net income, up 10%, with EPS of $3.23. Microsoft Cloud revenue jumped 21% to $40.9 billion, powered by a 31% gain in Azure and other cloud services. CEO Satya Nadella touted a $13 billion annualized run rate for the company’s AI business — up 175% year-over-year.\n\nAdvertisement\n\nProductivity and Business Processes rose 14%, lifted by Microsoft 365, LinkedIn, and Dynamics. More Personal Computing was flat, but search advertising, up 21%, and modest growth in Xbox and Windows OEM, helped balance things out.\n\nEven so, investors sold the news: Microsoft shares fell 6% the next day.\n\nThis time, however, the momentum looks like it’ll last. The stock is up 8% after hours."
    },
    {
        "title": "AI’s infrastructure problem isn’t tariffs, it’s unused capacity",
        "description": "The evolution of AI involves utilizing prolific existing computing power, not building new data megaliths.",
        "url": "https://www.techradar.com/pro/ais-infrastructure-problem-isnt-tariffs-its-unused-capacity",
        "source": "TechRadar",
        "publishedAt": "2025-05-05T14:08:21Z",
        "full_text": "The impact that the new US administration’s sweeping global tariffs will have on building out the country’s AI infrastructure is a hot topic. This is not least because US tech giants continue to announce huge budgets for sprawling data centers, with a recent report estimating that Microsoft, Google and Meta plan to spend a combined $325 billion on new data centers.\n\nAnalysts are focusing mainly on the rising cost of steel, copper, and other precious metals needed to expand physical infrastructure, as well as the specialized semiconductor and GPU chips essential to data centers. By some estimates, construction costs for commercial projects may rise by as much as 5% under the new US global tariff regime.\n\nWhile these estimates may be correct, the analysis - at least in terms of data centers - is missing a crucial element. Debates over importing steel, GPUs, servers, and networking equipment assume that meeting rising computing demand means we must continuously build. The reality, however, is that we are significantly underutilizing the computing resources we already have.\n\nTory Green Social Links Navigation CEO of io.net.\n\nThe resource utilization problem\n\nWhat we have here is a manufacturing solution being proposed for a distribution problem. Building additional facilities to meet the growing demand for AI computing not only requires enormous capital investment, but this centralized model creates vulnerabilities in terms of single points of failure and malicious actors.\n\nMeanwhile, vast computing resources are sitting idle worldwide. Indeed, we are wasting tremendous computing power. According to the SPECpower benchmark, 20% to 60% of the power consumed by computers connected to the public cloud is idle and so unattributable to active utilization. This means that a significant amount of cloud and GPU computing power is likely being unutilized globally.\n\nIn China, the situation is markedly worse. Due to the country’s recent AI infrastructure “gold rush”, billions of dollars were spent on data centers, around 80% of which are now sitting idle, according to a report published by The MIT Technology Review.\n\nThis is because the recent release of Chinese AI LLM DeepSeek has revealed a startling truth: AI doesn’t need $30,000 GPU chips with enormous computing power to operate. Indeed, as the release of DeepSeek V3 has just shown, an AI LLM can be run on a Mac computer.\n\nThis serves to underline the enormous inefficiencies in global computing power that have persisted for years simply because we have not sought to aggregate and redistribute existing resources. Rather than building entirely new infrastructure, we can tap into reserves of existing computational power. All we need to do is rethink computing infrastructure.\n\nUtilizing existing infrastructure\n\nHarnessing unused computational power is the most efficient way to meet the demands of AI, and Decentralized Physical Infrastructure Networks (DePINs) offer innovative solutions. Rather than building more centralized facilities, DePIN cloud models can connect buyers with suppliers of unused computing power through a globally distributed network that is not reliant on single data centers.\n\nOne might think of this as the \"Airbnb of Compute\" - a way to connect those who need GPU space with those who have GPU space to spare.\n\nThis approach, which is able to utilize the world’s idle computing power, has the potential to meet the needs of businesses and individuals without spending billions of dollars on new physical infrastructure.\n\nBy taking advantage of underutilized GPU power, cloud computing DePIN platforms can offer high-performance computing at prices significantly lower than established providers like AWS and Azure, and this structural advantage makes tariff concerns largely irrelevant.\n\nRather than spending billions of dollars on new facilities that will - as China’s dilemma demonstrates - be underutilized, DePIN models maximize existing resources. As DeepSeek has shown, the distribution of computing power can also widen participation in AI tools, enabling small and medium enterprises to leverage machine learning technology without prohibitive costs.\n\nComputing without constraints\n\nGlobally, we have come to realize the drawbacks of centralization. In the cloud space, by pivoting from large, centralized data centers, we can build networks that are resilient to trade wars or pandemics. These distributed systems respond to demand naturally – expanding when needed, contracting when not needed – without requiring vast new facilities.\n\nFor business leaders and policymakers, this represents an opportunity to rethink assumptions about infrastructure development. Rather than subsidizing more gigantic data centers, leaders could focus policies on incentivizing the better utilization of existing resources.\n\nIn the US especially, rather than poring over the potential increase in costs of importing more steel, copper, and computational hardware, the Stargate Infrastructure Project might focus on the vast unused capacity the country already has. And, as cost-cutting is a core focus of this administration, this approach would be fully aligned.\n\nPerhaps of even more interest to such a conservative administration would be the savings in energy costs that such an approach would bring. Distributed computing networks typically consume less energy than large data centers and, more importantly, in a way that does not negatively impact communities and businesses in the area.\n\nInnovation through efficiency\n\nMost importantly, though, distributed computing networks enable faster innovation cycles by lowering barriers to access. They create new economic opportunities for resource providers previously excluded from the AI economy - and innovation and entrepreneurship are nothing if not the raison d'etre of Republican idealism.\n\nUltimately, current debates around tariffs and their impact on data center construction costs distract from a glaring opportunity. By embracing decentralized computing models, the world can meet growing AI demands without being constrained by infrastructure limitations. The technology exists today, and the economic case is compelling. The only remaining question is how quickly businesses and policymakers will recognize this.\n\nWe've featured the best IT infrastructure management service.\n\nThis article was produced as part of TechRadarPro's Expert Insights channel where we feature the best and brightest minds in the technology industry today. The views expressed here are those of the author and are not necessarily those of TechRadarPro or Future plc. If you are interested in contributing find out more here: https://www.techradar.com/news/submit-your-story-to-techradar-pro"
    },
    {
        "title": "The open source advantage: Faster bugs, better builds, wider buy-in",
        "description": "Open-sourcing is definitely not something to rush into. Here are some pros, cons and general advice based on experience.",
        "url": "https://venturebeat.com/programming-development/the-open-source-advantage-faster-bugs-better-builds-wider-buy-in/",
        "source": "VentureBeat",
        "publishedAt": "2025-04-19T19:05:00Z",
        "full_text": ""
    },
    {
        "title": "Dot com era crash on the cards for AI datacenter spending? It's a 'risk'",
        "description": "Analysts say the bubble won't burst, but it is possible, admits world's largest colo provider\nInterview Those who ignore history are destined to repeat mistakes of the past and, with signs of an inflating bit barn spending bubble, comparisons are being made w…",
        "url": "https://www.theregister.com/2025/04/14/datacenter_spending_ai/",
        "source": "Theregister.com",
        "publishedAt": "2025-04-14T10:32:08Z",
        "full_text": "Interview Those who ignore history are destined to repeat mistakes of the past and, with signs of an inflating bit barn spending bubble, comparisons are being made with the infamous dotcom bust a quarter of a century ago.\n\nRather more recently, Synergy Research Group said in January that mergers and acquisitions of datacenter businesses throughout 2024 swelled to record highs of $73 billion, and yet more investors are lining up for a piece of the action.\n\nMicrosoft puts $1B US datacenter builds on hold amid AI, tariff uncertainty READ MORE\n\nHaving lived through the early days of the internet frenzy, Fabrice Coquio, senior veep at Digital Realty, which bills itself as the world's largest provider of cloud and carrier-neutral datacenter, colocation and interconnection services, is perhaps better placed than most to venture an opinion. Is there a bubble?\n\n\"I have been in this industry for 25 years, so I've seen some ups and downs. At the moment, definitely that's on the very bullish side, particularly because of what people believe will be required for AI,\" he tells The Register.\n\nGrabbing a box of Kleenex tissues, he quips that back at the turn of the millennium, if investors were told the internet was inside they would have rushed to buy it. \"Today I am telling you there is AI inside. So buy it.\"\n\n\"Is there a bubble? Potentially? I see the risk, because when some of the traditional investments in real estate – like housing, logistics and so on – are not that important, people are looking to invest their amazing capacity of available funds in new segments, and they say, 'Oh, why not datacenters?'\"\n\nHe adds: \"In the UK, in France, in Germany, you've got people coming from nowhere having no experiences… that have no idea about what AI and datacenters are really and still investing in them.\n\n\"It's the expression of a typical bubble. At the same time, is the driver of AI a big thing? Yes… [with] AI [there] is a sense of incredible productivity for companies and then for individuals. And this might change drastically the way we work, we operate, and we deliver something in a more efficient way.\n\nMicrosoft, AWS and Google have all sunk billions of dollars into AI, throwing money at promising startups to try to corner the market, and spending on infrastructure to train models or sell the software to customers. So far, it looks as though the market is top heavy and shareholders are starting to get twitchy.\n\nCoquio says: \"Is it going to be quick? I don't think so. I've seen the time needed for cloud. We've got more than 10 years of experience of cloud and cloud is now, today, mature.\"\n\nThe arrival in January of Chinese startup Deepseek indicates that the AI industry – one of the major causes of accelerating datacenter builds – may be built on quicksand. Deepseek made unverifiable claims about the cost of training its models and their performance. The stock market for American corporations operating in AI tanked as investors asked if buying expensive GPUs was the best - or most cost effective - way to create a large language model.\n\nUnperturbed, the biggest funders keep on believing, because at this point they've invested so much, and keeping faith is better than the alternative. Even Alibaba, which recently warned the datacenter market is overheating with too many new sites being constructed, is still going to plough billions into new bit barns.\n\nAccording to McKinsey, the \"future demand\" for dataceter capacity is likely to hinge upon multiple factors that are \"still hard to accurately determine\".\n\nIt says: \"The pace of adoption of advanced-AI use cases will certainly count, but so too will the mix of different types of chips deployed and their associated power consumption, as well as the balance between cloud and edge computing for AI workloads and the typical compute, storage, and network needs of AI workloads.\"\n\nTo cover its back, McKinsey has painted different potential scenarios. It mostly expects global demand for bit barn capacity to expand between 19 to 22 percent annually from 2023 to 2030 \"to reach an annual demand of 171 to 219 gigawatts (GW).\"\n\n\"A less likely yet still possible scenario sees demand rising by 27 percent to reach 298 GW. This contrasts with the current demand of 60 GW, raising the potential for a significant supply deficit. To avoid a deficit, at least twice the data center capacity built since 2000 would have to be built in less than a quarter of the time.\"\n\nManoj Sukumaran, senior principal analyst for data center compute and networking at Omdia, is forecasting the server market to hit $290 billion in 2025, up from $230 billion last year. \"This growth will be driven by the expansion of accelerated computing for AI.\"\n\nHis colleague, Alan Howard, who covers datacenters and colocation services at Omdia, agrees, saying the hyperscalers and the broader colocation data center industry \"will continue to grow aggressively reasonably unhampered.\n\n\"There is a growing sector of property developers chasing GW+ campus projects and that will be a market to watch for any kind of oversaturation. This will take time as master-planned energy campuses are typically a multi-year infrastructure project.\"\n\nOver at Synergy Research Group, chief analyst John Dinsdale, doesn't buy into any comparisons with the dot com era or that the AI and AI DC sectors are too inflated.\n\n\"With AI most of the applications and monetization are real. The leading cloud AI segments are growing by over 100 percent per year and generating some serious money. The dotcom bubble was caused by a frenzy of unrealistic plans and expectations that suckered in investors - many of which should have known better. It was an extreme example of herd mentality causing irrational exuberance.\"\n\nOffering something of a different viewpoint is John David Lovelock, distinguished VP analyst at Gartner, who told us a few months back that the business value from AI remains elusive for many businesses.\n\n\"Our expectations for what generative AI can and will do are starting to come down.\"\n\nDigital Realty does have skin in this game and Coquio agrees AI is an important driver for the datacenter industry, but he says it's not the only one with connectivity, content distribution, satellite comms, digital media in general and, of course, the cloud all playing a part.\n\n\"We are constantly deploying new cloud capacities for Microsoft, Oracle, French companies, German companies, almost every day. AI is another layer. We do serve normal enterprises which are just doing the same thing that they were doing 25 years ago, removing on-prem equipment and putting that in [our] datacenters, so there are constant requirements which are still the same.\"\n\nThat's not to say Digital Realty isn't feeling the impact of wider changes. \"We are generalist, meaning that we provide one rack up to 50/60, megawatts for a single purchase holder. There are 20 companies in the world representing, at the moment, 60 percent of the bookings [across the industry.]\"\n\nThis score of hyperscaler customers is made up of 17 US businesses and three in China, including AWS, Microsoft, Meta, Alibaba and so on.\n\n\"Most of them have completed their time to market initiative to be in multiple areas where they needed multiple subcontractors for colocation services. They are now engaged for many years already in reducing the number of partners, either because they build more and more themselves, or when they need absolutely a collaboration provider, providing connectivity, concentration of customers and so on, they reduce that to the bare minimum,\" Coquio tells The Register.\n\nWith such power hypercalers are demanding that colo providers dealing in commodity services cut prices. Only the biggest will survive, perhaps, or they will get niche to make better profits in countries that are already more developed.\n\n\"There might be some surprises in the US, in Europe, not really in Greece, the market is just developing, but in the flat markets. So England, Germany, France, Holland. There might be some movements at a certain point,\" he says, adding: \"There will be some projects not being really built.\"\n\nWith the high-flying datacenter industry's head in the clouds, maybe there's more than just over-hyped AI tech that could cause it to come crashing back down to Earth. What goes up, typically comes down. The thing that matters is how rapidly and when, so all those involved can at least try to be prepared for the bumpy landing. ®"
    },
    {
        "title": "Palantir, Google Expand Cloud Computing Partnership To Target Federal Agencies",
        "description": "Palantir stock rose amid an expanded cloud computing partnership with Google that targets federal government agencies.",
        "url": "https://www.investors.com/news/technology/palantir-stock-google-stock-fedramp-cloud-partnership/",
        "source": "Investor's Business Daily",
        "publishedAt": "2025-04-24T14:09:23Z",
        "full_text": ""
    },
    {
        "title": "300 billion and counting: most popular chip designer in the world turns 40, and it all started in a wooden barn",
        "description": "40 years on: How Arm’s minimalist chip design became the blueprint for modern computing.",
        "url": "https://www.techradar.com/pro/300-billion-and-counting-most-popular-chip-designer-in-the-world-turns-40-and-it-all-started-in-a-wooden-barn",
        "source": "TechRadar",
        "publishedAt": "2025-04-26T13:27:00Z",
        "full_text": "Acorn’s ARM1 chip kicked off a 40-year computing legacy\n\nArm chips now power over 300 billion devices worldwide and counting\n\n99% of smartphones run on Arm and there's growing adoption in IoT, cloud, and AI workloads\n\nIn April 1985, a small team at Acorn Computers in Cambridge, UK, set out to rethink what a processor could be. Engineers Sophie Wilson and Steve Furber developed the ARM1 (it originally stood for Advanced RISC Machines), an unassuming chip with just 25,000 transistors, to power the BBC Micro, crafting a 32-bit processor that emphasized reduced instruction sets for faster, more efficient computation.\n\nThe design's low power consumption was partially driven by practical constraints, namely the need to run in cheaper plastic packaging. ARM2 soon followed, incorporated into the Acorn Archimedes, the first RISC-based home computer. ARM3 introduced a 4KB cache and further improved performance.\n\nAfter the spin-off from Acorn in 1990, ARM Ltd. was founded as a joint venture between Acorn, Apple, and VLSI. One early commercial success was the Apple Newton, followed by widespread adoption in mobile phones like the Nokia 6110, which featured the ARM7TDMI.\n\n(Image credit: Arm)\n\nLooking to the future\n\nARM6, introduced in 1991, brought full 32-bit processing and an MMU, key to powering GSM mobile phones. In 2005, Armv7 architecture debuted with the Cortex-A8 processor, which brought SIMD (NEON) support and powered many early smartphones.\n\nIn 2011, Armv8 introduced 64-bit support and became the foundation for cloud, data center, mobile, and automotive computing. Features like SVE and Helium pushed performance and AI capabilities further.\n\nThe 2021 launch of Armv9 marked the architecture's shift into AI-centric workloads. It introduced Scalable Vector Extension 2 (SVE2), Scalable Matrix Extension (SME), and Confidential Compute Architecture (CCA).\n\nThese features made it suitable for everything from smartphones with advanced image processing to AI servers handling generative workloads. SME accelerates generative AI and MoE models, while SVE2 brings enhanced AI capability to general-purpose compute.\n\nAre you a pro? Subscribe to our newsletter Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nArm's compute subsystems (CSS), based on Armv9, now serve client, infrastructure, and automotive markets. By integrating CPUs, interconnects, and memory interfaces, these CSS platforms support rapid development of specialized silicon.\n\nFrom the original ARM1 with just 25,000 transistors to today’s Armv9 CPUs packing 100 million gates, the architecture has consistently driven computing forward for four decades. Arm-based chips now power over 300 billion devices worldwide, from tiny embedded sensors to full-scale data centers.\n\nWith 99% of smartphones running on Arm and growing adoption in IoT, cloud, and AI workloads, the architecture continues to scale thanks to its energy-efficient design and flexible licensing model.\n\nLooking ahead, there have been growing rumors that Arm could move beyond licensing and into chip production, something that would put it in competition with its biggest customers. This speculation intensified recently following the acquisition of Ampere Computing, Arm’s only independent server chip vendor, by SoftBank, Arm’s Japanese owner."
    },
    {
        "title": "Containerizing In Edge Computing: A Look At Efficiency And Scalability",
        "description": "While the advantages of containerizing edge workloads are clear, there are several best practices and common pitfalls to keep in mind.",
        "url": "https://www.forbes.com/councils/forbestechcouncil/2025/04/21/containerizing-in-edge-computing-a-look-at-efficiency-and-scalability/",
        "source": "Forbes",
        "publishedAt": "2025-04-21T10:45:00Z",
        "full_text": ""
    },
    {
        "title": "Synadia Response to CNCF",
        "description": "Synadia has always championed open source, and this foundational principle remains unwavering.",
        "url": "https://www.synadia.com/blog/synadia-response-to-cncf",
        "source": "Synadia.com",
        "publishedAt": "2025-04-25T20:37:39Z",
        "full_text": "Nearly 15 years ago, I created NATS as an OSS project. It has been a labor of love, and substantial effort, to guide its growth and witness its widespread adoption.\n\nRecently, there has been public discussion around NATS and its future, notably prompted by communications from the Cloud Native Computing Foundation (CNCF), available here. Synadia acknowledges the CNCF’s commitment to OSS; however, we respectfully disagree with certain characterizations. Our aim is always to move forward constructively, prioritizing support for the vibrant global NATS community. Our mission remains unchanged: to power modern and innovative distributed systems in the cloud and at the edge through a robust, lightweight, and secure messaging technology accessible to all developers and organizations.\n\nMore detailed information about Synadia’s vision for NATS’ future will follow soon, but today I want to clearly articulate several important points.\n\nCommitment to Open Source\n\nSynadia has always championed open source, and this foundational principle remains unwavering. Throughout our discussions with CNCF, we consistently reinforced our commitment to keeping NATS clients and a version of the server open source under the Apache 2.0 license.\n\nWe recognize the unique role Synadia plays in fostering the growth, innovation, funding, and development of NATS since its inception. We never considered blanket relicensing of the codebase. However, we did evaluate a potential commercial server variant.\n\nAt the same time, we must acknowledge the challenging incentive structures within the open source world. Paradoxically, improved stability, extensive documentation, and exemplary reference architectures have led some organizations to discontinue financial support, even when relying heavily on the software in mission-critical environments. As a passionate believer in the open source model, I nonetheless recognize that sustainable OSS requires users who derive substantial value and have the means to support it financially.\n\nFor the NATS ecosystem to flourish, Synadia must also thrive. This clarity has guided our decision-making and planning.\n\nServer Licensing Considerations\n\nSynadia’s customers, partners, and the broader NATS ecosystem derive tremendous value from the features and capabilities of the NATS server. Synadia and its predecessor company funded approximately 97% of the NATS server contributions1. To sustain long-term company and project viability, we explored excluding some advanced features and enhancements from the NATS server and licensing them separately instead. Ultimately, we considered a more community-minded approach: to include them in the NATS server while exploring a BSL license model for future versions. While the BSL is not OSI-approved, it ensures source code remains transparent and publicly accessible, reverting to Apache 2.0 after a defined period (typically 2-4 years). An Apache 2.0 licensed server version will always remain available and supported.\n\nOur intent is to ensure minimal disruption, extending generous usage allowances which permit most users to continue operating as they have been if they choose to use the BSL version of the server. Transparency, essential to maintaining trust, remains a core commitment.\n\nOpen Source Project Profiles\n\nOpen source projects come in various forms: some are heavily funded by multiple large corporations, others emerge from smaller teams or individuals evolving into enterprises. Single-company-backed projects seeking sustainability through commercial offerings naturally face unique challenges.\n\nNATS is primarily funded, supported, and maintained by Synadia. This does not align neatly with the CNCF’s model. Over recent years, it has become apparent that the CNCF may no longer be the best strategic fit for NATS. Rather than face forced archival, Synadia proactively initiated internal discussions with CNCF about a joint announcement regarding a departure, ensuring NATS’ continued health and development.\n\nWhile we understand CNCF’s perspectives on project governance and continuity, we believe a project’s real value lies with its maintainers, contributors, and vibrant community. Their well-being ultimately defines the project’s future.\n\nMoving Forward with Optimism\n\nUnderstandably, license changes evoke uncertainty. We are committed to sharing comprehensive information transparently as our plans solidify. We intended to do this on our own timeline and in conjunction with the CNCF, but felt it was important to provide as much clarity as possible at present.\n\nSynadia remains deeply proud of the NATS community, grateful for the innovations it has enabled, and optimistic about the future. Despite differing perspectives on recent events, we choose to focus constructively on what truly matters: creating exceptional technology and supporting those who depend on it.\n\n=derek"
    },
    {
        "title": "Dashlane’s Omnix gives IT teams new tools to fight credential risk",
        "description": "Dashlane is rolling out a major update to its enterprise tools with the launch of Omnix, a new platform designed to combat AI-driven phishing attacks and shadow IT. The product aims to move IT teams past traditional vault-based password management and gives t…",
        "url": "https://9to5mac.com/2025/04/24/dashlanes-omnix-gives-it-teams-new-tools-to-fight-credential-risk/",
        "source": "9to5Mac",
        "publishedAt": "2025-04-24T12:30:00Z",
        "full_text": "Dashlane is rolling out a major update to its enterprise tools with the launch of Omnix, a new platform designed to combat AI-driven phishing attacks and shadow IT. The product aims to move IT teams past traditional vault-based password management and gives them the tools to manage, detect, and respond to real-time credential threats.\n\nSome of my favorite gear Aqara Smart Lock U50 Upgrade your doors with Apple Home Key and the Aqara U50.\n\nIn a recent Dashlane report, 80% of organizations reported a jump in phishing attacks. Unmanaged GenAI apps also drive a new wave of shadow IT that often bypasses single sign on and 2FA protections. One example is Deepseek, now the second most used GenAI tool by Dashlane users, despite its limited transparency on how the data is being used.\n\n“So much of credential security has been reactive and siloed, where enterprises react to breaches rather than proactively address and mitigate the risk before it metastasizes,” said John Bennett, CEO of Dashlane. “The power of Omnix lies in its pairing of unparalleled insights with action to truly impact behavior and give enterprises the means to build long-term resilience and improve overall security.”\n\nOmnix is aimed at helping to manage the entire lifecycle of a credential-based threat. From identifying weak or compromised passwords to warning users in real time, credential security shifts from a reactive to a proactive model. A built-in AI engine flags suspicious sites to users inside the browser and helps security teams close the gap between detection and resolution.\n\n“Keeping our systems and employees secure from credential threats allows our company to focus on what’s most important—enabling the communities we serve to thrive,“ said Aidan Turner, Manager, Identity and Access Management at Downer Group. “But, reacting to credential threats doesn’t get you very far. With Dashlane, we’ve been able to take a more intelligent, proactive approach to reducing credential risk before it becomes a bigger issue.”\n\nWhat’s new in Dashlane Omnix\n\nProactive protection : Omnix continuously monitors for credential threats across apps, including those outside the control of SSO or corporate policies.\n\n: Omnix continuously monitors for credential threats across apps, including those outside the control of SSO or corporate policies. Enterprise credential management : This gives users a secure vault for every login, not just ones covered by existing identity tools. IT teams get detailed insights and reporting.\n\n: This gives users a secure vault for every login, not just ones covered by existing identity tools. IT teams get detailed insights and reporting. Browser-based visibility : Since most work happens inside web-based SaaS apps, Omnix brings its intelligence to the browser. The Dashlane Smart Extension flags issues in real time.\n\n: Since most work happens inside web-based SaaS apps, Omnix brings its intelligence to the browser. The Dashlane Smart Extension flags issues in real time. Breach-resistant design: The platform is built on Dashlane Secure Cloud, using zero-knowledge architecture and confidential computing. A Secure API is also available for teams that want to integrate Omnix with existing security tools.\n\nWrap up\n\nWe’re probably not prepared for the level of security problems that are going to come from next-generation GenAI apps. The industry is going to have to build a new generation of security tools, and Dashlane is out of the gate with a great addition to its portfolio."
    },
    {
        "title": "Why Intel Deprecated SGX?",
        "description": "The rumors about Intel SGX deprecated in new processors has been confirmed, 12th generation processors (Workstation/Desktop/Laptop/embedded platforms) will deprecate SGX and the SGX will continue to support only in high-end Xeon CPU for server:",
        "url": "https://hardenedvault.net/blog/2022-01-15-sgx-deprecated/",
        "source": "Hardenedvault.net",
        "publishedAt": "2025-05-08T05:32:18Z",
        "full_text": "January 15, 2022 | 6 min Read\n\nThe rumors about Intel SGX deprecated in new processors has been confirmed, 12th generation processors (Workstation/Desktop/Laptop/embedded platforms) will deprecate SGX and the SGX will continue to support only in high-end Xeon CPU for server:\n\nIntel’s official explanation is that final decision is made due to market reasons. Intel SGX (software guard extension) has been shipped with the release of the 6th generation processor Skylake in 2015. Its main purpose is to better solve the trust issue in cloud environments between CSPs (cloud service provider) and tenants. SGX proposes a solution called enclave where OS isn’t able to access to. The technical details of SGX has a lot of controversy since the beginning. This article will explore a bit about SGX:\n\nRoot problem: complexity\n\nIntel SGX is the most sophisticated implementation among the enclave solutions. Intel did a lot of work at multiple levels to leverage the support:\n\nHardware, e.g: MEE\n\nUcode, for special instructions\n\nFirmware, Intel CSME infrastructure upgrade and multiple CSME modules getting involved\n\nSGX basics\n\nSGX hands over paging (EPC) to an untrusted OS, which is similar to BASTION, where the host OS can evict.\n\nSGX uses Intel EPID to implement attestation, which is too complex for microcode to implement.\n\nIn addition to EPID, SGX also uses other CSME code modules such as iclsClient using CLS (Capability Licensing Services)\n\nAttestation process\n\nSeal Secret and Prospering Secret are stored in e-fuses, Provisioning Secret is generated by Intel Key Generation Facility and burned to the CPU and saved in Intel xx service, Seal secret is generated inside the CPU, theoretically unknowable to Intel.\n\nEGETKEY uses certificate-based identify (MRSIGNER, ISVPRODID, ISVSVN) and SGX implementation version (CPUSVN) to get the Provisioning key, which allows the Intel provisioning service to verify that Provisioning Enclave is signed by Intel. The provisioning service can also refuse communication based on CPUSVN to determine if there is a vulnerability.\n\nWhen Provisioning Enclave obtains the Provisioning Key and communicates with the Intel Provisioning service and authenticates itself, the service generates an Attestation key to provision the Enclave, which encrypts the AK using the Prospering Seal key and saves it.\n\nAK uses the EPID cryptography, EPID as a group signature scheme to provide some anonymity for the signer, Intel key provisioning service is the issuer, it will publish the Group Public Key and will save the Master Issuing Key itself, after the provisioning enclave verifies itself to the service, The service generates an EPID member private key as an AK and then executes the EPID Join protocol to join the signature group, after which Quoting Enclave uses the EPID MPK to generate an attestation signature.\n\nRisk： Issued SIGSTRUCT was leaked, and attackers could use the SGX debugging feature to build debugging provisioning or Quoting enclave to modify the code, or get a 128-bit provisioning key to communicate with the Intel service.\n\nAccording to Intel’s patent, the implementation of SGX relies on the complex KDF process inside the CPU circuit for the global secret keys and stored in the eFUSE, Chipworks offers $50-250k to fully extract the eFUSE of one Intel i5 processor, so the eFUSE content is encrypted by a master key (called “global wrapping logic key” in the patent). GWK is used to encrypt a 256-bit message for regenerating the EPID key of the CPU and a handful of 128-bit pre-seed key 0, eFUSE also contains a plaintext copy of 128-bit pre-seed key 1 and a 32-bit EPID group ID, GWK is hard-coded into the chip circuit, all chip manufacturers share the mask set, such a process increases the cost of the attack, But there is also the possibility of being reversed.\n\nSGX also uses PUF to generate symmetric keys for the device during the provisioning phase, the PUF key is encrypted by GWK and transmitted to the key generation server, after which the key generation server encrypts the fuse key of the chip with the PUF key and then transmits it to the chip, the PUF key increases the cost of obtaining the chip fuse key. The attacker must compromise provisioning stage simultaneously.\n\nCSME also has an eFUSE for saving the EPID key for fTPM. The first scheme is that Provision enclave uses the provisioning seal key to encrypt the DAK, which assumes that CSME is an untrusted flash memeory, so fTPM cannot be used. Another option is to use the key agreement protocol to establish a secure communication channel between DMI buses, which ME fw can be used to store DAKs or to implement fTPM.\n\nWrong threat model in the very beginning\n\nIntel SGX puts the owner of on-premise (cloud service vendors, system administrators, etc) into the threat model in the 1st place. Technically, SGX doesn’t trust the operating system and the entire stacks of firmware (except CSME), but Intel might have missed an important common sense: OS kernel may no longer the “CORE” but it’s still the entrance to the “underworld”, as the 0ldsk00l hacker was joking about “don’t make jokes about the underworld”. Most of the attacks targeting SGX are based on having kernel privilege. A proper threat model won’t guarantee a comprehensive solution for security but it could be the starting point at least.\n\nWrong threat model isn’t the only issue in SGX：\n\nOver-design and implementation leads to out-of-control complexity.\n\nLack of transparency, Intel SGX implementation is closed-source in terms of dependencies on the underlying firmware, which means it can’t be audited properly with acceptable cost. While SGX is highly dependent on Intel CSME, yet another issue about “god” mode in CSME can be referred to HardenedVault’s Intel CSME Risk Assessment.\n\nSGX can be used to protect malware, which malware detection become an impossible mission.\n\nIntel didn’t open up SGX-based third-party attestation services to SME client until December 2018, a decision that may have been a little late from a market and ecological perspective.\n\nSGX’s Linux kernel mainline process is slow. in April 2016, Intel submitted the first version of the SGX patch to the Linux kernel community, the Linux kernel community believes that there are many unresolved basic issues, including ABI compatibility issues and SGX as the core of enclave computing assumptions: if the Linux kernel is compromised, SGX can guarantee that the application is not interfered with by attackers. Even if this premise is correct, the kernel developer’s question is: If there is a malicious enclave application, who will protect the kernel? Moreover, the first premise has been denied by the industry after L1TF was exposed (although there were also relevant studies exposed before but the media did not report on a large scale), the exposure of L1TF and cryptocurrency bubble crashed in 2018 broke many people’s silver bullet expectations for SGX. A series of issues delayed the upstreaming until Linux kernel v5.11 merged SGX into mainline in Feb 2021.\n\nLaunch low cost of side-channel attacks with Linux kernel privileges.\n\nOver-hype in the market, this problem may be more prominent in China. The megacorp continues to advocate that SGX can become the “next generation” silver bullet level program, but the reality is that the general principle in infosec is that there is no silver bullet.\n\nIs SGX still an effective security feature?\n\nYes, SGX is still an effective security mechanism to protect your digital assets. Intel has adjusted its expectation for SGX and it’s targeting server-only markets. From the perspective of the production environment, SGX is still a very effective security mechanism that can be utilized to build your own defense-in-depth “Cyber Bunker”."
    }
]