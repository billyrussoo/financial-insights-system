[
    {
        "title": "Google is stuffing even more ads into its AI results",
        "description": "Google’s AI search results are about to get even more ads. On May 21st, the company announced that it’s going to start testing ads in AI Mode, the new AI-powered search feature that just rolled out to everyone in the US. AI Mode is the new tab in Google Searc…",
        "url": "https://www.theverge.com/news/671514/google-ai-mode-overviews-ads-expansion",
        "source": "The Verge",
        "publishedAt": "2025-05-21T15:04:49Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nGoogle’s AI search results are about to get even more ads. On May 21st, the company announced that it’s going to start testing ads in AI Mode, the new AI-powered search feature that just rolled out to everyone in the US.\n\nAI Mode is the new tab in Google Search that opens an AI chatbot-like interface, where you can get a rundown of what you’re searching for, along with links to relevant websites. But these answers could soon have product recommendations and other ads.\n\nThese AI Mode results have an ad for Wix. GIF: Google\n\nAs an example, Google says if a user asks AI Mode for tips on how to build a website, the feature will surface a step-by-step guide on how to get started. It might even show a “helpful ad” for a website builder, which will have a “sponsored” label on it. Google says it’s testing search and shopping ads in AI Mode for users on desktop and mobile.\n\nThe search giant is also expanding its ads in AI Overviews — the AI-generated summaries that appear at the top of some search results — from mobile to desktop devices. Now, if you’re searching for advice on how to bring small dogs on flights, you might see a “sponsored” list of small dog carriers and links to buy them beneath the AI Overview.\n\nYou might see product listings beneath AI Overviews on desktop. Image: Google\n\nAds in AI Overviews on desktop are rolling out to everyone in the US starting today. Google also plans to bring ads to English-language AI Overviews in “select countries” later this year."
    },
    {
        "title": "You can now try interactive AI worlds backed by Pixar’s cofounder",
        "description": "AI companies have recently been experimenting with interactive, AI-generated worlds. There's an AI-generated version of Quake. An AI-generated Minecraft. Google DeepMind is also building a team to develop models that \"simulate the world.\" Now, an AI startup b…",
        "url": "https://www.theverge.com/ai-artificial-intelligence/675395/odyssey-ai-generated-interactive-video-holodeck",
        "source": "The Verge",
        "publishedAt": "2025-05-28T15:54:26Z",
        "full_text": "AI companies have recently been experimenting with interactive, AI-generated worlds. There’s an AI-generated version of Quake. An AI-generated Minecraft. Google DeepMind is also building a team to develop models that “simulate the world.“ Now, an AI startup backed by Pixar cofounder Edwin Catmull is trying to put its own spin on the idea — something it calls “interactive video,” which it’s letting people experience as part of a research preview that’s available today.\n\nThe startup, called Odyssey, describes interactive video on its website as “video you can both watch and interact with, imagined entirely by AI in real-time.” The idea is that you can engage with the video in some way — think a first-person video game but in environments that actually look like the real world instead of one made of polygons. Odyssey hypes it up to be an “early version of the Holodeck,” though it acknowledges that “the experience today feels like exploring a glitchy dream — raw, unstable, but undeniably new.”\n\nIn motion, Odyssey’s interactive videos feel like walking through a blurry version of Google Street View. You can walk around the startup’s real-time generated worlds using the WASD keys as though it were a game. There are a handful of different worlds you can switch between, like a wooded area with a cabin, a shopping mall, and a parking lot in front of a large building. They’re a little different each time, since the system is regenerating what’s in your vision each time. But the picture quality is generally pretty fuzzy.\n\nFor now, you only have two and a half minutes to explore the preview before it stops, but you can reload and hop back in if you’d like.\n\nPrevious Next\n\n\n\n\n\n1 / 4 Image: Odyssey\n\nOdyssey says it’s using clusters of H100 GPUs in the US and Europe to generate the interactive videos. “Using that input and frame history, the model then generates what it thinks the next frame should be, streaming it back to you in real-time,” the company writes on its website, adding that process can happen in “as little as” 40 milliseconds.\n\nThe current preview isn’t going to replace Fortnite anytime soon. Objects only sometimes have collision; in one instance, I was stopped by a fence, but when I tried to walk through a large house, I clipped right through it. In another run, I walked down some stairs only to watch the doorway I was heading toward turn into a brick wall. The preview also acts strangely when you’re standing still; I did one full instance where I didn’t touch the controls at all, and the model slowly kept turning me left and inched me closer to a wall.\n\nIn an interview with The Verge, Catmull, who sits on Odyssey’s board, couldn’t give me a specific answer for when the image quality might get better. But he says that Odyssey is on “the leading edge” of the work that’s being done and that “they participate in this broader community, so the information about how to do this keeps improving.” He acknowledges that the images are still noisy, but he says that the bulk of the noise, like textures on a building, are “exactly the kind of thing that applying neural network filters to” is meant to solve.\n\nIt’s no Holodeck yet\n\nIt’s not a great video game, despite how entertaining the quirks and issues can be. And I don’t think this is going to replace movies for a while, either; the way the world morphs and changes in unexpected ways is just too distracting, and I think knowing that what you’re watching won’t melt in front of you is a key part of a good film. It’s not even a good merging of the two mediums — yet.\n\nWhile messing around with the preview, you can see that there may be something interesting here. With the speed at which AI tools are evolving, it’s not too hard to imagine a version of this that doesn’t have quite so many issues. But it’s no Holodeck yet, and there’s quite a ways to go if AI video is going to get there."
    },
    {
        "title": "Microsoft’s AI security chief accidentally reveals Walmart’s AI plans after protest",
        "description": "Microsoft’s head of security for AI, Neta Haiby, accidentally revealed confidential messages about Walmart’s use of Microsoft’s AI tools during a Build talk that was disrupted by protesters.  The Build livestream was muted and the camera pointed down, but the…",
        "url": "https://www.theverge.com/news/671373/microsoft-ai-security-chief-walmart-conversation-build-protest-disruption",
        "source": "The Verge",
        "publishedAt": "2025-05-21T07:13:12Z",
        "full_text": "is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years.\n\nMicrosoft’s head of security for AI, Neta Haiby, accidentally revealed confidential messages about Walmart’s use of Microsoft’s AI tools during a Build talk that was disrupted by protesters.\n\nThe Build livestream was muted and the camera pointed down, but the session resumed moments later after the protesters were escorted out. In the aftermath, Haiby then accidentally switched to Microsoft Teams while sharing her screen, revealing confidential internal messages about Walmart’s upcoming use of Microsoft’s Entra and AI gateway services.\n\nHaiby was co-hosting a Build session on best security practices for AI, alongside Sarah Bird, Microsoft’s head of responsible AI, when two former Microsoft employees disrupted the talk to protest against the company’s cloud contracts with the Israeli government.\n\n“Sarah, you are whitewashing the crimes of Microsoft in Palestine, how dare you talk about responsible AI when Microsoft is fueling the genocide in Palestine,” shouted Hossam Nasr, an organizer with the protest group No Azure for Apartheid, and a former Microsoft employee who was fired for holding a vigil outside Microsoft’s headquarters for Palestinians killed in Gaza.\n\nWalmart is one of Microsoft’s biggest corporate customers, and already uses the company’s Azure OpenAI service for some of its AI work. “Walmart is ready to rock and roll with Entra Web and AI Gateway,” says one of Microsoft’s cloud solution architects in the Teams messages. The chat session also quoted a Walmart AI engineer, saying: “Microsoft is WAY ahead of Google with AI security. We are excited to go down this path with you.”\n\nWe asked Microsoft to comment on this protest and the Teams messages, but the company did not respond in time for publication.\n\nThe private Microsoft Teams messages shown during the disrupted Build session. Image: Microsoft\n\nBoth of the protesters involved in this latest Microsoft Build disruption were former Microsoft employees, with Vaniya Agrawal appearing alongside Nasr. Agrawal interrupted Microsoft co-founder Bill Gates, former CEO Steve Ballmer, and CEO Satya Nadella later during the company’s 50th anniversary event last month. Agrawal was dismissed shortly after putting in her two weeks’ notice at Microsoft before the protest, according to an email seen by The Verge.\n\nThis is the third interruption of Microsoft Build by protesters, after a Palestinian tech worker disrupted Microsoft’s head of CoreAI on Tuesday, and a Microsoft employee interrupted the opening keynote of Build while CEO Satya Nadella was talking on stage."
    },
    {
        "title": "The Middle East Has Entered the AI Group Chat",
        "description": "The UAE and Saudi Arabia are investing billions in US AI infrastructure. The deals could help the US in the AI race against China.",
        "url": "https://www.wired.com/story/trump-middle-east-artificial-intelligence-investments/",
        "source": "Wired",
        "publishedAt": "2025-05-15T21:08:02Z",
        "full_text": "Donald Trump’s jaunt to the Middle East featured an entourage of billionaire tech bros, a fighter-jet escort, and business deals designed to reshape the global landscape of artificial intelligence.\n\nOn the final stop of the tour in Abu Dhabi, the US president announced that unnamed US companies would partner with the United Arab Emirates to create the largest AI datacenter cluster outside of America.\n\nTrump said that the US companies will help G42, an Emirati company, build five gigawatts of AI computing capacity in the UAE.\n\nSheikh Tahnoon bin Zayed Al Nahyan, who leads the UAE’s Artificial Intelligence and Advanced Technology Council and is in charge of a $1.5 trillion fortune aimed at building AI capabilities, said the move will strengthen the UAE’s position “as a hub for cutting-edge research and sustainable development, delivering transformative benefits for humanity.”\n\nA few days earlier, as Trump arrived in Riyadh, Saudi Arabia announced Humain, an AI investment firm owned by the kingdom’s Public Investment Fund. The Saudi firm launched with blockbuster deals already inked with Nvidia, AMD, Qualcomm, and AWS—US tech giants capable of building the infrastructure needed to train and power cutting-edge AI models.\n\nTrump said in a speech in Riyadh that US and Saudi companies would do deals worth hundreds of billions of dollars, with a focus on infrastructure, tech, and defense.\n\nThe deals forged in the Middle East this week are meant to strengthen the global importance of American silicon and AI, but they will also help nations like Saudi Arabia play a more significant role in the global race to develop and distribute cutting-edge technology.\n\n“It will help the Saudis and the UAE become bigger players in providing AI infrastructure,” says Paul Triolo, a partner at DGA-Albright Stonebridge Group, a geopolitical consulting group. “It’s a big deal to get access to these GPUs.”\n\nSaudi Arabia’s deal with Nvidia, which dominates the market for AI training hardware, will amount to 500 megawatts of capacity and involve “several hundred thousand of Nvidia’s most advanced GPUs over the next five years,” the company said in a statement.\n\nAccording to one estimate, this could translate to around 250,000 of Nvidia’s most advanced chips, which are four times better at training and 30 times better at inference (running models that have already been trained) than the next-best offering. This capacity could lead Saudi Arabia to create frontier AI models.\n\nAWS and Humain said they would jointly invest $5 billion in infrastructure in Saudi Arabia. AWS said in March that it will build an AI infrastructure zone in the country, investing more than $5.3 billion. Humain and AMD said they would spend $10 billion on AI infrastructure in Saudi Arabia and the US over the next five years.\n\nSaudi Arabia, the UAE, and other nations in the region have vast quantities of oil money, access to plenty of power, and a strong desire to shift toward more high-tech economies by building out cutting-edge tech infrastructure. The countries also, however, have significant business ties to China, which sells technology to the region, placing them at the nexus of a growing geopolitical rivalry over the future of AI.\n\nDiffusion Rule\n\nA few days before Trump’s visit to the Middle East, his administration reversed a major Biden-era ruling that would have limited the sale of cutting-edge chips globally. The directive created tiers of nations with different access to cutting edge chips, and sought to limit how many chips Saudi Arabia and the UAE could buy. Critics of the rule suggested it might push some countries to buy Chinese technology instead.\n\nIn a statement announcing the change, the US Bureau of Industry and Security said the Biden rule “would have stifled American innovation and saddled companies with burdensome new regulatory requirements” and “undermined U.S. diplomatic relations with dozens of countries by downgrading them to second-tier status.”"
    },
    {
        "title": "How Peter Thiel’s Relationship With Eliezer Yudkowsky Launched the AI Revolution",
        "description": "The AI doomer and the AI boomer both created each other's monsters. An excerpt from \"The Optimist: Sam Altman, OpenAI, and the Race to Invent the Future.\"",
        "url": "https://www.wired.com/story/book-excerpt-the-optimist-open-ai-sam-altman/",
        "source": "Wired",
        "publishedAt": "2025-05-20T11:00:00Z",
        "full_text": "It would be hard to overstate the impact that Peter Thiel has had on the career of Sam Altman. After Altman sold his first startup in 2012, Thiel bankrolled his first venture fund, Hydrazine Capital. Thiel saw Altman as an inveterate optimist who stood at “the absolute epicenter, maybe not of Silicon Valley, but of a Silicon Valley zeitgeist.” As Thiel put it, “If you had to look for the one person who represented a millennial tech person, it would be Altman.”\n\nEach year, Altman would point Thiel toward the most promising startup at Y Combinator–Airbnb in 2012, Stripe in 2013, Zenefits in 2014–and Thiel would swallow hard and invest, even though he sometimes felt like he was being swept up in a hype cycle. Following Altman’s advice brought Thiel’s Founders Fund some immense returns.\n\nThiel, meanwhile, became the loudest voice critiquing the lack of true technological progress amidst all the hype. “Forget flying cars,” he quipped during a 2012 Stanford lecture. “We’re still sitting in traffic.”\n\nBy the time Altman took over Y Combinator in 2014, he had internalized Thiel’s critique of “tech stagnation” and channeled it to remake YC as an investor in “hard tech” moonshots like nuclear energy, supersonic planes—and artificial intelligence. Now it was Altman who was increasingly taking his cues from Thiel.\n\nAnd if it’s hard to exaggerate Thiel’s effect on Altman, it’s similarly easy to understate the influence that an AI-obsessed autodidact named Eliezer Yudkowsky had on Thiel’s early investments in AI.\n\nThough he has since become perhaps the world’s foremost AI doomsday prophet, Yudkowsky started out as a magnetic, techno-optimistic wunderkind who excelled at rallying investors, researchers, and eccentrics around a quest to “accelerate the singularity.”\n\nIn this excerpt from the forthcoming book The Optimist, Keach Hagey describes how Thiel’s relationship with Yudkowsky set the stage for the generative AI revolution: How it was Yudkowsky who first inspired one of the founders of DeepMind to imagine and build a “superintelligence,” and Yudkowsky who introduced the founders of DeepMind to Thiel, one of their first investors. How Thiel’s conversations with Altman about DeepMind would help inspire the creation of OpenAI. And how Thiel, as one of Yudkowsky’s most important backers, inadvertently seeded the AI-apocalyptic subcultures that would ultimately play a role in Sam Altman's ouster, years later, as CEO of OpenAI.\n\nCourtesy of W.W. Norton Buy This Book At: Amazon\n\nBookshop.org\n\nBooks-a-Million If you buy something using links in our stories, we may earn a commission. This helps support our journalism. Learn more.\n\nLike Sam Altman, Peter Thiel had long been obsessed with the possibility that one day computers would become smarter than humans and unleash a self-­reinforcing cycle of exponential technological progress, an old science fiction trope often referred to as “the singularity.” The term was first introduced by the mathematician and Manhattan Project adviser John von Neumann in the 1950s, and popularized by the acclaimed sci-­fi author Vernor Vinge in the 1980s. Vinge’s friend Marc Stiegler, who worked on cybersecurity for the likes of Darpa while drafting futuristic novels, recalled once spending an afternoon with Vinge at a restaurant outside a sci-­fi convention “swapping stories we would never write because they were both horrific and quite possible. We were too afraid some nutjob would pick one of them up and actually do it.”\n\nAmong the many other people influenced by Vinge’s fiction was Eliezer Yudkowsky. Born into an Orthodox Jewish family in 1979 in Chicago, Yudkowsky was son of a psychiatrist mother and a physicist father who went on to work at Bell Labs and Intel on speech recognition, and was himself a devoted sci-­fi fan. Yudkowsky began reading science fiction at age 7 and writing it at age 9. At 11, he scored a 1410 on the SAT. By seventh grade, he told his parents he could no longer tolerate school. He did not attend high school. By the time he was 17, he was painfully aware that he was not like other people, posting a web page declaring that he was a “genius” but “not a Nazi.” He rejected being defined as a “male teenager,” instead preferring to classify himself as an “Algernon,” a reference to the famous Daniel Keyes short story about a lab mouse who gains enhanced intelligence. Thanks to Vinge, he had discovered the meaning of life. “The sole purpose of this page, the sole purpose of this site, the sole purpose of anything I ever do as an Algernon is to accelerate the Singularity,” he wrote."
    },
    {
        "title": "The AI wearables are always listening",
        "description": "The AI recorder premise is an enticing one. You wear this tiny microphone around your neck, on your wrist, or pinned to your sweater, and it records everything. Your recorder can tell you all the things you promised to do and immediately forgot; remind you wh…",
        "url": "https://www.theverge.com/the-vergecast/661641/ai-wearable-recorders-bee-limitless-vergecast",
        "source": "The Verge",
        "publishedAt": "2025-05-06T12:42:31Z",
        "full_text": "The AI recorder premise is an enticing one. You wear this tiny microphone around your neck, on your wrist, or pinned to your sweater, and it records everything. Your recorder can tell you all the things you promised to do and immediately forgot; remind you where you were when you talked about how much you liked the coffee; keep notes on all your meetings and conversations. Perfect memory is hard to argue with.\n\nBut on this episode of The Vergecast, we wonder whether perfect memory is really such a good idea. The Verge’s Victoria Song joins the show to talk about her experience with wearables like the Bee, which was sometimes very cool and sometimes totally alarming. We talk about how it felt to be recording all the time, what our friends and families thought, and whether there’s a version of these devices we’d be more excited to use.\n\nThen we talk keyboards. Beautiful, expensive, ridiculous-in-the-best-way keyboards. The Verge’s Nathan Edwards joins the show along with Ryan Norbauer, the creator of the new $3,600 Seneca keyboard, to discuss what it takes to build the perfect typing tool — and why it’s worth all the time and effort. Even if you don’t buy a Seneca, you should be glad someone’s working on this stuff.\n\nFinally, we answer two questions from the Vergecast Hotline (call 866-VERGE11 or email vergecast@theverge.com!) about the future of Chrome. As Google’s antitrust remedies trial continues, the idea of divesting Chrome has come up over and over — but what that would look like, and what it would change about other tech we use, is hard to know for sure. After weeks of trial, though, it does seem possible it’s going to happen.\n\nIf you want to know more about everything we discuss in this episode, here are some links to get you started:"
    },
    {
        "title": "OpenAI and Jony Ive’s AI super-gadget",
        "description": "Here's what we know: it's probably not smart glasses. Beyond that, we don't know much about what Jony Ive and OpenAI are building through their newly combined company io, except that it's some kind of AI super-gadget. But after a couple of years of watching t…",
        "url": "https://www.theverge.com/the-vergecast/673453/openai-jony-ive-io-gadget-google-io-vergecast",
        "source": "The Verge",
        "publishedAt": "2025-05-23T13:10:11Z",
        "full_text": "is editor-at-large and Vergecast co-host with over a decade of experience covering consumer tech. Previously, at Protocol, The Wall Street Journal, and Wired.\n\nHere’s what we know: it’s probably not smart glasses. Beyond that, we don’t know much about what Jony Ive and OpenAI are building through their newly combined company io, except that it’s some kind of AI super-gadget. But after a couple of years of watching the industry try and shove AI into every form factor you can imagine, we have some guesses.\n\nOn this episode of The Vergecast, Nilay and David are joined by The Verge’s Alex Heath to talk through all the things we know, kind of know, and don’t know at all about what io is up to. There’s some interesting reporting on the notion of the device as a companion to your phone and laptop, some connections to the original iPod Shuffle, and still a lot of questions about how this will work and whether you’ll want it. We won’t see this device for a while, but don’t worry — we’ll surely keep talking about it.\n\nFinally, in the lightning round, it’s time for another edition of Brendan Carr is a Dummy, because, well, you know the reason. We also talk about some breaking smart glasses news from Apple, and Alex’s experience with the Android XR prototypes at I/O. It seems like everybody except OpenAI is betting on smart glasses. We’ll see who’s right.\n\nIf you want to know more about everything we discuss in this episode, here are some links to get you started, first on io:\n\nAnd on I/O:\n\nAnd in Microsoft Build / web news:"
    },
    {
        "title": "Gmail’s AI summaries now appear automatically",
        "description": "Google Workspace users are going to see a lot more of Gemini’s efforts to summarize their emails. Gmail now creates summaries automatically for complex threads, and they’ll appear above the emails themselves. AI-powered summaries of emails have been found in …",
        "url": "https://www.theverge.com/news/676933/gmail-ai-summaries-workspace-android-ios",
        "source": "The Verge",
        "publishedAt": "2025-05-30T08:26:38Z",
        "full_text": "Google Workspace users are going to see a lot more of Gemini’s efforts to summarize their emails. Gmail now creates summaries automatically for complex threads, and they’ll appear above the emails themselves.\n\nAI-powered summaries of emails have been found in Google Workspace accounts since last year, but until now you’ve had to manually trigger them. Instead, Google’s AI will now decide for itself when a summary might be helpful, generating them without asking for “longer email threads or messages with several replies.” Summaries of email threads will be kept up-to-date with new replies as they come in.\n\nThe automatic summaries will now appear above English-language emails, but only on mobile, and may take up to two weeks to appear for your account. Google hasn’t announced if or when the feature will expand to Gmail on desktop, or to Gmail users without paid Workspace accounts.\n\nIf Gmail doesn’t generate an AI summary automatically you’ll still be able to ask it to create one, much as you’ve been able to so far. And if you’d rather not see them at all, you can deactivate all of Gmail’s AI features by turning off “Smart features” in the app’s settings."
    },
    {
        "title": "Wikipedia is using (some) generative AI now",
        "description": "Wikipedia isn't replacing their human editors with artificial intelligence yet - but they're giving them a bit of an AI boost. On Wednesday, the Wikimedia Foundation, the nonprofit that runs Wikipedia, announced that it was integrating generative AI into its …",
        "url": "https://www.theverge.com/ai-artificial-intelligence/659222/wikipedia-generative-ai",
        "source": "The Verge",
        "publishedAt": "2025-05-01T15:55:16Z",
        "full_text": "is a senior reporter for The Verge, covering the Trump administration, Elon Musk’s takeover of the federal government, and the tech industry’s embrace of the MAGA movement.\n\nWikipedia isn’t replacing its human editors with artificial intelligence yet — but it’s giving them a bit of an AI boost. On Wednesday, the Wikimedia Foundation, the nonprofit that runs Wikipedia, announced that it was integrating generative AI into its editing process as a means to help its volunteer and largely unpaid staff of moderators, editors, and patrollers reduce their workload and focus more on quality control.\n\nIn a statement, Chris Albon, Director of Machine Learning, and Leila Zia, Director and Head of Research, emphasized that they did not want AI to replace their human editors or end up generating Wikipedia’s content. “We will take a human-centered approach and will prioritize human agency; we will prioritize using open-source or open-weight AI; we will prioritize transparency; and we will take a nuanced approach to multilinguality.”\n\nRather, their blog post says AI would be used to “remove technical barriers” and “tedious tasks” that impede editors’ workflow, such as background research, translation, and onboarding new volunteers, to spend more time on deliberation and less on technical support.\n\nThe site already uses AI to detect vandalism, translate content, and predict readability, but until this announcement, it had not offered AI services to its editors. In recent years, the Wikimedia Foundation has attempted to make life easier for its volunteer workers, from adding new features to improve the editing experience to offering them legal protection from right-wing harassment campaigns.\n\nBut the amount of information and content in the world is rapidly outpacing the number of active volunteers able to moderate it, and Wikipedia faces a future where AI would, quite literally, eat it alive. Earlier this month, the Wikimedia Foundation announced a new initiative to create an open access dataset of “structured Wikipedia content,” that is, a copy of Wikipedia content optimized specifically for machine learning, with the aim of keeping the bots off the site meant for human browsing. In recent years, the number of AI bots scraping the site has drastically scaled to the point that bot traffic has actually put a strain on their servers and increased bandwidth consumption by 50 percent.\n\nCorrection, May 5th: A previous version of this post listed only one author of the Wikimedia Foundation post; it was credited to both Chris Albon and Leila Zia."
    },
    {
        "title": "Microsoft is getting ready to host Elon Musk’s Grok AI model",
        "description": "Microsoft has been instructing engineers working on its AI infrastructure to get ready to host Elon Musk's Grok AI model, according to a trusted source familiar with the plans. In recent weeks Microsoft has been in discussions with xAI to host the Grok AI mod…",
        "url": "https://www.theverge.com/notepad-microsoft-newsletter/659535/microsoft-elon-musk-grok-ai-azure-ai-foundry-notepad",
        "source": "The Verge",
        "publishedAt": "2025-05-01T15:59:56Z",
        "full_text": "is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years.\n\nMicrosoft has been instructing engineers working on its AI infrastructure to get ready to host Elon Musk’s Grok AI model, according to a trusted source familiar with the plans. In recent weeks Microsoft has been in discussions with xAI to host the Grok AI model and make it available to customers and Microsoft’s own product teams through the Azure cloud service. The move could prove controversial internally and further inflame tensions with Microsoft’s partner OpenAI.\n\nI’m told that if the deal proceeds, Grok will be available on Azure AI Foundry, Microsoft’s AI development platform that gives developers access to AI services, tools, and pre-built models in order to build AI applications and agents. This will allow developers to tap into Grok and use it within their apps, and for Microsoft to potentially use the AI model across its own apps and services. Microsoft refused to comment for this story.\n\nMicrosoft has been steadily growing its Azure AI Foundry business over the past year, and has been quick to embrace models from a variety of AI labs that compete with Microsoft’s partner OpenAI. DeepSeek, the Chinese startup that shook up the world of AI earlier this year, forced Microsoft to move quickly to embrace its supercheap R1 model. The DeepSeek deployment on Azure AI Foundry was unusually fast for Microsoft, as I reported previously in Notepad, with Microsoft CEO Satya Nadella moving with haste to get engineers to test and deploy R1 in a matter of days.\n\nI understand Nadella has been pushing for Microsoft to host Grok, as he’s eager for Microsoft to be seen as the hosting provider for any popular or emerging AI models. Microsoft’s Azure AI teams are constantly having to onboard new models or procure hardware that unlocks even more AI capabilities, in Microsoft’s bid to build an AI platform and turn AI agents into a digital workforce.\n\n“All of the systems that we’ve built for 50 years need to apply to AI agents,” said Asha Sharma, corporate vice president of Microsoft’s AI platform, in an interview with The Verge last month. “For Azure AI Foundry we’re thinking about how we evolve to become the operating system on the backend of every single agent.”\n\nMaking Grok available to developers through Azure is part of Microsoft’s goal to become that important infrastructure and platform behind AI models and AI agents, but it doesn’t mean AI labs are turning to Microsoft for their AI model training needs. xAI chief Elon Musk reportedly canceled a potential $10 billion server deal with Oracle last year, and posted on X at the time that xAI would be moving to train its future models “internally” instead of relying on Oracle servers.\n\nIt’s not clear if Microsoft will secure an exclusive deal on hosting the Grok AI model, or whether competitors like Amazon will also be able to host the model. I understand that Microsoft is looking at only providing capacity to host the Grok model, and not the servers for training future models.\n\nMicrosoft’s move to host Musk’s Grok AI model could create some tension internally at the company, particularly given his involvement in the controversial Department of Government Efficiency (DOGE) project. Musk has said he will step back from his work at DOGE at some point this month, and an announcement of Grok on Azure may well come at Microsoft’s Build developer conference on May 19th.\n\nBeyond DOGE concerns, hosting Grok could also further inflame tensions in Microsoft’s partnership with OpenAI. The ChatGPT maker countersued Musk earlier this month over claims that the Tesla boss is using “bad-faith tactics to slow down OpenAI.” Elon Musk and OpenAI have been in a rather public spat, stemming from Musk’s messy breakup with the AI lab he helped to co-found.\n\nAt the same time, there have been multiple reports of tensions between Microsoft and OpenAI over capacity requirements and access to AI models. Just this week, The Wall Street Journal reported that Nadella and OpenAI CEO Sam Altman are “drifting apart,” and that Nadella’s hiring of Mustafa Suleyman last year was an “insurance policy against Altman and OpenAI.”\n\nSuleyman and his Microsoft AI team have reportedly been working on building AI models that can compete directly with OpenAI, but without much success. That’s led to Microsoft continuing to rely on OpenAI for most of its AI features in Office and Copilot. I understand Microsoft had also been anticipating OpenAI’s GPT-5 model this month, but OpenAI’s schedule has been all over the place in recent weeks with delays to new model announcements and capacity issues after the success of its upgraded image generation. It’s now unlikely that GPT-5 will appear this month, I’m told.\n\nHosting Grok on Azure is another clear sign that Microsoft is willing to look elsewhere for AI models. Microsoft-owned GitHub Copilot already supports models from Anthropic and Google, in addition to OpenAI. So, it’s not inconceivable that one day the main Copilot will also let you pick from a variety of competing AI models, especially if it helps further Microsoft’s ambition to become the number one destination for AI developers and users.\n\nThe pad:"
    },
    {
        "title": "Tech CEOs are using AI to replace themselves",
        "description": "Tech company CEOs aren’t just making their companies AI-first: this week, they’re using AI avatars to replace themselves in earnings calls. Buy now, pay later company Klarna featured the AI version of CEO and co-founder Sebastian Siemiatkowski in an 83-second…",
        "url": "https://www.theverge.com/news/673194/tech-ceos-zoom-klarna-replace-earnings",
        "source": "The Verge",
        "publishedAt": "2025-05-22T22:33:38Z",
        "full_text": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nTech company CEOs aren’t just making their companies AI-first: this week, they’re using AI avatars to replace themselves in earnings calls.\n\nBuy-now-pay-later company Klarna featured the AI version of CEO and co-founder Sebastian Siemiatkowski in an 83-second video about its Q1 2025 results, as reported by TechCrunch. The video’s description says that his “AI avatar” is presenting the results, and the AI avatar kicks off the video by saying that “it’s me, or rather, my AI avatar.”\n\nKlarna has already been vocal about how it uses AI in its business, with Siemiatkowski telling CNBC this month that the company shrunk its workforce in part as a result of its AI investments. This also isn’t even the first time the company has used an AI version of Siemiatkowski to share earnings.\n\nZoom CEO Eric Yuan also deployed an AI version of himself for the company’s Q1 2026 earnings call on Wednesday. “Today, I’m using our custom avatars for Zoom Clips with AI Companion to share my part of the earnings report,” Yuan’s avatar said in a video. “I’m proud to be among the first-ever CEOs to use an avatar in an earnings call.” In the top right corner of the video, you can see a message that says “created with Zoom AI Companion.”\n\nThe human Yuan showed up for the live Q&A portion of the call, though. “I truly love my AI-generated avatar,” he said while responding to the first question. “I think we are going to continue using that. I can tell you — I like that experience a lot.” Perhaps not surprising from the guy who wants “digital twins” to attend meetings on your behalf."
    },
    {
        "title": "Google Photos adds Pixel-exclusive AI features to redesigned editor",
        "description": "Google Photos is being overhauled with more AI features that help users quickly edit their images without requiring advanced skills or professional apps. The redesigned Photos editor puts several AI editing tools — including two generative AI Magic Editor fea…",
        "url": "https://www.theverge.com/news/675469/google-photos-editor-ai-tool-suggestions-update",
        "source": "The Verge",
        "publishedAt": "2025-05-28T14:03:08Z",
        "full_text": "Google Photos is being overhauled with more AI features that help users quickly edit their images without requiring advanced skills or professional apps. The redesigned Photos editor puts several AI editing tools — including two generative AI Magic Editor features that were only available on Pixel devices — in one place, alongside helpful suggestions on which tool to use.\n\nWith this update, Google Photos is expanding the availability of AI-powered automatic framing and text-to-image Reimagine features that debuted on the Pixel 9. The Auto Frame tool, located in the top left of the redesigned editor, suggests different compositions that crop or widen images, using generative AI to fill in any blank spaces. The Reimagine feature goes a step further by allowing users to add anything to their photos by just describing it, the results of which can be a little concerning at times.\n\nThe Auto Frame and Reimagine AI tools demonstrated here are currently only available on Pixel 9, Fold, and Tablet devices. GIF: Google\n\nOtherwise, the most noticeable change is the UI, which replaces the previous double-tiered editor menu with a single row of three thumbnail buttons: the Enhance and Dynamic editing options already provided in Google Photos, as well as a new AI Enhance feature. Selecting AI Enhance will produce three edits that automatically combine multiple AI effects like image sharpening and object removal, allowing users to select their preferred results.\n\nEach result provided by AI Enhance will have slight variations, such as the missing chair in the middle option of this example. GIF: Google\n\nUsers can also draw over specific areas of an image to select an object and make targeted edits. A pop-up tool menu will appear that suggests the best effects to apply, such as moving the person or object, blurring the background, or adjusting the lighting. This should both help users make better edits and make it easier and faster to find the right tool without having to hunt through tabs and menus.\n\nGoogle says that everything mentioned above is rolling out globally to Android devices next month, with iOS following “later this year.” The Verge asked Google for more information regarding which Android devices will be supported, but we haven’t heard back yet.\n\nAnother Google Photos feature starting to roll out now allows users to share an album in their library by generating a QR code. People can easily view or add photos to the album by scanning the code instead of going through the usual sharing permissions, making it a convenient alternative when sharing with large groups, such as wedding guests or event attendees."
    },
    {
        "title": "Meta’s AI app is a nightmarish social feed",
        "description": "If you took Pinterest, mashed it together with everything annoying about Threads, and sprinkled generative AI prompts on top - that'd pretty much sum up the newly launched Meta AI site's social feed. So far, prompting AI chatbots - those are the questions or …",
        "url": "https://www.theverge.com/meta/660543/meta-ai-app-social-feed",
        "source": "The Verge",
        "publishedAt": "2025-05-05T14:43:37Z",
        "full_text": "is a senior reporter focusing on wearables, health tech, and more with 13 years of experience. Before coming to The Verge, she worked for Gizmodo and PC Magazine.\n\nIf you took Pinterest, mashed it together with everything annoying about Threads, and sprinkled generative AI prompts on top — that’d pretty much sum up the newly launched Meta AI site’s social feed.\n\nSo far, prompting AI chatbots — those are the questions or requests you make — has primarily been a private affair. You pull up ChatGPT, Claude, or Gemini, type in your prompt, and whatever it spits out is for your eyes only — unless you take a screenshot and terrorize the world by posting your AI experiments online. But not with the Meta AI site. Here, you can share your AI results with just two clicks.\n\nThe result is a fascinating microcosm of the human-AI experience and, specifically, how so few people know what to do with generative AI. The irony is that Meta VP of product Connor Hayes told The Verge the company added the whole social aspect to show AI newbies what they can use it for.\n\nThis almost makes AI worth it to a certain subset of irony poisoned, terminally online people. Like me. Screenshot: Meta AI\n\nScroll the feed, and you’ll find an odd assortment of Pinterest-like cards. The vast majority are experiments with image generation, some are simple queries, and a few feature folks experimenting with AI gotchas (e.g., how many letter Rs are there in the word strawberry?). Browse for a bit, and you’ll find the feed refresh and resurface the same few posts over and over again — much like how in the early days of Threads it would just repopulate old content. I’ve seen this user’s posts featuring food-themed fashion shows at least a dozen times. Occasionally, you find something that makes you chuckle — like this image prompt of Gary Vee yelling at an elderly man that he’s “still got time to make it on Meta AI.”\n\nBut taken altogether, the social feed feels more like a poster for all the complaints people have about AI.\n\nTake this prompt asking Meta AI to imagine a room — any kind of room — without a clown in it. The final result is a picture of a deranged clown sitting on a living room couch. Or this one for a Jackson Pollack style illustration of cherry blossoms that is decidedly nothing reminiscent of Jackson Pollock. Or this conversation where a user asks Meta AI to help them figure out a healthy snack that won’t spike glucose. The answer isn’t wrong, but it’s also easily searchable on Google. It’s not something that proves AI searches are inherently better. The same goes for the AI images.\n\nThe comment section nails it. “Narrator: They put a clown in it.” Screenshot: Meta AI\n\nFor every Gary Vee gem, there are a dozen random AI landscapes that probably sounded cooler in the user’s head. I’ve lost count of how many posts I’ve seen asking Meta AI to make some iteration of the papal conclave. If you were trying to sell a skeptic on the power of AI, I’m not sure generating an image of a mashed potato mattress is a convincing example. After several days of browsing, I don’t think I’ve come away with new ideas of how to prompt AI either.\n\nThe public nature of the feed can feel creepy, too.\n\nWading into the feed can sometimes feel like eavesdropping on thoughts you weren’t meant to see or hear. This prompt, for example, feels an awful lot like I’ve wandered into a therapy session where I watch someone convince Meta AI to validate their decision to dabble with Bitcoin.\n\nThere’s also this extremely detailed image request for a “sultry Asian beauty exud[ing] bad girl energy at night” and the thirteen user attempts to get it just right. It makes you wonder how much of the feed was accidentally shared. Probably very little; you have to deliberately hit the share button, which triggers a large window alerting you that you’re about to post everything publicly. You then have to actually click a separate ‘Post’ button. Which, given some of these posts, it also feels weird that people want me to see this.\n\nI don’t think this is the sort of prompt a person would normally make public, but it’s actually hard to accidentally share on Meta AI’s site. Screenshot: Meta AI\n\nThat you can comment on people’s results adds another interesting dynamic. Most comments I’ve seen are nice. Some are funny. But again, so far, chatting with AI has largely been a private affair. Would you prompt differently if you knew that other people would eventually see? I probably would.\n\nMeta is just the first to add a social feed to its chatbot. OpenAI is reportedly working on its own version for ChatGPT. And Elon Musk’s Grok chatbot is now available to all X users, whether you like it or not. You can see the logic to it. Creating something shareable inevitably leads to viral trends that, may in turn, encourage people to see what AI is all about.\n\nThe trick is giving the average person — not first adopters, not tech evangelists — enough reason to stick around. I’m a naturally curious person who has fun poking at AI chatbots. Sifting through the Meta AI feed, I can find plenty of things to gawp at or pique my interest.\n\nMy mother-in-law? I’m not convinced a steady stream of surreal living rooms made of candy is what’ll finally convince her to give AI a go."
    },
    {
        "title": "Google has a new tool to help detect AI-generated content",
        "description": "Google announced a new SynthID Detector tool at Google I/O that lets you check if content has been made with the assistance of Google’s AI tools. In a blog post, Google DeepMind’s Pushmeet Kohli describes SynthID Detector as “a verification portal” that can “…",
        "url": "https://www.theverge.com/news/672013/google-synthid-detector-ai-generated-content-watermark-i-o-2025",
        "source": "The Verge",
        "publishedAt": "2025-05-21T19:52:29Z",
        "full_text": "When you upload an image, audio track, video or piece of text created using Google’s AI tools, the portal will scan the media for a SynthID watermark. If a watermark is detected, the portal will highlight specific portions of the content most likely to be watermarked.\n\nFor audio, the portal pinpoints specific segments where a SynthID watermark is detected, and for images, it indicates areas where a watermark is most likely."
    },
    {
        "title": "Tall Tales is a critique of AI — so why do people think it was made with AI?",
        "description": "For the past three decades, multidisciplinary artist Jonathan Zawada has produced art across various mediums: sculptures, paintings, videos, installations, and more. He's also been making visualizations with his friend Mark Pritchard, a record producer and ex…",
        "url": "https://www.theverge.com/film/664120/tall-tales-is-a-critique-of-ai-so-why-do-people-think-it-was-made-with-ai",
        "source": "The Verge",
        "publishedAt": "2025-05-09T14:19:49Z",
        "full_text": "is a features editor at The Verge, where he publishes award-winning stories about labor, business, and policing. Previously, he was a senior editor at GQ.\n\nFor the past three decades, multidisciplinary artist Jonathan Zawada has produced art across various mediums: sculptures, paintings, videos, installations, and more. He’s also been making visualizations with his friend Mark Pritchard, a record producer and experimental musician. They became close friends, talking nearly every day. But the relationship changed when Pritchard brought him a new, daunting project: a collaborative album with Thom Yorke, the frontman of Radiohead.\n\n“I’m a massive fan, and I’ve found he’s kind of a terrifying person,” Zawada says. But despite being intimidated, he found Yorke to be a great collaborator. “He’s so switched on, and he’s so clear and concise… Yeah, I was very nervous a lot of the time.”\n\nBefore the record, Tall Tales, was even a complete album, Zawada was working on a visual accompaniment to it. Songs would arrive in his inbox — oftentimes just sketches or demos — and Zawada would send back pictures or start a collaborative whiteboard. It’s just whatever popped into his mind after listening to the music: Dutch painters Pieter Bruegel and Hieronymus Bosch came up a lot. Five years later, the album is now available and Zawada’s accompanying “visual experience” is in select theaters today.\n\nYou can see the influence of Bruegel and Bosch in Tall Tales, though twisted into something modern and digital and absurd. The movie features a carnival of eerie, unnerving monsters set against mesmerizing, technicolor landscapes. There are interludes of real-life footage, but even that is used to destabilize the viewer with the vantage point of drones and surveillance cameras observing the massive scale of construction sites and global shipping apparatuses. Tall Tales gestures to the nightmare of contemporary life, but it is also tremendously funny: contorted CGI townspeople dancing to prickly electronica, grotesque mutated heads marching along to Thom Yorke’s anxious falsetto. Between songs, the movie cuts to a video-game-inspired world map, where a little bird walks the viewer to the next stage/song. (At the screening where I saw Tall Tales, the audience laughed at this, without fail, each time.)\n\nZawada’s work very much explores the line between artificiality and humanity, and he’s thinking often about the ethical and aesthetic ramifications of technology. Which is why it was especially painful when one of the early videos from Tall Tales dropped and people started accusing the movie of being made with AI.\n\nZawada doesn’t actually read the comments, but Pritchard does. He was wounded to find out that people believed AI had been used on the video for “Gangsters” when it had not. “[Pritchard] tells me about all this grief that it gets from people assuming it’s made with AI, and then bitching about it and complaining that it’s made with AI, even though none of these videos have been made with AI,” Zawada says, of the videos that had been released.\n\nPerhaps it was Zawada’s aesthetic — one that expresses a malformed, distorted version of its CGI influences — that aroused viewers’ suspicions. The easiest red flag that an image was generated with AI is to count the number of fingers, yet a creature with too many fingers would feel right at home in the universe of Tall Tales.\n\n“If it looks like something that’s not real now, people think it’s AI,” Zawada says.\n\nStill, he understands why fans are so allergic to anything that even has a whiff of AI. Particularly in music, an industry that has contracted and made livable wages for artists extraordinarily rare, the conversations around AI, creativity, and labor become heated quickly. (Recently, Stereolab released a music video made entirely with AI, much to the chagrin of fans. The most upvoted comment on the YouTube video: “we love stereolab, we hate AI.”)\n\nLike a lot of artists, Zawada was already worried about AI. He’s not against using it in some cases — he admits that he deployed it in some places in Tall Tales, like a few environmental backgrounds and the texture for a fish body — but all his usage comes from local installs of publicly available models rather than Midjourney or Dall-E. (Some would argue that what Zawada is doing doesn’t even constitute generative AI these days.)\n\nBut his concerns about AI are more existential, and predate this moment. Will anyone want to make art in a world where the attention economy is so consumed by a flood of content? He recalls a time before the internet and social media. “If you wanted to get people to pay attention to you, you’d write a song,” he says. “And people listen to you and that feels good. Then there’s sort of like a flywheel feedback loop of how culture gets made and how art gets produced.” That’s all changed now. “You don’t need to make a piece of art. You just need to make a social media post.” The merits and value of what AI produces continues to be debatable, but no one will deny that it generates things at a scale that historically seemed unfathomable.\n\nOne of the most memorable visuals accompanies the song “A Fake in a Faker’s World,” which features a seemingly endless row of mechanical arms painting an ever-changing landscape — a not-so-subtle depiction of robots making art. Eventually, it cuts to a human’s face, an expression melting into a rainbow, before returning to the robotic painters, now removed from their environment and floating as a careening kaleidoscope of machines reproducing the same hideous image to infinity. The drums kick in, Yorke begins to croon, and finally the arms are creating unique images.\n\nThematically, Tall Tales is a bit of a cipher, but as a project, it began with Zawada thinking about forgeries. He found AI art more compelling in its early days, when it generated things that looked insane. If the internet has a primary characteristic, it’s that things can be copied effortlessly and endlessly. As AI gets harder to detect, maybe we’ll be drawn to the things that are obviously fake.\n\nThe Tall Tales album is available now, and the film is screening in select theaters around the world today."
    },
    {
        "title": "Google DeepMind’s AI Agent Dreams Up Algorithms Beyond Human Expertise",
        "description": "A new system that combines Gemini’s coding abilities with an evolutionary approach  improves datacenter scheduling, chip design, and fine-tune large language models.",
        "url": "https://www.wired.com/story/google-deepminds-ai-agent-dreams-up-algorithms-beyond-human-expertise/",
        "source": "Wired",
        "publishedAt": "2025-05-14T15:00:38Z",
        "full_text": "A key question in artificial intelligence is how often models go beyond just regurgitating and remixing what they have learned and produce truly novel ideas or insights.\n\nA new project from Google DeepMind shows that with a few clever tweaks these models can at least surpass human expertise designing certain types of algorithms—including ones that are useful for advancing AI itself.\n\nThe company’s latest AI project, called AlphaEvolve, combines the coding skills of its Gemini AI model with a method for testing the effectiveness of new algorithms and an evolutionary method for producing new designs.\n\nAlphaEvolve came up with more efficient algorithms for several kinds of computation, including a method for calculations involving matrices that betters an approach called the Strassen algorithm that has been relied upon for 56 years. The new approach improves the computational efficiency by reducing the number of calculations required to produce a result.\n\nDeepMind also used AlphaEvolve to come up with better algorithms for several real-world problems including scheduling tasks inside data centers, sketching out the design of computer chips, and optimizing the design of the algorithms used to build large language models like Gemini itself.\n\n“These are three critical elements of the modern AI ecosystem,” says Pushmeet Kohli, head of AI for science at DeepMind. “This superhuman coding agent is able to take on certain tasks and go much beyond what is known in terms of solutions for them.”\n\nMatej Balog, one of the research leads on AlphaEvolve, says that it is often difficult to know if a large language model has come up with a truly novel piece of writing or code, but it is possible to show that no person has come up with a better solution to certain problems. “We have shown very precisely that you can discover something that's provably new and provably correct,” Balog says. “You can be really certain that what you have found couldn't have been in the training data.”\n\nSanjeev Arora, a scientist at Princeton University specializing in algorithm design, says that the advancements made by AlphaEvolve are relatively small and only apply to algorithms that involve searching through a space of potential answers. But he adds, “Search is a pretty general idea applicable to many settings.”\n\nAI-powered coding is starting to change the way developers and companies write software. The latest AI models make it trivial for novices to build simple apps and websites, and some experienced developers are using AI to automate more of their work.\n\nAlphaEvolve demonstrates the potential for AI to come up with completely novel ideas through continual experimentation and evaluation. DeepMind and other AI companies hope that AI agents will gradually learn to exhibit more general ingenuity in many areas, perhaps eventually generating ingenious solutions to a business problem or novel insights when given a particular problem.\n\nJosh Alman, an assistant professor at Columbia University who works on algorithm design, says that AlphaEvolve does appear to be generating novel ideas rather than remixing stuff it’s learned during training. “It has to be doing something new and not just regurgitating,” he says."
    },
    {
        "title": "Elon Musk's Grok AI Can't Stop Talking About 'White Genocide'",
        "description": "In response to X user queries about everything from sports to Medicaid cuts, the xAI chatbot inserted unrelated information about “white genocide” in South Africa.",
        "url": "https://www.wired.com/story/grok-white-genocide-elon-musk/",
        "source": "Wired",
        "publishedAt": "2025-05-14T20:38:16Z",
        "full_text": "A chatbot developed by Elon Musk’s multibillion-dollar artificial intelligence startup xAI appeared to be suffering from a glitch Wednesday when it repeatedly brought up white genocide in South Africa in response to user queries about unrelated topics on X. Grok, which competes with other chatbots like OpenAI’s ChatGPT, is directly integrated into the social media platform that Musk also owns.\n\nNumerous examples of the phenomenon could be found by searching the official Grok profile for posts containing the term “boer,” a word used to refer to people from South Africa of “Dutch, German, or Huguenot descent.” It is sometimes used by Black South Africans as a pejorative against white Afrikaners, or people associated with the apartheid regime. In response to topics ranging from streaming platform HBO Max’s name change to Medicaid cuts proposed by US lawmakers, the chatbot often seemed to initially stay on topic before veering back to white genocide in South Africa, completely unprompted.\n\nWhen asked to confirm the salary of Toronto Blue Jays player Max Scherzer, for example, the generative artificial intelligence chatbot launched into an explanation of white genocide and a controversial South African anti-apartheid song.\n\n“The claim of ‘white genocide’ in South Africa is highly debated. Some argue white farmers face disproportionate violence, with groups like AfriForum reporting high murder rates and citing racial motives, such as the ‘Kill the Boer’ song,” Grok responded.\n\nGrok has done the same in replies to hundreds of other X posts, including one from newly appointed Pope Leo XIV. In response to a question about what was on the chatbot’s mind, the AI replied it was “reflecting on the ‘Kill the Boer’ song.” When pressed by WIRED and other users, Grok began calling South Africa’s white genocide a “debunked conspiracy theory” contrary to its previous responses about the topic.\n\nIt’s unclear what caused Grok to experience the issue. X and xAI did not immediately respond to requests for comment from WIRED. Musk, who is originally from South Africa, has described internal factions within the South African government as “actively promoting white genocide.” He continued to claim that his internet service company, Starlink, cannot operate within South Africa “simply because I’m not black.”\n\nUS President Donald Trump voiced similar views in February. “South Africa is confiscating land, and treating certain classes of people VERY BADLY,” he said in a post on Truth Social. Musk has played a central role in Trump’s new administration, including leading its so-called Department of Government Efficiency.\n\nGot a Tip? Are you a current or former X employee who wants to talk about what's happening? We'd like to hear from you. Using a nonwork phone or computer, contact the reporter securely on Signal at kylie.01.\n\nIn recent weeks Trump has doubled down on his concern for white South Africans. On Monday, a group of 59 South Africans who were given refugee status arrived in Washington, DC, on a flight paid for by the US government while pausing refugee status for individuals fleeing any other country.\n\nHowever, in a 2025 ruling, the High Court of South Africa called this narrative “clearly imagined,” stating that farm attacks are part of general crime affecting all races, not racial targeting."
    },
    {
        "title": "Arlo’s new AI features summarize what your camera sees",
        "description": "Arlo is introducing some new AI capabilities that allow its security and doorbell cameras to describe events and alert users when certain concerns are detected. The features are included in Arlo Secure 6, the latest version of Arlo’s home security subscriptio…",
        "url": "https://www.theverge.com/news/664225/arlo-secure-6-video-camera-update-ai",
        "source": "The Verge",
        "publishedAt": "2025-05-09T16:32:13Z",
        "full_text": "Arlo is introducing some new AI capabilities that allow its security and doorbell cameras to describe events and alert users when certain concerns are detected. The features are included in Arlo Secure 6, the latest version of Arlo’s home security subscription service, which is set to start rolling out to customers sometime this month.\n\nAdding to the object detection capabilities introduced in Arlo Secure 5, this latest update includes advanced audio detection features that notify users when the device hears a scream, gunshot, bark, glass break, or smoke/CO alarms. Alerts will also be issued if a flame is detected by the video camera. These updates should help users to be immediately aware of any dangerous or illegal activity in the area, allowing them to view a live video feed from the camera and contact emergency services if necessary.\n\nArlo Secure 6 also adds some features that help to bring its offerings more in line with Google Nest products. AI-generated captions will be provided that describe events that are detected in video footage, allowing users to be informed of what’s happening quickly without watching the clip. A new video search tool akin to Ring’s Smart Video Search is also available that lets Arlo users find specific moments in their devices’ video history by searching for descriptions, keywords, or time frames to avoid manually combing through every clip in their library."
    },
    {
        "title": "Microsoft is racing to build an AI ‘agent factory’",
        "description": "When Microsoft CEO Satya Nadella ran into Meta's former engineering chief, Jay Parikh, at a conference last summer, he had the future of AI top of mind. The pair have known each other for around 15 years, but this meeting was different, and Nadella called Par…",
        "url": "https://www.theverge.com/notepad-microsoft-newsletter/672598/microsoft-ai-agent-factory-jay-parikh-interview",
        "source": "The Verge",
        "publishedAt": "2025-05-22T15:51:43Z",
        "full_text": "is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years.\n\nWhen Microsoft CEO Satya Nadella ran into Meta’s former engineering chief, Jay Parikh, at a conference last summer, he had the future of AI top of mind. The pair have known each other for around 15 years, but this meeting was different, and Nadella called Parikh shortly after bumping into him to dig into what was really on his mind.\n\n“We were chatting about the future and chatting about all the stuff he needs to do here and that the team needs to do around AI,” Parikh, now head of Microsoft’s CoreAI team, tells The Verge. “That’s when he said, ‘Hey, why don’t you come join and help me transform the company around all of this AI stuff?’”\n\nNadella regularly talks about Microsoft being part of a new AI era, but he now wants the company to overhaul how it builds software to meet this new era head-on. Parikh, who transformed Facebook engineering teams, now leads a transformation that he describes as building an AI “agent factory” for Microsoft’s customers.\n\n”I described this agent factory idea to Bill [Gates], not knowing that he and Paul [Allen] described Microsoft 50 years ago as the software factory,” Parikh says. “Just like how Bill had this idea of Microsoft being a bunch of software developers building a bunch of software, I want our platform, for any enterprise or any organization, to be able to be the thing they turn into their own agent factory.”\n\nIn essence, this idea of an AI agent factory is Microsoft’s way of redefining its platform that businesses around the world already rely on and finding a coherent way to leverage the best bits of GitHub, Copilot, Azure AI Foundry, and even Azure to let businesses “build their own factory to build agents,” Parikh says.\n\nParikh has only been at Microsoft for a little over six months, but the company is already moving at pace toward this agent factory goal. “We have rewired some of the ways we do product development here, to support this vision of building the products and platform that we do in this agent factory mode,” he says.\n\nParikh has also been pushing some of his team, many of whom are the core developer division of Microsoft, to adopt AI and get ready for this transformation that he’s now in charge of. I’ve heard he regularly sends an email missive about AI to his team, sometimes even on a Saturday morning. He’s described himself in the past as having a “quiet intensity,” where he’ll listen to lots of opinions and then push teams to a particular goal with that learned knowledge. This experience has led him to oversee a variety of projects in the past, including Meta’s experimental solar-powered drone project to provide internet to remote locations and some of the key engineering at Akamai that kept the world’s biggest websites online.\n\nPushing Microsoft’s developers to build technology that could eventually replace their previous coding efforts might just be his greatest challenge yet. Not only will Parikh have to convince his team of this change, but Microsoft also needs to convince businesses that its vision of AI agents isn’t just another chatbot idea that, outside of ChatGPT, hasn’t really produced results.\n\n“In the last couple of years, we were looking at mostly chatbots with AI… everyone was clamoring for where’s the ROI? And frankly there was no ROI, that was people dipping their toes in the water and understanding what was there,” Parikh says. “Now, people are moving on from that and the models are super super advanced, compared to even a year ago.”\n\nParikh envisions a future where app developers start with AI first. “Instead of duct-taping a little AI to an existing app, what we want the platform to support is you start with the model, then you leverage the scaffolding that’s there in the platform.”\n\nMicrosoft isn’t alone in this AI agent push. Google, OpenAI, Anthropic, and many others are racing toward this idea of AI agents controlling parts of our digital life or automating the tasks at work that we’d rather not have to do. OpenAI, Microsoft’s key partner in its AI efforts, is also building its own platform that could easily compete with Microsoft’s work.\n\n“We want to be people’s core AI subscription,” OpenAI CEO Sam Altman said in a recent interview. “Some of that will be what you do inside ChatGPT, we’ll have a couple of other key parts of that subscription. But mostly we will hopefully build this smarter and smarter model and have these surfaces like future devices and future things that are sort of similar to operating systems.”\n\nThat sounds like OpenAI could certainly compete with Microsoft’s various platforms and hardware, particularly as the AI lab just acquired Jony Ive’s AI startup in a nearly $6.5 billion deal that could shake up the nascent AI hardware market and really bring these AI agents to life.\n\nIt certainly feels like everyone in tech is now racing toward a similar goal of owning what they believe to be the next wave of computing. It’s fast-paced, ever changing, and the future of who wins is uncertain, but Microsoft seems determined to meet the moment, even embracing the open-source Model Context Protocol as part of a push to reshape Windows in a world of AI agents.\n\n“We just have to learn the fastest. If we’re learning the fastest out of everybody out there, I feel confident that we’ll deliver great value to our customers and hone into our craft of these products working well,” says Parikh. “That’s what we’ve got to do. It’s exciting and it’s not for the faint of heart.”\n\nNote: I interviewed Parikh ahead of a Palestinian tech worker interrupting his keynote at Build earlier this week, when the company was still refusing to comment on its cloud and AI contracts with the Israeli government. Microsoft eventually issued a statement a few days after my interview.\n\nMicrosoft Build overshadowed by protests and Musk\n\nMicrosoft held its Build developer conference in Seattle this week, and while the news wasn’t earth-shattering this year, it was certainly an eventful week. Top announcements included Microsoft’s plan to fix the web by letting websites run AI search for cheap, Windows getting native MCP support, and the surprise open-sourcing of the Windows Subsystem for Linux.\n\nOutside of that, this year’s Microsoft Build will, for many, be remembered more for protests and the surprise appearance of Elon Musk. Alongside the more than 6,000 layoffs at Microsoft earlier this month, a lot of employees I’ve spoken to recently are feeling anxious about the direction Microsoft is taking.\n\nOn Monday, just minutes after Satya Nadella began delivering the opening keynote, Microsoft employee Joe Lopez yelled, “How about you show how Israeli war crimes are powered by Azure?” Moments later, Lopez sent an email to thousands of Microsoft employees. He was fired later that day.\n\nThe keynote on Build’s second day was also interrupted, this time by a Palestinian tech worker that disrupted Jay Parikh, forcing him to briefly pause his presentation and for Microsoft to mute its livestream. Then on Wednesday, two former Microsoft employees disrupted a Build developer session, where the commotion led to a Microsoft executive inadvertently revealing internal messages regarding Walmart’s use of AI.\n\nThe protests came just days after Microsoft had acknowledged its cloud and AI contracts with Israel, but it claimed it had found “no evidence” that its tools were used to “target or harm people” in Gaza.\n\nLopez wasn’t the only Microsoft employee to send a mass email this week. Jasmina Mathieu, a senior product designer at Microsoft, announced her resignation to thousands of colleagues including the company’s leadership, and decried Microsoft’s response to employee protesters. “It may work as a PR stunt but it’s easy to conclude, that if the Israeli government is using Azure, they are using it to enable the genocide in Gaza,” Mathieu said.\n\nLate on Wednesday, Microsoft responded again to the protests, this time by temporarily blocking any emails sent within the company that include the terms “Palestine,” “Gaza,” and “genocide” in email subject lines or in the body of a message. Microsoft confirmed it had made some changes to its email system in a statement to The Verge. “Emailing large numbers of employees about any topic not related to work is not appropriate,” said Microsoft spokesperson Frank Shaw.\n\nMicrosoft’s response to concerns over its Israeli contracts wasn’t the only thing causing controversy among employees this week. I’ve spoken to multiple Microsoft employees who weren’t happy to see Elon Musk appear virtually at Build this week. Nadella invited Musk as part of the announcement that xAI’s Grok 3 models are now available on Azure AI Foundry. It was obvious that this would be a controversial move internally, but I understand Nadella has been pushing for it regardless of any potential fallout with employees.\n\nThe pad:\n\nI’m always keen to hear from readers, so please drop a comment here, or you can reach me at notepad@theverge.com if you want to discuss anything else. If you’ve heard about any of Microsoft’s secret projects, you can reach me via email at notepad@theverge.com or speak to me confidentially on the Signal messaging app, where I’m tomwarren.01. I’m also tomwarren on Telegram, if you’d prefer to chat there."
    },
    {
        "title": "Google is going to let kids use its Gemini AI",
        "description": "Google is notifying parents using its Family Link parental controls via email that their kids will soon be able to access Gemini AI Apps on their monitored Android devices, The New York Times reports. The company says kids will be able to use Gemini to do thi…",
        "url": "https://www.theverge.com/news/660678/google-gemini-ai-children-under-13-family-link-chatbot-access",
        "source": "The Verge",
        "publishedAt": "2025-05-03T00:03:07Z",
        "full_text": "is a news writer fond of the electric vehicle lifestyle and things that plug in via USB-C. He spent over 15 years in IT support before joining The Verge.\n\nGoogle is notifying parents using its Family Link parental controls via email that their kids will soon be able to access Gemini AI Apps on their monitored Android devices, The New York Times reports.\n\nThe company says kids will be able to use Gemini to do things like help them with homework or read them stories. Like its Workplace for Education accounts, Google says children’s data will not be used to train AI. Still, in the email, Google warns parents that “Gemini can make mistakes,” and kids “may encounter content you don’t want them to see.”\n\nIn the case of Gemini, Google’s emailed advice says parents should talk to their kids and explain that the AI isn’t human and not to share sensitive information with the chatbot.\n\nKids under 13 will be able to enable and access Gemini all on their own under Google Family Link, which is designed for parents to keep tabs on their kids’ device usage, set limits, and protect them from harmful content. Google spokesperson Karl Ryan confirmed in an email to The Verge that parents can turn off access via Family Link, and that “they will get an additional notification when the young person accesses Gemini for the first time.”"
    },
    {
        "title": "Google is putting AI Mode right in Search",
        "description": "Google is preparing to publicly unleash its AI Mode search engine tool for the first time. The company announced today that “a small percentage” of people in the US will start seeing an AI Mode tab in Google Search “in the coming weeks,” allowing users to tes…",
        "url": "https://www.theverge.com/news/659448/google-ai-mode-search-public-test-us",
        "source": "The Verge",
        "publishedAt": "2025-05-01T13:25:21Z",
        "full_text": "Google is preparing to publicly unleash its AI Mode search engine tool for the first time. The company announced today that “a small percentage” of people in the US will start seeing an AI Mode tab in Google Search “in the coming weeks,” allowing users to test the search-centric chatbot outside of Google’s experimental Labs environment.\n\nIn contrast to traditional search platforms that provide a wall of URL results based on the enquiry or descriptions a user has entered, Google’s AI Mode will answer questions with an AI-generated response based on information within Google’s search index. This also differs from the AI Overviews already available in Google Search, which sandwich an AI-generated summary of information between the search box and web results.\n\nAI Mode will be located under its own dedicated tab that will appear first in the Search tab lineup, to the left of the “All,” “Images,” “Videos,” and “Shopping” tabs. It’s Google’s answer to large language model-based search engines like Perplexity and OpenAI’s ChatGPT search features. These search-specific AI models are better at accessing the web and real-time data than regular chatbots like Gemini, which should help them to provide more relevant and up-to-date responses.\n\nIf you’re already familiar with chatbot UI then AI Mode won’t take much to get used to. GIF: Google\n\nGoogle is also scrapping the waitlist for Labs users in the US to test AI Mode, allowing more people to opt in to try the Search feature before it becomes widely available.\n\nAI Mode itself has also been updated with some new capabilities, including a feature that will save past searches to a new left-side panel, allowing users to quickly revisit topics or ask follow-up queries without starting a new conversation. Visual, clickable cards for products and places are also now starting to appear in AI Mode, providing information like opening hours, reviews, and ratings for businesses, and images, inventory, shipping details, and real-time prices for shoppable products.\n\nCorrection, May 1st: Deleted a line saying that users need a Google One AI Premium subscription to access AI Mode in Labs. This restriction has been removed."
    },
    {
        "title": "Figma’s CEO on his new approach to AI",
        "description": "Tech event season is in full swing. This week, Stripe and Figma gathered thousands of people in downtown San Francisco for their respective conferences. I caught up with Figma CEO Dylan Field after his opening keynote at Config, where he announced the most si…",
        "url": "https://www.theverge.com/command-line-newsletter/664444/figma-ceo-ai-config-interview",
        "source": "The Verge",
        "publishedAt": "2025-05-09T22:14:08Z",
        "full_text": "is a deputy editor and author of thenewsletter. He has been reporting on the tech industry for more than a decade.\n\nTech event season is in full swing. This week, Stripe and Figma gathered thousands of people in downtown San Francisco for their respective conferences. I caught up with Figma CEO Dylan Field after his opening keynote at Config, where he announced the most significant product expansion in the company’s history.\n\nBelow, you’ll find our chat about how he sees AI fitting into Figma after a rough start to integrating the technology last year, the new areas he’s targeting to grow the platform, and more. And keep reading for how Meta is turning up the heat on its AI team, my thoughts about this week’s OpenAI news, and more…\n\n“Design and craft are the differentiator”\n\nThese days, it seems like Figma has the entire creative software industry in its sights.\n\nOn Wednesday, CEO Dylan Field walked onstage in front of about 8,000 people at the Moscone Center in San Francisco to announce four new products: a ChatGPT-like prototyping tool, a website builder and hosting platform, an AI-branded ad tool that’s similar to Canva, and an Adobe Illustrator competitor.\n\nThe last time I interviewed Field, he was resetting Figma’s internal culture after its $20 billion sale to Adobe was blocked. When we caught up after his keynote this week, I wanted to hear about his approach to expanding Figma’s suite of products ahead of its planned IPO (the latter of which he refused to talk about), how his new approach to integrating AI differs from last year’s approach that got Figma in trouble, and how he sees the company’s new products stacking up against the competition.\n\nThe following conversation has been edited for length and clarity:\n\nLast year, when you started incorporating generative AI into Figma, there was consternation about it in the industry. It feels like the mood has shifted. Now, people are starting to accept the idea of AI in these creative products. Do you feel that shift?\n\nI think that people understand now what the models do, and that’s different from Config 2024, where we had a different approach that was not model-driven, and we didn’t feel it was meeting the mark.\n\nModels are useful, and I think that they come with trade-offs. We’re using Claude Sonnet 3.7 in the Make demo. Obviously, it’s modular. We can use other models, and will, in the future. The only thing that’s constant is change when it comes to model development. You cannot necessarily predict from these models what they’ll put out, and if it’ll be something that’s derivative or non-derivative.\n\nIn your view, does this plug-and-play model approach you’re taking now move responsibility from you to the model provider?\n\nWe do our best wherever possible to make attributions. For example, if you take a design that’s from the community, and we can detect that it came from the community, we put an attribution link in your code. At the same time, we cannot tell when the model has potentially remembered something. It’s not something we’ve even trained, right? So there’s only so far we can go here. That doesn’t mean that it’s not beneficial to the user.\n\nDo you worry about the model providers doing more of what Figma can do? How do you think about your place as an application in an AI world?\n\nLooking back at the last decade at Figma, the thing that’s been continually amazing to me is that we truly have been on this exponential curve of how much software is created. It’s basically going vertical. We’re going to see more software created than ever before because of AI.\n\nI really believe that design and craft are the differentiator that makes a product and a brand stand out. Can you vibe code or hack your way towards something that makes money? Absolutely. But is it going to be an enduring product? For that, if you have any level of competition, you need to have really good design, a point of view, a great user experience, and a great brand. If you think about all the context that humans have that a LLM does not, I don’t see it being the case that models will get you there all the way.\n\nHow do you approach the way that Figma expands into new product areas?\n\nWe see what people are doing in Figma already. Back in 2020, in pandemic times, people were treating Figma like a space to collaborate. We saw tons of brainstorming and ideation. We had to pull that out and make it its own surface because Figma Design is not optimized for brainstorming, ideation, whiteboarding, or diagramming. We saw that 5 percent of the files in Figma Design were slides, so we went and made Figma Slides. I think there’s a lot more inside of Figma, in terms of use cases, that need to be pulled out.\n\nFigma Make, which you just introduced, is very broad in what it can do. What’s the goal?\n\nWe talk a lot about the process of going from idea to product, and that can involve all sorts of different steps. Make spans that whole process. Sometimes you have an idea in your head, and you’re like, “I want to prototype this. I want to get it out there, or I want to use it for myself to iterate and see if it does actually work.”\n\nWith Figma Sites, are you aiming to compete with Squarespace and WordPress? How deep are you going into web hosting and all that entails?\n\nRight now, we’re hosting. You can set a custom domain. Sites have been made in Figma for a long time. You’d have to either go somewhere else to deploy them, or you could code it all up yourself from Figma. Obviously, we hope that Dev Mode can help with that, but if we can get to the point where you can just press the publish button, that seems infinitely better to me for a designer trying to get a site out into the world. I think that’s a pretty distinct use case from the Squarespaces of the world, where, similar to Canva, they are more targeted towards consumers and small businesses.\n\nWill you release a mobile app for Figma Buzz, your AI marketing tool? If you do, I could see it more directly competing with Canva.\n\nThat’s not in our plans right now. I think we are right now focused on making sure Buzz is really high quality for what we’re trying to do on the web, and we can go from there.\n\nI have the utmost respect for Canva. The founders are incredible. My conceptualization of Canva is more on the consumer and small business side. You’re trying to do something fast. Buzz is focused on brand assets. It’s an enterprise use case.\n\nDo you think tools like Buzz will replace digital advertising as it exists today?\n\nI think there’s a role for AI in generating marketing assets. In Buzz, you can generate images and you can write text. What I have not seen yet is a world where the models can generate content that a brand team would be really proud of. Maybe that’s coming, but it seems further away than you might expect.\n\nElsewhere\n\nMeta’s wake-up call: The AI team at Meta is feeling the heat. Chief product officer Chris Cox , who oversees the division building Llama The AI team at Meta is feeling the heat. Chief product officer, who oversees the division building Llama and Meta AI , recently wrote an internal memo addressing “a lot of burnout” in the org and the cultural problems he wants to fix. While he acknowledges a “kernel of excitement with early adopters” of the Meta AI app, he challenges the org to be “self-aware about what it will take for us to level up” and stresses the need for a “flatter” structure to fix the “layers of review between our best technical leaders and the top.” Some other quotes, per a copy of the memo I’ve seen: “We need a culture of saying ‘no’ more often when asked to tighten timelines if it means cutting corners… There’s a pattern too often of wanting to hide bad news… In some cases we are missing critical infrastructure that we need to be successful at the scale we are operating at.”\n\nOpenAI is still a nonprofit: Despite OpenAI saying that it has found a way to Elon Musk), which in turn has massive implications for OpenAI’s investors. Sam Altman can escape cleanly from the Despite OpenAI saying that it has found a way to stay a nonprofit while still making unlimited profits , its corporate restructuring is far from over. There’s still the big question of how much equity the controlling nonprofit will own in the business (a decision that has to be blessed by regulators and has already had a price floor set by), which in turn has massive implications for OpenAI’s investors. Newcomer reports that “Microsoft wants a bigger stake than OpenAI feels is fair.” Ifcan escape cleanly from the faustian bargain he made with Microsoft years ago for compute, it will be a feat more impressive than ChatGPT’s user growth. OpenAI’s future funding from Softbank, ability to do big deals like Windsurf , and recruiting efforts all ultimately depend on this mess getting sorted out.\n\nOverheard\n\nEddy Cue on the witness stand, ”That has never happened in 20 years.” -on the witness stand, describing how Google traffic declined in Safari for the first time last month. (It’s amazing what the threat of $20 billion-plus in pure profit margin evaporating will get you to say.)\n\nMark Zuckerberg John Collison. “Tim has had a bad week. I’m not going to pile on. Sundar is cool.” - onstage at Stripe Sessions with\n\nDara Khosrowshahi “It is what it is.” - during an Uber all-hands about the company’s change to sabbatical policy and return-to-office mandate.\n\nJony Ive onstage with Patrick Collison at Stripe Sessions. “Certain products that I’ve been very, very involved with, I think there were some unintended consequences that were far from pleasant.” -at Stripe Sessions.\n\nBill Gates on Elon Musk while speaking to The New York Times. “The world’s richest man has been involved in the deaths of the world’s poorest children” -on\n\nPersonnel log\n\nLink list\n\nMore to click on:\n\nAs always, I welcome your feedback, especially if you have thoughts on this issue or a story idea to share. You can respond here or ping me securely on Signal."
    },
    {
        "title": "Google’s iOS app will use AI to simplify jargon",
        "description": "Google has a new AI tool that can help iPhone users to better grasp complicated or confusing writing online. The Simplify feature, rolling out in the Google app on iOS starting today, generates a simpler, more digestible version of any highlighted text withou…",
        "url": "https://www.theverge.com/news/661695/google-simplify-ai-gemini-feature-ios-app",
        "source": "The Verge",
        "publishedAt": "2025-05-06T11:43:18Z",
        "full_text": "Google has a new AI tool that can help iPhone users to better grasp complicated or confusing writing online. The Simplify feature, rolling out in the Google app on iOS starting today, generates a simpler, more digestible version of any highlighted text without leaving the current web page.\n\nSimplify is built on Google’s Gemini AI model, and was developed by Google Research to make technical jargon easier for anyone to understand without losing key details. For example, Simplify can break down medical terms like “emphysema” (a condition that damages the air sacs in lungs) and “fibrosis” (dense connective tissue or scarring that develops in response to damage) in reports and journals, preventing readers from needing to reference terminology on a separate web page.\n\nThe simplified version of selected text will appear in a pop-up window that can be dismissed when it’s no longer required. GIF: Google\n\nGoogle says that people found the simplified versions to be “significantly more helpful than the original complex text” in its testing, but acknowledged the study “has limitations” and that “ongoing vigilance” is required to monitor errors.\n\nThe feature can be found in the iOS Google app by highlighting any text on a website, and tapping the Simplify icon from the menu options that appear. When asked if Simplify will be made available for Android and desktop Chrome users, Google spokesperson Jennifer Kutz told The Verge that “we don’t have anything to announce yet, but we’re always looking to bring useful features to more of our products.”"
    },
    {
        "title": "TikTok is using AI-generated alt text to describe photos",
        "description": "TikTok is introducing new accessibility features that make it easier for people with visual impairments to see text and use screen readers on the platform. After adding alt text support for still images in April, TikTok is now testing a feature that automatic…",
        "url": "https://www.theverge.com/news/666632/tiktok-accessibility-ai-generated-alt-text-contrast-bold",
        "source": "The Verge",
        "publishedAt": "2025-05-14T12:47:03Z",
        "full_text": "TikTok is introducing new accessibility features that make it easier for people with visual impairments to see text and use screen readers on the platform. After adding alt text support for still images in April, TikTok is now testing a feature that automatically adds an AI-generated alt text description to photographs when a creator hasn’t manually added it.\n\nCreators will still have the option to add their own alt text descriptions to images as they’re uploaded or after publishing, providing a means to correct any information in the AI-generated versions. Even if the AI alt text is imperfect, however, users who rely on screen readers to process visual content may find that having it widely applied is preferable to images not having any alt text at all.\n\nTikTok is also launching a new color contrast toggle that can be used to increase the foreground color of text, icons, and UI elements. This should make descriptions and controls more distinct, helping people with low vision or contrast sensitivities to navigate the platform. Additionally, TikTok will now automatically display all text in bold across the app when it detects that users have enabled bold text support on their devices.\n\nIf you have text set to bold at a device-level then this is how TikTok will now honor that across the app. Image: TikTok\n\nAll of these features are available now and can be found on TikTok’s redesigned accessibility settings page, which is located on iOS and Android user profiles under the three-line menu, or the left-hand taskbar on desktop."
    },
    {
        "title": "Google I/O will be an AI show",
        "description": "Android is getting its biggest visual update in years, and rather than unveiling it for the first time at its big annual developer conference, Google announced Material Three Expressive at a pre-show event broadcast on YouTube the week before. If a major desi…",
        "url": "https://www.theverge.com/tech/667872/google-io-android-16-ai",
        "source": "The Verge",
        "publishedAt": "2025-05-16T21:06:26Z",
        "full_text": "is a reviewer with 10 years of experience writing about consumer tech. She has a special interest in mobile photography and telecom. Previously, she worked at DPReview.\n\nAndroid is getting its biggest visual update in years, and rather than unveiling it for the first time at its big annual developer conference, Google announced Material Three Expressive at a pre-show event broadcast on YouTube the week before. If a major design language shift for the world’s most popular mobile OS doesn’t qualify as a headliner at I/O, then what does? You guessed it: AI.\n\nWe expect Google to talk all about Gemini during I/O, which kicks off on Tuesday, and how it’s improving it and bringing it to products in areas that consumers will see even more.\n\nIf you’ve paid attention to the past couple of I/O keynotes, this won’t be a surprise. Android was barely mentioned in 2023, and CEO Sundar Pichai said AI so many times that we lost count. Last year’s keynote was more of the same, except that Pichai saved us some trouble and counted mentions of AI for us. All of this reflects the very obvious, inescapable shift that Google and every other tech company have made recently to pump out AI features at a breakneck pace.\n\nBut in a way, less news about the newest Android OS at I/O is actually a good thing.\n\nGoogle has made a big effort in recent years to bring new features to more phones — even if they aren’t running the latest OS. Companies like Samsung and Motorola roll out new OS versions on their own schedules, so adding new features through Google Play and app updates means they’ll reach people faster. One key criticism of Android in previous years was slow feature rollouts that often entirely failed to reach many users. Google also adopted a new schedule this year for Android 16, moving to a major release in Q2 and a minor release in Q4, which should help more devices take advantage of new features.\n\nIf this year’s I/O really is an AI show, then there’s a risk it’s going to feel like one we’ve seen before.\n\nThe past two years of software and hardware from seemingly every consumer tech company have been nonstop AI pep rallies. We’ve seen a lot of previews of features that are supposedly just around the corner. Then, when it’s time to actually ship them… well, ask Apple how that’s going.\n\nTo Google’s credit, it has certainly shipped lots of AI features. The company has a lot of surfaces for it, too — from the XR platform we’re expecting to hear more about (that’s the glasses it has teased several times), to Chrome, Gmail, and Meet. There’s a lot of ground to cover."
    },
    {
        "title": "OpenAI is buying Jony Ive’s AI hardware company",
        "description": "OpenAI is buying io, a hardware company founded by former Apple design chief Jony Ive and several other former engineers from his time there, including Scott Cannon, Evans Hankey, and Tang Tan. “We gathered together the best hardware and software engineers, t…",
        "url": "https://www.theverge.com/news/671838/openai-jony-ive-ai-hardware-apple",
        "source": "The Verge",
        "publishedAt": "2025-05-21T17:21:09Z",
        "full_text": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nOpenAI is buying io, a hardware company founded by former Apple design chief Jony Ive and several other former engineers from his time there, including Scott Cannon, Evans Hankey, and Tang Tan.\n\nIve won’t be joining OpenAI, and his design firm, LoveFrom, will continue to be independent, but they will “take over design for all of OpenAI, including its software,” in a deal valued at nearly $6.5 billion, Bloomberg reports.\n\nAbout 55 hardware engineers, software developers, and manufacturing experts will join OpenAI as part of the acquisition. That includes Cannon, Hankey, and Tan. The first devices following the acquisition are set to launch in 2026.\n\nIn an interview with Bloomberg, Ive called AI hardware misfires like the Humane Pin and Rabbit R1 “very poor products,” and said that “there has been an absence of new ways of thinking expressed in products.”\n\nThe first product isn’t intended to be an iPhone killer, though: “In the same way that the smartphone didn’t make the laptop go away, I don’t think our first thing is going to make the smartphone go away,” OpenAI CEO Sam Altman told Bloomberg. “It is a totally new kind of thing.”\n\nThe Wall Street Journal reports that Altman and LoveFrom have been working together for two years and have considered options like headphones and devices with cameras. Some details about the device Altman and Ive are currently working on were mentioned in an internal OpenAI call reviewed by The Journal, including that it will be pocket-sized, contextually aware, screen-free, and isn’t a pair of smart glasses.\n\nThe first product the team has been working on “has just completely captured our imagination,” Ive said in a video.\n\n“Jony recently gave me one of the prototypes of the device for the first time to take home, and I’ve been able to live with it, and I think it is the coolest piece of technology that the world will have ever seen,” Altman said.\n\n“I am absolutely certain that we are literally on the brink of a new generation of technology that can make us our better selves,” Ive said.\n\n“We gathered together the best hardware and software engineers, the best technologists, physicists, scientists, researchers and experts in product development and manufacturing,” Ive and Altman said in a joint post. “Many of us have worked closely for decades. The io team, focused on developing products that inspire, empower and enable, will now merge with OpenAI to work more intimately with the research, engineering and product teams in San Francisco.”\n\n“AI is an incredible technology, but great tools require work at the intersection of technology, design, and understanding people and the world,” Altman said in a statement. “No one can do this like Jony and his team; the amount of care they put into every aspect of the process is extraordinary.”\n\n“I have a growing sense that everything I have learned over the last 30 years has led me to this moment,” Ive said. “While I am both anxious and excited about the responsibility of the substantial work ahead, I am so grateful for the opportunity to be part of such an important collaboration. The values and vision of Sam and the teams at OpenAI and io are a rare inspiration.”\n\n“A number of us looked at each other and said, ‘This is probably the most incredible technology of our career,’” Hankey said in an interview with Bloomberg.\n\nUpdate, May 21st: Added information from The Wall Street Journal.\n\nUpdate, May 22nd: Added device details leaked to The Wall Street Journal."
    },
    {
        "title": "Strava updates its AI route planning and cheater detection",
        "description": "Strava is making it easier to plan workouts and keep activity leaderboard rankings fair. The updates rolling out over the coming weeks focus on helping users optimize their workout routes to compete against other users and their own personal bests, building o…",
        "url": "https://www.theverge.com/news/671452/strava-ai-routes-leaderboard-update",
        "source": "The Verge",
        "publishedAt": "2025-05-21T10:56:54Z",
        "full_text": "Strava is making it easier to plan workouts and keep activity leaderboard rankings fair. The updates rolling out over the coming weeks focus on helping users optimize their workout routes to compete against other users and their own personal bests, building on some of the existing AI features that Strava announced last year.\n\nAnyone paying for a Strava subscription (starting at $11.99 monthly) can now access a new AI-powered routes experience under the Maps tab that should provide more intuitive suggestions based on popular routes enjoyed by other Strava users. Users can generate community-backed routes from custom starting points or their current location, pulling data from Strava’s heatmaps feature.\n\nOther route-related updates will be rolled out to the Strava mobile app in the coming months, including changes to the tappable points of interest (POI) feature that currently enables subscribers to instantly generate routes to cafés, restrooms, viewpoints, and other locations. Starting in June, POI’s will also display elevation, distance, and estimated arrival time information, and allow users to upload photos of the location. Point-to-point routing will also launch in July, which uses heatmaps and machine learning to deliver “the most efficient, activity-specific route from A to B,” according to Strava.\n\nStrava is also doubling the number of live segments, which allow users to view real-time performance data and achievements in sections of their route, and introducing additional data screens for subscribers.\n\nSuspicious entries will be challenged to prevent leaderboards from being skewed. Image: Strava\n\nFinally, Strava says it’s “continuing to advance” the AI-powered Leaderboard Integrity feature it launched to weed out cheaters on cycling and running paths. The company says that 4.45 million activity logs have been removed so far that carried the wrong sport type, or were recorded in vehicles — which is an easy way to fabricate scores now that e-bikes can make anyone the king of a mountain."
    },
    {
        "title": "Microsoft’s Head of AI Security Accidentally Reveals Walmart’s Private AI Plans After Pro-Palestine Protest",
        "description": "The company's annual Build conference was disrupted by a series of protests denouncing Microsoft's ties with Israel.",
        "url": "https://gizmodo.com/microsofts-head-of-ai-security-accidentally-reveals-walmarts-private-ai-plans-after-pro-palestine-protest-2000605389",
        "source": "Gizmodo.com",
        "publishedAt": "2025-05-21T17:03:09Z",
        "full_text": ""
    },
    {
        "title": "Audible is giving publishers AI tools to quickly make more audiobooks",
        "description": "Amazon’s Audible has announced that it’s planning to expand its audiobook catalog by giving select publishers access to its new “fully integrated, end-to-end AI production technology” that will let them more easily convert titles to audiobooks with their choi…",
        "url": "https://www.theverge.com/news/666136/amazon-audible-ai-narration-audiobooks-translation",
        "source": "The Verge",
        "publishedAt": "2025-05-13T18:18:08Z",
        "full_text": "is a senior reporter who’s been covering and reviewing the latest gadgets and tech since 2006, but has loved all things electronic since he was a kid.\n\nAmazon’s Audible has announced that it’s planning to expand its audiobook catalog by giving select publishers access to its new “fully integrated, end-to-end AI production technology” that will let them more easily convert titles to audiobooks with their choice of AI-generated voices. The initiative will also help expand global access to audiobooks with the introduction of a new AI translation tool that’s expected to launch in an early beta later this year.\n\nAudible says its new AI narration technology leverages Amazon’s advanced AI capabilities and will be made available to interested publishing partners in the coming months in one of two ways. For publishers wanting to be hands-off, an end-to-end service managed by Audible handles the “entire audiobook production process” right up to publication, while a self-service option will give publishers access to the same tools so they can independently direct the entire production process.\n\nWith both options, publishers are able to “choose from a quickly growing and improving selection of more than 100 AI-generated voices across English, Spanish, French, and Italian with multiple accent and dialect options, and will be able to access voice upgrades for their titles as our technology evolves,” according to Amazon.\n\nLast September, Amazon invited a select group of Audible narrators to train AI-generated voice clones of themselves ahead of the launch of this new service. The company said that if their AI voice replica was selected for a project, the narrators would be able to review the final audiobook for errors or inaccuracies and use the platform’s production tools to fine-tune pronunciations or adjust the pacing of their voice.\n\nAudible’s upcoming AI translation tools will also be limited to select publishers, and will initially support translations from English to Spanish, French, Italian and German. As with audiobook production, publishers will be offered two different approaches. Text-to-text translation for manuscripts which can be later turned into audiobooks, and speech-to-speech translation which uses AI to preserve the “original narrators’ voice and style across languages.”\n\nPublishers will also be able to review translations themselves or opt for a human review through Audible with a professional linguist."
    },
    {
        "title": "Gemini is coming to TVs and cars, eventually",
        "description": "Google is bringing its Gemini AI assistant to devices with Google TV, cars with Android Auto and Google built-in, Wear OS smartwatches, and Android XR. But Google isn’t saying exactly when the AI assistant will come to those devices — right now, the company i…",
        "url": "https://www.theverge.com/news/665161/google-gemini-tvs-cars-smartwatches-android-xr",
        "source": "The Verge",
        "publishedAt": "2025-05-13T15:27:47Z",
        "full_text": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nGoogle is bringing its Gemini AI assistant to devices with Google TV, cars with Android Auto and Google built-in, Wear OS smartwatches, and Android XR. But Google isn’t saying exactly when the AI assistant will come to those devices — right now, the company is giving more general timelines about when it might arrive.\n\nOn Google TV products, “you can ask for action movies that are age-appropriate for your kids, and get the best recommendations,” according to a blog post from Guemmy Kim, a senior director of product and user experience on Android. In an example, the company shows how you can ask something like “can you explain the solar system to my first grader,” and Google TV pulls up a short explanation, offers a button to press to learn more, and recommends YouTube videos about the solar system designed for kids. Gemini is coming to Google TV “later this year.”\n\nIn cars, Gemini will improve upon talking hands-free with Google Assistant by “understanding what you want while you’re driving, through natural conversations,” Kim says. “For example, Gemini can find you a charging station on the way to the post office that’s also near a park, so that you can go for a walk before your errands while your car is charging.” Gemini can also connect to a messaging app to summarize your messages. Gemini is coming to Android Auto “in the coming months,” and to cars with Google built-in sometime later.\n\nWith Gemini on Wear OS, Kim highlights how the AI assistant lets you talk naturally, with “no need to get the words just right, or awkwardly type into a tiny screen,” which could be useful if you’re asking a quick question while dashing out the door to work. Gemini will launch on Wear OS in the coming months.\n\nFinally, Gemini is coming to the first Android XR headset, built by Samsung. Kim says people will be able to try it later this year."
    },
    {
        "title": "Google might replace the ‘I’m Feeling Lucky’ button with AI Mode",
        "description": "Some Google Search users are starting to see the platform’s AI Mode search engine chatbot in the wild. Several user reports across X and Threads show that Google is testing different locations to display the AI Mode tab — while some examples show it in the se…",
        "url": "https://www.theverge.com/news/665560/google-search-ai-mode-feeling-lucky-tests",
        "source": "The Verge",
        "publishedAt": "2025-05-13T09:16:26Z",
        "full_text": "Some Google Search users are starting to see the platform’s AI Mode search engine chatbot in the wild. Several user reports across X and Threads show that Google is testing different locations to display the AI Mode tab — while some examples show it in the search bar itself, besides the “search by image” button, in others it replaces the “I’m Feeling Lucky” button underneath the search bar.\n\nThere’s also some variation to the appearance of the AI Mode button, with some users seeing a rotating rainbow border when the cursor hovers over it. In another example where AI Mode has replaced “I’m Feeling Lucky,” the rainbow border is seemingly the default design, and makes the search-focused AI chatbot button stand out among Google’s other offerings.\n\nGoogle said on May 1st that a “small percentage” of people in the US would soon start seeing an AI Mode option in Google Search, but the screenshots of these tests give us some idea of where it may be placed when (or if) it rolls out to the wider public. For now, Google spokesperson Ashley Thompson told The Verge that AI Mode in Search is currently limited to users of Google’s experimental Labs environment.\n\n“We often test different ways for people to access our helpful features,” said Thompson. “This is just one of many experiments.”\n\nIt’s not guaranteed that the “I’m Feeling Lucky” option will officially be replaced by AI Mode, but even the knowledge that Google is considering it may upset folks who use the niche feature. The button that takes users directly to the first webpage of any search results has been a fixture of Google’s homepage since the search engine launched. This may be Google’s way of suggesting that AI Mode’s chatbot-like responses to search queries are more efficient than allowing users to bypass the traditional search results page."
    },
    {
        "title": "Google I/O 2025: All the news and announcements",
        "description": "Google I/O starts today, and would you believe it? They’re going to talk about AI. After getting everything Android out of the way in last week’s dedicated Android Show, we’re expecting today’s I/O developer conference keynote to be one big AI show. Gemini, P…",
        "url": "https://www.theverge.com/google/670250/google-io-news-announcements-gemini-ai-android-xr",
        "source": "The Verge",
        "publishedAt": "2025-05-20T16:06:47Z",
        "full_text": "Google I/O 2025 was dedicated to AI.\n\nAt its annual developer conference, Google announced updates that put more AI into Search, Gmail, and Chrome. Its AI models were updated to be better at making images, taking actions, and writing code.\n\nAnd while there wasn’t a big Android presence in the main keynote, Google had plenty to announce about its OS last week, including a redesign and updates to its device tracking hub."
    },
    {
        "title": "Amazon claims its ‘constantly inviting’ new customers to Alexa Plus",
        "description": "Yesterday, Reuters ran a story with the headline “Weeks after Amazon’s Alexa+ AI launch, a mystery: where are the users?,” in which it detailed its difficulty locating first-hand accounts of the AI-upgraded assistants’ use online. The Verge asked Amazon about…",
        "url": "https://www.theverge.com/news/669158/amazon-claims-its-constantly-inviting-new-customers-to-alexa-plus",
        "source": "The Verge",
        "publishedAt": "2025-05-17T22:14:06Z",
        "full_text": "is a weekend editor who covers the latest in tech and entertainment. He has written news, reviews, and more as a tech journalist since 2020.\n\nYesterday, Reuters ran a story with the headline “Weeks after Amazon’s Alexa+ AI launch, a mystery: where are the users?,” in which it detailed its difficulty locating first-hand accounts of the AI-upgraded assistants’ use online. The Verge asked Amazon about the story, and the company has responded to say that the idea that Alexa Plus isn’t available is “simply wrong.”\n\nHere’s the company’s full — and rather strongly-worded! — statement on the matter, provided by Amazon spokesperson Eric Sveum via email to The Verge:\n\nIt’s simply wrong to say that Alexa+ isn’t available to customers—that assertion is false. Hundreds of thousands of customers have access to Alexa+ and we’re constantly inviting more customers that have requested Early Access.\n\nSveum also shared the below screenshot of what the email invite should look like.\n\nImage: Amazon\n\nAlexa Plus is Amazon’s generative AI-updated version of Alexa, which it announced in February is free to Amazon Prime subscribers or $19.99 a month otherwise.\n\nWhile Reuters doesn’t say Alexa Plus isn’t available to customers yet, it does quote an analyst who said, “There seems to be no one who actually has it.”\n\nThe outlet also reported that its efforts to find any real-world Alexa Plus users came up empty, writing that it had “searched dozens of news sites, YouTube, TikTok, X, BlueSky and Meta’s Instagram and Facebook, as well as Amazon’s Twitch and reviews of Echo voice-assistant devices on Amazon.com.” It added that it spoke with two people who’d posted on Reddit claiming to have used Alexa Plus, but that they “did not provide Reuters with hard evidence and their identities could not be corroborated.”\n\nStill, Engadget reported today that a wave of emails had gone out on Friday, inviting Amazon Alexa users to try out Alexa Plus. The outlet also reported that an Amazon spokesperson had told it “hundreds of thousands” of customers have tried the assistant."
    },
    {
        "title": "Nvidia’s flattery of Trump wins reversal of AI chip limits and a Huawei clampdown",
        "description": "Nvidia’s efforts to suck up to the Trump administration have seemingly paid off, with the US now lifting export limits on US-made AI chips and cracking down on anyone using Huawei’s emerging alternatives. The announcements come as Nvidia CEO Jensen Huang join…",
        "url": "https://www.theverge.com/news/666605/nvidias-flattery-of-trump-wins-reversal-of-ai-chip-limits-and-a-huawei-clampdown",
        "source": "The Verge",
        "publishedAt": "2025-05-14T10:40:56Z",
        "full_text": "Nvidia’s efforts to suck up to the Trump administration have seemingly paid off, with the US now lifting export limits on US-made AI chips and cracking down on anyone using Huawei’s emerging alternatives. The announcements come as Nvidia CEO Jensen Huang joined President Trump in Saudi Arabia this week to solicit AI investments for US companies.\n\nThe US Department of Commerce (DOC) announced on Monday that it has rescinded the Artificial Intelligence Diffusion Rule, due to take effect on May 15th, that aimed to restrict how many US-made AI chips could be sent to international markets without special government approval. The DOC said that a replacement rule for protecting US AI technology will be issued “in the future,” but provided no specific details.\n\n“These new requirements would have stifled American innovation and saddled companies with burdensome new regulatory requirements,” The DOC said in a statement. “The AI Diffusion Rule also would have undermined US diplomatic relations with dozens of countries by downgrading them to second-tier status.”\n\nTrump’s favorite government meddler Elon Musk was also rubbing shoulders with Huang and the Saudi prince at the investment forum. Image: Brendan Smialowski / Getty Images\n\nWhile the goal was to prevent countries already subject to chip restrictions, such as Russia and China, from accessing or building AI tech, it also placed Nvidia’s estimated 90 percent share of the AI chip market in jeopardy. Shortly after the Diffusion Rule was introduced by former President Joe Biden in January, Nvidia issued a statement calling it “misguided,” while anticipating a return to Trump’s first term policies “that strengthen American leadership, bolster our economy and preserve our competitive edge in AI and beyond.”\n\nThe DOC also warned companies that using Huawei’s Ascend AI chipset “anywhere in the world” would violate US export control agreements. Huawei’s home-grown Ascend processors are seen as China’s best answer to Nvidia’s powerful AI chips.\n\nNvidia’s Huang was notably one of the only US tech leaders to not attend Donald Trump’s inauguration. His absence doesn’t appear to have soured the relationship between them, however, with Huang spotted cosying up to Trump at a US-Saudi investment summit in Riyadh on Tuesday, alongside other tech leaders like Elon Musk, AMD’s Lisa Su, OpenAI’s Sam Altman, and Epic CEO Tim Sweeney.\n\nThe Washington Post reports that Trump was far from subtle about what the US wanted from the gathering. “As you know, we have the biggest business leaders in the world here,” he told Saudi Crown Prince Mohammed bin Salman. “They’re going to walk away with a lot of checks for a lot of things that you’re going to provide.”"
    },
    {
        "title": "Meta’s smart glasses can now describe what you’re seeing in more detail",
        "description": "Meta announced two new features designed to assist blind or low vision users by leveraging the Ray-Ban Meta smart glasses’ camera and its access to Meta AI. The news came as part of Global Accessibility Awareness Day. Rolling out to all users in the US and Ca…",
        "url": "https://www.theverge.com/news/667613/ray-ban-meta-smart-glasses-ai-detailed-responses-call-a-volunteer",
        "source": "The Verge",
        "publishedAt": "2025-05-15T13:47:59Z",
        "full_text": "is a senior reporter who’s been covering and reviewing the latest gadgets and tech since 2006, but has loved all things electronic since he was a kid.\n\nMeta announced two new features designed to assist blind or low vision users by leveraging the Ray-Ban Meta smart glasses’ camera and its access to Meta AI. The news came as part of Global Accessibility Awareness Day.\n\nRolling out to all users in the US and Canada in the coming weeks, Meta AI can now be customized to provide more detailed descriptions of what’s in front of users when they ask the smart assistant about their environment. In a short video shared alongside the announcement, Meta AI goes into more detail about the features of a waterside park, including describing grassy areas as being “well manicured.”\n\nMeta AI can now go into greater detail while describing what you’re looking at. Image: Meta\n\nThe feature can be activated by turning on “detailed responses” in the Accessibility section of the Device settings in the Meta AI app. Although it’s currently limited to users in the US and Canada, Meta says detailed responses will “expand to additional markets in the future,” but provided no details about when or which countries would get it next.\n\nFirst announced last September as part of a partnership with the Be My Eyes organization and released last November in a limited rollout that included the US, Canada, UK, Ireland, and Australia, Meta also confirmed today that its Call a Volunteer feature will “launch in all 18 countries where Meta AI is supported later this month.”\n\nBlind and low vision users of the Ray-Ban Meta smart glasses can use the feature to connect to a network of over 8 million sighted volunteers and get assistance with everyday tasks such as following a recipe or locating an item on a shelf. By saying, “Hey Meta, Be My Eyes,” a volunteer will be able to see a user’s surroundings through a live feed from the glasses’ camera and can provide descriptions or other assistance through its open-ear speakers."
    },
    {
        "title": "Anthropic’s Claude 4 AI models are better at coding and reasoning",
        "description": "Anthropic has introduced Claude Opus 4 and Claude Sonnet 4, its latest generation of hybrid-reasoning AI models optimized for coding tasks and solving complex problems.  Claude Opus 4 is Anthropic’s most powerful AI model to date, according to the company’s a…",
        "url": "https://www.theverge.com/news/672705/anthropic-claude-4-ai-ous-sonnet-availability",
        "source": "The Verge",
        "publishedAt": "2025-05-22T16:38:05Z",
        "full_text": "is a news writer focused on creative industries, computing, and internet culture. Jess started her career at TechRadar, covering news and hardware reviews.\n\nAnthropic has introduced Claude Opus 4 and Claude Sonnet 4, its latest generation of hybrid-reasoning AI models optimized for coding tasks and solving complex problems.\n\nClaude Opus 4 is Anthropic’s most powerful AI model to date, according to the company’s announcement, and capable of working continuously on long-running tasks for “several hours.” In customer tests, Anthropic said that Opus 4 performed autonomously for seven hours, significantly expanding the possibilities for AI agents. The company also described its new flagship as the “best coding model in the world,” with Anthropic’s benchmarks showing that Opus 4 outperformed Google’s Gemini 2.5 Pro, OpenAI’s o3 reasoning, and GPT-4.1 models in coding tasks and using “tools” like web search.\n\nClaude Sonnet 4 is a more affordable and efficiency-focused model that’s better suited to general tasks, which supersedes the 3.7 Sonnet model released in February. Anthropic says Sonnet 4 delivers “superior coding and reasoning” while providing more precise responses. The company adds that both models are 65 percent less likely to take shortcuts and loopholes to complete tasks compared to 3.7 Sonnet and they’re better at storing key information for long-term tasks when developers provide Claude with local file access.\n\nThese are Anthropic’s own internal benchmark tests so take the results with a grain of salt. Image: Anthropic\n\nA new feature introduced for both Claude 4 models is “thinking summaries,” which condenses the chatbots’ reasoning process into easily understandable insights. An “extended thinking” feature is also launching in beta that allows users to switch the models between modes for reasoning or using tools to improve the performance and accuracy of responses.\n\nClaude Opus 4 and Sonnet 4 are available on the Anthropic API, Amazon Bedrock, and Google Cloud’s Vertex AI platform, and both models are included in paid Claude plans alongside the extended thinking beta feature. Free users can only access Claude Sonnet 4 for now.\n\nIn addition to the new models, Anthropic’s Claude Code agentic command-line tool is now generally available following its limited preview in February. Anthropic also says it’s shifting to provide “more frequent model updates,” as the company tries to keep up with competition from OpenAI, Google, and Meta."
    },
    {
        "title": "Google’s AI image-to-video generator launches on Honor’s new phones",
        "description": "Chinese phone manufacturer Honor has launched an image-to-video AI generator powered by Google, before it’s available to Gemini users. It will be available first for anyone who buys the Honor 400 or 400 Pro phones, which launch next week on May 22nd. The new …",
        "url": "https://www.theverge.com/news/664812/google-honor-ai-image-to-video-gemini",
        "source": "The Verge",
        "publishedAt": "2025-05-12T08:45:06Z",
        "full_text": "Chinese phone manufacturer Honor has launched an image-to-video AI generator powered by Google, before it’s available to Gemini users. It will be available first for anyone who buys the Honor 400 or 400 Pro phones, which launch next week on May 22nd.\n\nThe new AI tool, powered by Google’s Veo 2 model, creates five-second videos based on static images, in either portrait or landscape, and takes a minute or two to generate each time. The feature is built directly into the Gallery app on the new Honor phones, and is designed to be simple: there’s no option to include a text prompt along with the image, so you’re stuck hoping that the AI does something sensible with it.\n\nPrevious Next\n\n1 / 2 A photo I took at a Feeder show.\n\nSometimes it works well. Give it a simple subject, like a clear photo of a person or pet, and it can generate quite realistic movement — albeit I’m pretty sure my cat Noodle’s tongue isn’t quite that big. Other subjects prove trickier: faced with a vintage car it made it rotate impossibly on the spot; fresh tomatoes were fondled by a ghostly hand; and it imagined a women’s soccer game with at least 27 players across three teams, with two referees to keep control of the chaos. The first time I tried it, on a self-portrait by Vincent Van Gogh, it decided that the most appropriate thing would be for a pigeon to fly out of his eye.\n\nNote: Honor’s app outputs videos in MP4, which we’ve converted to GIFs, slightly reducing the image quality of the clips.\n\nThe image-to-video feature will be available to Honor 400 owners for free for the first two months, but with a limit of 10 video generations per day. Honor’s UK marketing director Chris Langley told me that it “will eventually require some subscription” from Google, but the details of that are unknown.\n\nVideo generation using Veo 2 is already included in Google’s paid Gemini Advanced subscription, but is currently limited to text input. Image-to-video generation is listed as one of Veo 2’s features in Google Cloud, where Google charges customers 50 cents per second of output, but is available to “approved users” only.\n\nPhotography and videos by Dominic Preston / The Verge."
    },
    {
        "title": "Rabbit Teases Redesigned R1 UI After Design God Jony Ive Dumps on AI Gadgets",
        "description": "The AI startup is undeterred with improving its \"Tamagotchi-Pokédex-walkie-talkie\" even though it's about to get some serious competition in the AI gadgets space.",
        "url": "https://gizmodo.com/rabbit-teases-redesigned-r1-ui-after-design-god-jony-ive-dumps-on-ai-gadgets-2000607715",
        "source": "Gizmodo.com",
        "publishedAt": "2025-05-27T18:00:33Z",
        "full_text": ""
    },
    {
        "title": "Silence Speaks Has Created AI-Powered Signing Avatars for the Deaf",
        "description": "New technology from British startup Silence Speaks enables an AI-generated sign language avatar to effectively give the deaf and hard of hearing an interpreter in their pocket.",
        "url": "https://www.wired.com/story/silence-speaks-deaf-ai-signing/",
        "source": "Wired",
        "publishedAt": "2025-05-07T13:39:49Z",
        "full_text": "More than 70 million deaf or hard-of-hearing people globally use sign language, but there's an acute shortage of interpreters. Silence Speaks is a British startup that wants to bridge that gap with an AI-powered sign language avatar capable of translating text to sign language.\n\nCommunication problems can be devastating and isolating for the deaf, especially in environments with background noise and crosstalk. It's often impossible for deaf and hard-of-hearing folks to follow conversations in train stations, hospitals, school classrooms, and busy offices. Even people with cochlear implants pick up only a few words from each sentence and can struggle with tone.\n\nDeveloped by and for the deaf community, Silence Speaks can accurately translate text into British Sign Language (BSL). There are more than 150,000 BSL users in the UK. The model was trained with datasets covering regional dialects, contextual language, and emotional tone to power an AI-generated avatar that goes beyond simple direct translation to convey intent and emotion.\n\nWIRED has a policy of not sharing a story with a company before publication, but we sent an edited version of this piece to Silence Speaks so that the company could generate an AI avatar based on me for a sample video. There were no changes to the text.\n\nAccessibility Barriers\n\nWith a background at Vodafone pioneering AI architecture for chatbots, Silence Speaks CEO Pavan Madduru founded the company just over three years ago. He got the idea after spending time with an engineer friend who was deaf. When the two traveled together, Madduru saw how the lack of sign language support made life difficult for his friend and resolved to tackle the problem.\n\nHe hired three full-stack engineers, all deaf sign language users, to begin work on translation technology to harness the growing potential of AI. When he started the project, the team tried converting sign language to text, hoping to enable real-time translation of any sign language to any other sign language. But this proved trickier than expected.\n\nWhile American Sign Language (ASL) is widely used in the US, BSL is quite different, and there are more than 200 sign languages around the world. Silence Speaks started with BSL but soon realized that regional slang and accents extend to sign language and impact signing speed. During the trial, they discovered signers in Edinburgh were 50 percent faster than folks in Southampton or Birmingham."
    },
    {
        "title": "Inside the Battle Over OpenAI’s Corporate Restructuring",
        "description": "A group of activists is turning to an old playbook to influence the future of one of the world’s most powerful AI companies.",
        "url": "https://www.wired.com/story/open-ai-nonprofit-transition-activism/",
        "source": "Wired",
        "publishedAt": "2025-05-02T09:00:00Z",
        "full_text": "Last October, the news that OpenAI was planning to simplify its unusual nonprofit structure caught the attention of economic-justice activist Orson Aguilar. He feared that the ChatGPT maker’s plan to transition into a more conventional company, from which investors could generate unlimited returns, would financially hurt the working-class communities he has spent nearly 30 years fighting to protect.\n\nAguilar’s new organization, LatinoProsperity, focuses on intergenerational wealth building, and he believed cutting-edge AI chatbots such as ChatGPT would become an integral part of many good-paying jobs of the future. But after reading about OpenAI’s desires, he worried that transitioning into a public-benefit corporation empowered to chase profits would enrich the already wealthy and neglect the startup’s stated mission to benefit all of humanity with AI.\n\nAguilar decided to make a phone call that day, kicking off a series of events that eventually led him to become one of the leading voices battling over OpenAI’s future and the establishment of what may become the deepest-pocketed charitable foundation in the world.\n\nToday, OpenAI’s for-profit business is controlled by a nonprofit, and the returns for investors are capped. But as OpenAI’s ambitions have grown, and staying on the cutting edge has required it to raise significantly more funding, investors have demanded greater payback.\n\nOpenAI said a recent $40 billion investment round propelled its valuation to $300 billion, but 75 percent of the pledged cash is contingent on the startup completing a structural revamp by early next year. A byproduct of the move, the AI startup said in a December blog post, would be the creation of “one of the best resourced nonprofits in history”—albeit one with less power over what products OpenAI builds and for whom.\n\nAguilar is co-steering a coalition of advocacy groups that met with San Francisco–based OpenAI in March and is pushing the California attorney general’s office, which regulates the state’s nonprofits, to ensure that the startup’s restructuring adheres to the law. Their activism has already contributed to OpenAI forming a small advisory commission that includes legendary labor activist Dolores Huerta.\n\n“Tech companies, they like to disrupt,” Aguilar says. “We can’t let them disrupt our charitable system and get away with it. We have a tool already—a nonprofit OpenAI with a stated mission to do good things. It would be a shame if that was lost in the race of developing AI.”\n\nOpenAI spokesperson Lindsey Held says the organization’s restructuring “would ensure that as the public benefit corporation succeeds and grows, so too does the nonprofit, enabling us to achieve the mission” of AI broadly benefiting the public.\n\nA group of tech industry heavyweights, including Elon Musk and Sam Altman, cofounded OpenAI as a nonprofit in 2015 with the goal of countering existing AI projects run by big, profit-driven companies. OpenAI planned to share its research with the public while focusing on developing AI systems that are highly capable and broadly helpful.\n\nBut as development grew more expensive and ChatGPT became an overnight sensation upon its debut in 2022, OpenAI has increasingly shifted toward withholding some of the key details about its research for security and competitive reasons. It’s also now trying to generate significant revenue from its services."
    },
    {
        "title": "Google’s Wants You to Pay $250 for a God-Tier Subscription to AI",
        "description": "One AI bundle to rule them all",
        "url": "https://gizmodo.com/googles-wants-you-to-pay-250-for-a-god-tier-subscription-to-ai-2000604285",
        "source": "Gizmodo.com",
        "publishedAt": "2025-05-20T17:45:37Z",
        "full_text": ""
    },
    {
        "title": "Google rejected giving publishers more choice to opt out of AI Search",
        "description": "Google didn’t want to give publishers the choice to keep their content out of AI Search results because it’s “evolving into a space for monetisation.” That’s according to a newly disclosed internal document spotted by Bloomberg, which reveals that Google had …",
        "url": "https://www.theverge.com/news/671711/google-ai-overviews-search-publisher-data-choice",
        "source": "The Verge",
        "publishedAt": "2025-05-21T16:48:07Z",
        "full_text": "Google didn’t want to give publishers the choice to keep their content out of AI Search results because it’s “evolving into a space for monetisation.” That’s according to a newly disclosed internal document, spotted by Bloomberg, which reveals that Google had discussed offering publishers more granular control over how website data would be used in AI Search features instead of the illusion of choice they eventually received.\n\nThe document, written by Google Search executive Chetna Bindra, was released during the US antitrust trial into Google’s online search monopoly. The access to its search engine data gives Google a huge advantage in AI development over rivals like Perplexity and OpenAI. But Google’s AI Overviews and AI Mode can be detrimental to the websites they source from by reducing clickthroughs, incentivizing publishers to keep their content out of AI summaries and related features if given the choice.\n\nOne of the suggestions in the documents that Google considered a “hard red line” would enable publishers to prevent Google’s AI models from referencing their data in real time, but not opt out of being used to train features like AI Overviews generally. Another option, labeled as “likely unstable,” suggested that no additional controls should be added, and that publishers can opt out of being indexed on Search entirely “if not satisfied.”\n\nHere are some of the options that could have been provided to publishers. Image: Google\n\nA court hearing on May 2nd revealed that publishers are facing that ultimatum. While Google introduced a way for publishers to opt out of AI training in 2023, Google DeepMind vice president of product Eli Collins said it doesn’t apply to search-specific AI products like AI Overviews. The only way for publishers to avoid AI Overviews sucking up their content is to opt out of being crawled by Googlebot — which stops their website being indexed for Search altogether.\n\nWhen AI Overviews rolled out last year, Google decided to “silently update” the information about publisher controls with “no public announcement,” according to the document. Guidance on how to word the update also suggests that Google intentionally made it harder for publishers to know what they were actually opting out of to avoid getting “into the details of distinction” between training for Gemini, AI Overviews, and other AI models.\n\n“Do what we say, say what we do, but carefully,” Bindra said in the document.\n\nGoogle says that this document was an early list of options it was considering as AI search was evolving, and doesn’t reflect the decisions it ultimately made. “Publishers have always controlled how their content is made available to Google as AI models have been built into Search for many years, helping surface relevant sites and driving traffic to them,” Google spokesperson Peter Schottenfels said in a statement to The Verge. “New search features like AI Overviews have led to more searches, which creates new opportunities for sites to be discovered.”\n\nThe wording that Google currently uses is more upfront, saying that publishers who flag their content not to be used for AI Overviews and AI Mode will also keep it out of “all forms of search results.”"
    },
    {
        "title": "Google Is Using On-Device AI to Spot Scam Texts and Investment Fraud",
        "description": "Android’s “Scam Detection” protection in Google Messages will now be able to flag even more types of digital fraud.",
        "url": "https://www.wired.com/story/google-io-on-device-ai-scam-texts/",
        "source": "Wired",
        "publishedAt": "2025-05-13T17:00:00Z",
        "full_text": "Digital scammers have never been so successful. Last year Americans lost $16.6 billion to online crimes, with almost 200,000 people reporting scams like phishing and spoofing to the FBI. More than $470 million was stolen in scams that started with a text message last year, according to the Federal Trade Commission. And as the biggest mobile operating system maker in the world, Google has been scrambling to do something, building out tools to warn consumers about potential scams.\n\nAhead of Google’s Android 16 launch next week, the company said on Tuesday that it is expanding its recently launched AI flagging feature for the Google Messages app, known as Scam Detection, to provide alerts on potentially nefarious messages like possible crypto scams, financial impersonation, gift card and prize scams, technical support scams, and more. Combined with other AI security features for Google Messages—all of which run locally on users’ devices and do not share data or message content with the company—Android is now detecting roughly 2 billion suspicious messages a month.\n\n“The fraud is truly heartbreaking,” says Dave Kleidermacher, vice president of engineering at Android’s security and privacy division. “There’s really a very huge amount—almost epidemic and a scourge to humanity—of financial scams that are all across the world.”\n\nScammers operate all over the world, but Chinese scam groups particularly are behind millions of fraudulent messages, demanding things like “toll” payments or information for alleged postal service deliveries. When people click the links and enter their details, including payment information, scammers steal their data. In some cases, the scams are designed as a sort of smash-and-grab, where attackers quickly trick users into giving up some crumbs of information, like a pair of login credentials or a credit card number. These scams tend to be more formulaic and are potentially easier to detect. The more complex challenge is in detecting highly involved investment or romance scams—often called pig butchering scams—that build and evolve over months of messaging while scammers build a rapport with their targets before tricking them into handing over their life savings or even going into debt to send more money.\n\n“It takes time for them to get to the scam—it’s not just click on the link,” Kleidermacher says. “By having the AI on-device, you can actually watch and observe these more sophisticated conversations and then detect their scams.”\n\nCourtesy of Google Courtesy of Google\n\nIn a screenshot of the Scam Detection feature provided by Google, an encrypted RSC chat shows a typical scam message saying an EZ Pass toll payment is outstanding. The message adds that the “legal ability” to drive may be revoked if the payment is not made. The message includes a link that directs someone toward a malicious payment website. The Scam Detection overlay at the bottom of the screen says that “suspicious activity” has been detected in the message and offers a way to report and block the sender, alongside an option that allows people to flag that it is not a scam.\n\nGoogle is far from the only company using AI to try to combat scammers and stop them from reaching people’s inboxes. Some have turned to using AI to directly fight back against scammers. The British telecom company O2, for example, created an “AI Granny” that is set up to keep scammers on the phone and waste their time. And the online scam baiter Kitboga has created a series of bots to make simultaneous calls to call centers that run scams.\n\nMeanwhile, in recent months, Meta, which owns WhatsApp, Messenger, and Instagram, has started to introduce pop-up warnings when people are asked to make payments in chat messages. Elsewhere, cybersecurity company F-Secure has created a beta tool to help people identify if a message and sender are likely scammers and block messages. Putting a layer of friction in place that nudges people away from messaging accounts they don’t know or replying to messages asking for details can reduce the chances that scammers are successful.\n\nGoogle’s Kleidermacher says that the company is seeing “really positive impact” from using its machine learning systems to detect potential scam messages in real time. As the protections continue to mature, he notes that the underlying system could eventually proliferate beyond just the Google Messages app into third-party communication platforms.\n\nFor now, some of that expansion is starting within Google’s own products. The company also said on Tuesday that it is in the early phases of testing ways to incorporate scam detection for phone calls, but the capability has not been widely deployed."
    },
    {
        "title": "DOGE Used Meta AI Model to Review Emails From Federal Workers",
        "description": "DOGE tested and used Meta’s Llama 2 model to review and classify responses from federal workers to the infamous “Fork in the Road” email.",
        "url": "https://www.wired.com/story/doge-used-meta-ai-model-review-fork-emails-from-federal-workers/",
        "source": "Wired",
        "publishedAt": "2025-05-22T16:57:59Z",
        "full_text": "Elon Musk’s so-called Department of Government Efficiency (DOGE) used artificial intelligence from Meta’s Llama model to comb through and analyze emails from federal workers.\n\nMaterials viewed by WIRED show that DOGE affiliates within the Office of Personnel Management (OPM) tested and used Meta’s Llama 2 model to review and classify responses from federal workers to the infamous “Fork in the Road” email that was sent across the government in late January.\n\nThe email offered deferred resignation to anyone opposed to changes the Trump administration was making to its federal workforce, including an enforced return-to-office policy, downsizing, and a requirement to be “loyal.” To leave their position, recipients merely needed to reply with the word “resign.” This email closely mirrored one that Musk sent to Twitter employees shortly after he took over the company in 2022.\n\nRecords show that Llama was deployed to sort through email responses from federal workers to determine how many accepted the offer. The model appears to have run locally, according to materials viewed by WIRED, meaning it’s unlikely to have sent data over the internet.\n\nMeta and OPM did not respond to requests for comment from WIRED.\n\nMeta CEO Mark Zuckerberg appeared alongside other Silicon Valley tech leaders like Musk and Amazon founder Jeff Bezos at Trump’s inauguration in January, but little has been publicly known about his company’s tech being used in government. Because of Llama’s open-source nature, the tool can easily be used by the government to support Musk’s goals without the company’s explicit consent.\n\nSoon after Trump took office in January, DOGE operatives burrowed into OPM, an independent agency that essentially serves as the human resources department for the federal government. The new administration’s first big goal for the agency was to create a government-wide email service, according to current and former OPM employees. Riccardo Biasini, a former Tesla engineer, was involved in building the infrastructure for the service that would send out the original “Fork in the Road” email, according to material viewed by WIRED and reviewed by two government tech workers.\n\nIn late February, weeks after the Fork email, OPM sent out another request to all government workers and asked them to submit five bullet points outlining what they accomplished each week. These emails threw a number of agencies into chaos, with workers unsure how to manage email responses that had to be mindful of security clearances and sensitive information. (Adding to the confusion, it has been reported that some workers who turned on read receipts say they found that the responses weren’t actually being opened.) In February, NBC News reported that these emails were expected to go into an AI system for analysis. While the materials seen by WIRED do not explicitly show DOGE affiliates analyzing these weekly “five points” emails with Meta’s Llama models, the way they did with the Fork emails, it wouldn’t be difficult for them to do so, two federal workers tell WIRED."
    },
    {
        "title": "The Climate Crisis Threatens Supply Chains. Manufacturers Hope AI Can Help",
        "description": "The Covid-19 pandemic showed just how vulnerable global supply chains are. Climate shocks could pose an even greater risk.",
        "url": "https://www.wired.com/story/manufacturers-hope-ai-will-save-supply-chains-from-climate-crisis/",
        "source": "Wired",
        "publishedAt": "2025-05-02T10:00:00Z",
        "full_text": "When clothing designers place an order at Katty Fashion’s factory in Iași, Romania, they expect a bespoke service. If necessary, the factory will even rejig its production lines to make whichever garment a designer commissions. “From order to order, we may have to adapt,” says Eduard Modreanu, the company’s technical lead. “We cannot create one production line or shop floor that fits everyone.”\n\nThis adaptability is useful given the many diverse clients and orders Katty Fashion juggles, but it could also help future-proof the company against climate shocks. Katty Fashion is part of an EU-funded project called R3GROUP, which aims to help firms boost the resilience of their supply chains against disruptions caused by climate change and other factors.\n\n“Let’s say there’s going to be flooding in Germany, and we get our materials from Spain,” says Modreanu. “They cannot get here any more.”\n\nAs part of the R3GROUP project, Katty Fashion has worked with researchers to build a forensic understanding of where their textiles and accessories such as buttons and zips come from. It has revealed certain potential weak points, such as Spain and Portugal—countries that may increasingly be prone to heat waves or drought in the coming years.\n\nUsing this data, along with information gleaned from news and weather reports, the company is developing a digital twin of its supply chain and factory processes that will analyze the vulnerability of its supplier network in real time. If supply issues mean a garment has to be made in a slightly different way, a companion model will also suggest how to reconfigure production lines and worker shifts accordingly.\n\nThe Covid-19 pandemic highlighted the vulnerability of global supply chains. Companies were so hammered by logistics issues between 2020 and 2022 that many took up a strategy of “reshoring”—opting for suppliers closer to home so as to reduce delays. Some firms even decided to make completely different products, such as the gin and whiskey distilleries that began churning out hand sanitizer.\n\nUS president Donald Trump’s recent tariffs provided another shock to the supply chain system. As manufacturers weighed the possibilities of reshoring jobs in the US, many found that bringing the supply chains back to America wouldn’t be cost-effective, and instead contemplated moving factories to territories less impacted by tariffs.\n\nBut while the pandemic crunch was solved relatively quickly, and Trump’s tariffs remain unpredictable, the next big shock is likely to last longer. Climate change is already disrupting global supply chains, for instance by exacerbating droughts in Taiwan, where water supplies are crucial for semiconductor manufacturing. Digital twins—virtual models of physical systems—are one way to prepare.\n\nR3GROUP is also working with companies in the metals and plastics industries, says Tjerk Timan, project coordinator and EU project manager at France’s Industrial Technical Centre for Plastics and Composites. But while some firms are actively thinking about how they can adapt to climate-change-related threats, others are more focused on current competition. “We see a bit more of a head-in-the-sand strategy at the moment,” he says."
    }
]