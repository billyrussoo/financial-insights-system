[
    {
        "title": "Can the music industry make AI the next Napster?",
        "description": "Sure, everyone hates record labels - but the AI industry has figured out how to make them look like heroes. So that's at least one very impressive accomplishment for AI. AI is cutting a swath across a number of creative industries - with AI-generated book cov…",
        "url": "https://www.theverge.com/ai-artificial-intelligence/695290/suno-udio-ai-music-legal-copyright-riaa",
        "source": "The Verge",
        "publishedAt": "2025-06-30T19:06:06Z",
        "full_text": "is a reporter who writes about tech, money, and human behavior. She joined The Verge in 2014 as science editor. Previously, she was a reporter at Bloomberg.\n\nSure, everyone hates record labels — but the AI industry has figured out how to make them look like heroes. So that’s at least one very impressive accomplishment for AI.\n\nAI is cutting a swath across a number of creative industries — with AI-generated book covers, the Chicago Sun-Times publishing an AI-generated list of books that don’t exist, and AI-generated stories at CNET under real authors’ bylines. The music industry is no exception. But while many of these fields are mired in questions about whether AI models are illegally trained on pirated data, the music industry is coming at the issue from a position of unusual strength: the benefits of years of case law backing copyright protections, a regimented licensing system, and a handful of powerful companies that control the industry. Record labels have chosen to fight several AI companies on copyright law, and they have a strong hand to play.\n\nHistorically, whatever the tech industry inflicts on the music industry will eventually happen to every other creative industry, too. If that’s true here, then all the AI companies that ganked copyrighted material are in a lot of trouble.\n\nCan home prompting kill music careers?\n\nThere are some positive things AI music startups can accomplish — like reducing barriers for musicians to record themselves. Take the artist D4vd, who recorded his breakout hit “Romantic Homicide” in his sister’s closet using BandLab, an app for making music without a studio that includes some AI features. (D4vd began creating music to soundtrack his Fortnite YouTube montages without getting a copyright strike for using existing work.) The point of BandLab is giving more musicians around the world the opportunity to record music, send it into the world, and maybe get paid for their work, says Meng Ru Kuok, the CEO of the app’s parent company. AI tools can supercharge that, he says.\n\nThat use, however, isn’t exactly what big-time AI companies like Suno and Udio have in mind. Suno declined to comment for this story. Udio did not respond to a request for comment.\n\nSuno and Udio are designed to let music consumers generate new songs with a few words. Users type in, say, “Prompt: bossa nova song using a wide range of percussion and a horn section about a cat, active, energetic, uptempo, chaotic” and get a song, wholesale, without even writing their own lyrics. The idea that most listeners will do this regularly seems unlikely — making music is more work than just listening to it, even with text prompts — as does the idea that AI will replace people’s favorite human artists. (Also, the music is pretty bad.)\n\n“AI flooded the market with it.”\n\nA lot of listening is passive consumption, like a person putting on a playlist while doing the dishes or studying, or a business piping background tunes to customers. That background music is up for grabs — not by consumers, but by spammers using these tools. They’re already generating consumer-facing slop and putting it on Spotify, effectively crowding out real artists.\n\nThat seems to be the major use case for these apps. Generating a two-minute song on Udio costs a minimum of eight credits; free users get around 400 credits monthly; for $10 a month, you’ll get 1200, the equivalent of, at most, 150 songs. Spotify Premium individual costs $12 a month and gets you just about everything ever recorded, plus audiobooks. Also, it takes many, many fewer clicks to listen to Spotify than it does to generate your own songs — so if you’re looking for something to listen to while you cook, Spotify is just easier.\n\nBut the math there changes if you’re looking for background music for your YouTube videos — or anything else that’s meant to be listened to publicly. That means AI music threatens people who support themselves by making incidental music for advertisements, or recording “perfect fit content” for Spotify, or other, less-glamorous work. Taylor Swift’s career isn’t endangered by AI music — but the real people who make the background music for Chill Beats to Study To, or the hold music you hear on the phone, are.\n\n“I wouldn’t want to be [new-age musician] Steven Halpern and have my future career based on meditation music,” says David Hughes, who served as CTO for the Recording Industry Association of America (RIAA) for 15 years. He now works as a tech consultant for the music industry at Hughes Strategic. “AI flooded the market with it. There’s no business making it anymore.”\n\nAs in other creative industries, AI music tools are poised to hollow out the workaday middle of the market. Even new engineering tools have their downsides. Jimmy Iovine, who eventually founded Interscope Records and Beats Electronics, started his career as an audio engineer before making his name by producing Patti Smith’s Easter. This is kind of like starting in the mail room and becoming the CEO; if more of the engineering work is done by AI, that removes career paths. The next Jimmy Iovine might not get his start, Hughes says. “How does anyone apprentice?”\n\nAnd it’s (possibly) illegal\n\nAbout a year ago, the major labels brought suit against Suno and Udio. The fight is about training data; the labels say the companies stole copyrighted work and violated copyright law by using it to build their models. Suno has effectively admitted it trained its AI song generator on copyrighted work in documents filed in court; so has Udio. They’re saying it was fair use, a legal framework under which copyrighted work can be used to create new work.\n\n“Thou shalt not steal.”\n\nSo far, the legal takes on AI have been contradictory, and at times, baffling. There doesn’t seem to be a consistent through line, so it’s hard to know where the law will ultimately end up. Still, music has its own legal history that comes to bear — from unauthorized sampling. That may mean it’s entitled to stronger protections.\n\nIn Bridgeport Music v. Dimension Films, a case about NWA’s sample of Funkadelic’s “Get Off Your Ass and Jam,” the US Court of Appeals ruled that the uncompensated sampling was in violation of copyright law. In the decision, the court found that only the copyright owner could duplicate the work — so all sampling requires a license. Some other courts have rejected that ruling, but it remains influential. There’s also Grand Upright Music v. Warner Bros. Records, in which the US Southern District of New York ruled that Biz Markie’s sample of Gilbert O’Sullivan’s “Alone Again (Naturally)” was copyright infringement. The written opinion in the case begins, “Thou shalt not steal.”\n\n“Some of the sampling cases have suggested that sound recordings might be entitled to stronger protections than other copyrighted works,” says James Grimmelmann, a professor at Cornell Law School. Those protections may extend beyond sampling to generative AI, especially if the AI outputs too closely resemble copyrighted work. “From that perspective, music becomes kind of untouchable. You just can’t do this kind of work on it.”\n\nMusic is also complicated — since performances are bound up in rights of publicity. In the case of the fake Drake track, the soundalike may violate Drake’s right to publicity. Artists such as Tom Waits and Bette Midler have won suits against more mundane human soundalikes. Proving that someone meant to violate Drake’s right to publicity might be even more straightforward if the lawsuit contains the prompt.\n\nThis may be an easier case for music companies to make\n\nAs in other AI fair use cases, one of the key questions is whether a derivative work, such as “BBL Drizzy,” is intended to replace or disrupt a market for an original one. In 2023, the Supreme Court ruled that Lynn Goldsmith’s copyright had been infringed on by Andy Warhol when he screenprinted one of her photos of Prince. One of the key factors was that Vanity Fair had licensed Warhol’s work instead of Goldsmith’s — and she received no credit or payment.\n\nIn May, Register of Copyrights Shira Perlmutter released a pre-publication report that found that AI training in general was not necessarily fair use. In the report, one of the factors considered was whether an AI product supplanted the use of the original. “The use of pirated collections of copyrighted works to build a training library, or the distribution of such a library to the public, would harm the market for access to those works,” the report said. “And where training enables a model to output verbatim or substantially similar copies of the works trained on, and those copies are readily accessible by end users, they can substitute for sales of those works.”\n\nThis may be an easier case for music companies to make than, let’s say, ad writers. (What copywriter wants to admit they’re so uncreative they can be replaced by a machine, first of all?) Not only are there fewer of them, which allows them to easily negotiate as a bloc, it’s simple enough to point to the output of AI music singing Jason Derulo’s name, or mimicking “Great Balls of Fire.” That’s pretty clear-cut.\n\nAnother crucial factor — one that matters particularly to the music industry — was lost licensing opportunities. If copyrighted works are being licensed as AI training data, doing a free-for-all snatch and grab robs rights holders of their ability to participate in that market, the report notes. “The copying of expressive works from pirate sources in order to generate unrestricted content that competes in the marketplace, when licensing is reasonably available, is unlikely to qualify as fair use,” the report says.\n\nThe RIAA alleges illegal copying on the front end and infringing outputs on the back end\n\nRecently, Anthropic got a ruling in a copyright case that differs from this analysis. According to Judge William Alsup of the Northern District of California, using books for training data is fair play — with two big caveats. First, any inputs must be legally acquired, and second, the outputs must be non-infringing. Since Anthropic pirated millions of books, that still leaves the door open for massive damages, even if using the books to train isn’t wrong.\n\nWhen it comes to the Suno and Udio suits, the RIAA alleges illegal copying on the front end and infringing outputs on the back end, Grimmelman says. Suno and Udio can introduce evidence to rebut those allegations, but the ruling isn’t ideal to knock down the RIAA’s suit. It’s also not clear Suno can rebut those allegations. “Suno’s training data includes essentially all music files of reasonable quality that are accessible on the open Internet, abiding by paywalls, password protections, and the like,” its lawyers wrote in the filing arguing Suno’s training data was fair use. While Udio admits it may have used some copyrighted recordings, its response to the suit doesn’t mention how they were acquired; if Udio bought those songs, under the Anthropic case’s reasoning, it might be off the hook.\n\nBut that’s not the only pertinent ruling. The very next day, in a case where authors alleged Meta had infringed on their copyright by training on their books, Judge Vince Chhabria directly addressed Alsup’s ruling, saying it was based on an “inept analogy” and brushed aside “concerns about the harm it can inflict on the market for the works it gets trained on.” While Chhabria found in favor of Meta, he noted that it was because of bad lawyering on the part of the authors’ team.\n\nStill, the finding is better for music companies on the input side, because it doesn’t draw a distinction around piracy, Grimmelman says. It is much, much worse for Suno and Udio on the output side. “Chhabria holds that ‘market dilution’ — creating lots of works that compete with the plaintiffs’ works — is a plausible theory of market harm,” he says in an email after the ruling. That’s also in line with the copyright office’s memo.\n\n“We live in a world where everything is licensed.”\n\nSuno and Udio have some other trouble; some generative AI companies have been licensing artists’ works. By offering nothing for works that other companies have licensed, they are messing up the market. “The fact that there are existing licensing deals for music training is relevant, if that market is better-developed than the market for licensing books,” Grimmelman says. Chhabria’s opinion points out that it’s quite difficult to license books for training, because the rights are so fragmented. “Either finding that there is a market that copyright owners should be able to exploit, or finding that there isn’t one, is circular, in that the court’s holding tends to reinforce its findings about the market.”\n\nThat effectively stacks the deck against Suno and Udio, and any other music companies that didn’t license their AI training data. Music licenses for AI training cost between $1 and $4 per track. High-quality datasets can cost from $1 to $5 per minute for non-exclusive licenses, and from $5 to $20 per minute for exclusive licenses. Transcription and emotion labeling, among other factors, garner higher prices.\n\nAnd unlike in other industries, music already has an IP copyright and collection system, notes Kuok, of the BandLab recording app. The app has its own AI tool called SongStarter, which lets people who are making music begin with an AI-generated track. Kuok favors licensing music for AI training, and making sure musicians get paid.\n\n“We live in a world where everything is licensed,” Kuok says. “The solution is an evolution of what existed before.” How to collect, who collects, and how much gets collected strikes Kuok as being open questions, but licensing itself is not. “We work in an all-rights-reserved world where we believe copyright is an important institution.”\n\n“Everyone knew it was required.”\n\nTo address that, BandLab has options for its licensing program. Artists can say they are open to AI licensing, which means they’ll be contacted if a company wants to license their work. If they agree, their work is then bundled with an assortment of other artists’ approved works for the licensing deal, which BandLab negotiates on their behalf. Kuok says Bandlab is discussing training deals now, though he declined to give specifics about the financial components of those deals, or who he was in talks with,\n\nKuok did say there were some other things he considers in negotiations. “It’s important what the use is for,” he says. “That has to be specified. These are fixed-term contracts, fairly large deals, worth six figures over a multiyear period.” He recommends maintaining as much control as possible over copyrighted work to avoid diluting the value of existing IP.\n\nThat may be why Suno and Udio are reportedly in talks with the majors to license music for training their models. Other AI companies do already. Ed Newton-Rex, formerly of Stability AI, told me all the music he’d worked with at Stability was licensed; he even quit his position as a vice president at Stability after the company decided training on copyrighted data was fair use. He’d been working on the systems since 2010, and licensing had been the norm until fairly recently, he told me.\n\n“Everyone knew it was the law,” he says. “Everyone knew it was required.”\n\nBut after ChatGPT came out, some music AI companies thought they might also just grab whatever existed and let the courts sort it out. “I don’t think it’s fair use,” he says. “Given that gen AI generally competes with what it’s trained on, it’s a bad thing to take creators’ works and outcompete them.” Newton-Rex has also demonstrated ways to get Suno in particular to output music that’s strikingly similar to copyrighted work. That, too, is a problem.\n\n“I don’t think there’s an outcome where this winds up being all fair use,” says Grimmelman.\n\nCorrection, July 1st: This story originally identified the CEO of BandLab as Kuok Meng Ru. His name is Meng Ru Kuok."
    },
    {
        "title": "Here comes the AI sponcon",
        "description": "Social media is filled with an endless supply of people selling things, from Shein try-on hauls to health supplement and gadget product placements. Influencer marketing disrupted traditional advertising, creating an army of living room salespeople pumping out…",
        "url": "https://www.theverge.com/news/684572/tiktok-ai-advertising-videos-try-on-product-placement",
        "source": "The Verge",
        "publishedAt": "2025-06-14T22:15:34Z",
        "full_text": "Social media is filled with an endless supply of people selling things, from Shein try-on hauls to health supplement and gadget product placements. Influencer marketing disrupted traditional advertising, creating an army of living room salespeople pumping out content meant to entice strangers at scale — and tech companies’ vision for the future includes more automation.\n\nTikTok announced today it was adding new capabilities to Symphony, the company’s AI ads platform it launched in 2024. The features go beyond generating basic videos and images — instead, the system’s new output mimics what audiences are used to seeing from human influencers. The company says advertisers will be able to upload images, provide a text prompt, and generate videos with virtual avatars holding products, trying on and modeling clothing, and displaying a brand’s app on a phone screen. Some features already available to TikTok users — like creating a video out of a photo — will also now be available to advertisers.\n\nAI creep in the influencer industry has been a steady development: advertisers already have the option of using synthetic characters (sometimes resembling real people) to do things like read scripts to promote brands and products. This new set of features brings an interactivity, with virtual avatars essentially acting like human influencers by using and modeling products. or advertisers, the appeal is a mix of automating processes and cutting costs — an AI avatar can’t demand specific rates or terms in a contract, and a brand can generate an endless amount of content without recording each video separately. AI tools are also being used to target specific audience members, generate ideas for content, and dub audio into different languages. Some advertisers are moving slowly with AI-generated content or are even outright resistant to it. But the expansion of AI ads tools on TikTok signals that the platform, at least, is taking it seriously: why share TikTok Shop affiliate earnings with a thousand random creators when you could instead farm it out to a few virtual faces and bodies?\n\nFor human influencers, the potential threat of AI is two-fold: synthetic content could be used in place of human work, and the influx of AI-generated videos could drive rates down for everyone. But so far, AI tools in the influencer space are largely behind the scenes: content creators say they are using AI tools to edit and plan content or find brand deals, even as tech companies continue to push AI-generated profiles and characters. AI-generated sponsored videos — especially of something like trying on clothes or using an app — significantly expand the bounds of influencer content. Is it really a product recommendation if the entity trying to sell you on it doesn’t exist? And if all brands need to promote something is a body, what does it mean for the human influencers that the cheapest, fastest path with the least resistance is being pushed by the platforms they rely on for their income?\n\nTikTok says all content generated using the ads tool will have a label indicating it as AI-generated, and that it will go through “multiple rounds of safety review.”"
    },
    {
        "title": "Klarna’s CEO is now taking your calls — over an AI hotline",
        "description": "Klarna CEO Sebastian Siemiatkowski has already used AI to replace himself during earnings reports, and now Klarna is using AI to clone him another way: through an AI-powered phone hotline. The hotline is now open for users to provide feedback to an interactiv…",
        "url": "https://www.theverge.com/news/685814/klarna-ceo-calls-ai-hotline",
        "source": "The Verge",
        "publishedAt": "2025-06-11T22:46:19Z",
        "full_text": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nKlarna CEO Sebastian Siemiatkowski has already used AI to replace himself during earnings reports, and now Klarna is using AI to clone him another way: through an AI-powered phone hotline.\n\nThe hotline is now open for users to provide feedback to an interactive AI version of Siemiatkowski that’s trained on his “real voice, insights, and experiences,” according to a press release.\n\n“Customers, including consumers and merchants alike, can speak to it about product features, issues and suggestions for improvements,” Klarna says. “‘AI Sebastian’ can also answer questions about Klarna’s vision and mission, and speak about its founding story and journey so far, based on learnings from ‘real life Sebastian.’”\n\nAfter the hotline takes a call, a transcript and summary are generated that are “automatically analyzed by a large language model (LLM), populating an internal live feed and innovation dashboard.” Klarna says its product and engineering teams will then review the “actionable insights immediately” and assign “priority tasks.” The company claims that issues raised today could “translate into tangible product improvements already the following day.”\n\nPhone numbers for the hotline are available now in the US, +14246675385, and Sweden, +46852512594. I called the US hotline just to see what the experience was like. After a brief message telling me the call may be recorded, the AI version of Siemiatkowski prompted me to share feedback about using Klarna. No matter what I asked AI Siemiatkowski, even sillier questions, the AI generally tried to push me back to sharing feedback.\n\nWhen I asked if it had talked to Fortnite’s AI Darth Vader, for example, here’s what it said:\n\nThat’s an interesting question. My focus is entirely on Klarna and gathering feedback to improve our services. I don’t have information or conversations with characters from games like Fortnite. I’m here to hear your thoughts on Klarna. Is there anything else about your experience with Klarna that you’d like to share, or any ideas you have for how we could make things even better?\n\nOpenAI also has a hotline to talk with an AI chatbot. You can call ChatGPT at 1-800-CHATGPT."
    },
    {
        "title": "The unbearable obviousness of AI fitness summaries",
        "description": "After nearly a decade of wearables testing, I've amassed a truly terrifying amount of health and fitness data. And while I enjoy poring over my daily data, there's one part I've come to loathe: AI summaries. Over the last two years, a deluge of AI-generated s…",
        "url": "https://www.theverge.com/fitness-trackers/694140/ai-summaries-fitness-apps-strava-oura-whoop-wearables",
        "source": "The Verge",
        "publishedAt": "2025-06-27T16:56:26Z",
        "full_text": "is a senior reporter focusing on wearables, health tech, and more with 13 years of experience. Before coming to The Verge, she worked for Gizmodo and PC Magazine.\n\nAfter nearly a decade of wearables testing, I’ve amassed a truly terrifying amount of health and fitness data. And while I enjoy poring over my daily data, there’s one part I’ve come to loathe: AI summaries.\n\nOver the last two years, a deluge of AI-generated summaries has been sprinkled into every fitness, wellness, and wearable app. Strava introduced a feature called Athlete Intelligence, pitched as AI taking your raw workout data and relaying it to you in “plain English.” Whoop has Whoop Coach, an AI chatbot that gives you a “Daily Outlook” report summarizing the weather, your recent activity and recovery metrics, and workout suggestions. Oura added Oura Advisor, another chatbot that summarizes data and pulls out long-term trends. Even my bed greets me with summaries every morning of how its AI helped keep me asleep every night.\n\nEach platform’s AI has its nuances, but the typical morning summary goes a bit like this:\n\nGood morning! You slept 7 hours last night with a resting heart rate of 60 bpm. That’s in line with your weekly average, but your slightly elevated heart rate suggests you may not be fully recovered. If you feel tired, try going to bed earlier tonight. Health is all about balance!\n\nThat might seem helpful, but those summaries are usually placed next to a chart with the same data. It’s worse for workouts. Here’s one that Strava’s Athlete Intelligence generated for a recent run:\n\nIntense run with high heart rate zones, pushing into anaerobic territory and logging a relative effort well above your typical range.\n\nThanks? I can ask Athlete Intelligence to “say more,” but it regurgitates my effort, heart rate zone, and pace metrics I can see in graphs in the workout summary. If you didn’t know anything about my athletic history or the circumstances surrounding this run, this summary might read as insightful. Here’s what the summary left out:\n\nIt was dangerous to triple my mileage in only my second high-heat run of the year, given the high humidity, 85-degree-plus weather, and my spotty workout history over the past two months compared to the six months before it. Strava has access to weather data and every workout I’ve done in the past five years.\n\nI had to cut this run short because I fell and shredded my hand and knees. This is information Strava has access to, as I uploaded a gnarly picture in addition to text notes. After adding said notes, the updated summary only reflected that I cut the run short. My injury changed nothing about its insights, even though it’s the most important thing that happened during this run.\n\nI don’t know guys, without Athlete Intelligence, would I have known my elevation gain was a modest 88ft? Screenshot: Strava\n\nA more helpful insight might’ve been: “You ran during record-breaking heat for your region. While you maintained a consistent and steady pace, you have a bad habit of ramping up mileage too quickly after prolonged breaks, leading to several self-reported injuries in the past five years. A safer alternative would be lower mileage runs over two weeks to acclimate to rising temperatures. Since you’re injured, stick to low-intensity walks until your wounds have healed.”\n\nRunna, a popular running app that also features AI insights, generated a slightly more useful summary. It said my next run should be “easy,” one that’s perfectly timed for me to recharge. I’m sorry, but 48 hours isn’t enough time for my knees to safely heal without risk of re-opening my wounds.\n\nThe in-app chatbots aren’t much better. Yesterday morning, I asked Whoop Coach if I should run today because I injured myself on my last run. It told me: “Whoop is unable to reply to the message you sent. Please try sending a different message.” I tried reframing my prompt, saying, “I’m injured and have a limp. Generate a low-intensity workout alternative while I recover.” I was prompted to contact Whoop Membership Services to continue the conversation.\n\nOura Advisor was more helpful, noting in my daily summary: “With your Readiness dipping and recent stressors like heat, an injury, and higher glucose, your body may feel more fatigued than usual today.” It suggested I prioritize rest. When asked, “What types of movement are okay when you have an injured knee and a slight limp?” it responded with common-sense answers like a short and easy walk if there’s no pain, light stretching, and a reminder to completely rest if I feel any sharp discomfort. This is closer to an ideal response, but I had to guide it to the type of answer I wanted. The insights are so general-purpose that they benefit self-quantification newbies — and even then, only if they’re allergic to googling.\n\nMy botched run is exactly the type of scenario where tech CEOs say AI insights could be most useful. In theory, I agree! It would be nice to have a competent, built-in chatbot that I could ask more nuanced questions.\n\nPrevious Next\n\n\n\n1 / 3 I tried this exact query twice and got the same result. Screenshot: Whoop\n\nFor example, I’ve had an irregular sleep schedule this month. I asked Oura Advisor if my sleep and readiness trends showed signs of an elevated risk of injury. I also asked if I had abnormally high levels of sleep debt this month. In both cases, it said no — it said I was improving.\n\nWhat resulted was an hour-long debate with a chatbot that left me questioning my own lived experience. When I tried asking it to delve into a particularly stressful week earlier this month, it told me its insights were “limited to [my] most recent week and current trends.” That sort of defeats the point of having six years’ worth of Oura data.\n\nAfter months of perusing Reddit and other community forums, I know I’m not the only person who finds these AI features to be laughable. And yet, Holly Shelton, Oura’s chief product officer, tells me that the response to Oura Advisor has been “overwhelmingly positive,” with 60 percent of users using it multiple times a week and 20 percent using it daily. “Beyond frequency,” Shelton says, “It’s delivering real impact: 60 percent say Advisor has helped them better understand metrics or health concepts they previously found confusing.”\n\nMeanwhile, Strava spokesperson Brian Bell tells me Athlete Intelligence was intended to help beginner athletes and that “the response to the feature remains strong” with about “80 percent of those opting in to give feedback finding the feature ‘very helpful’ to ‘helpful.’”\n\nA Whoop spokesperson wasn’t able to respond by publication.\n\nThese milquetoast summaries? They’re probably the best compromise between speed, cost, usefulness, data privacy, and legal liability\n\nI understand my frustrations stem from the inherent limitations of LLMs and the messiness of private health data. Strava might be a de facto fitness data hub, but it lacks all the health data points necessary to create holistic, useful, and personalized insights. It’d take Oura Advisor a long time to crunch a year’s worth of sleep data for trends. That latency is guaranteed to offer a bad user experience. Not to mention, they’d likely have to up their subscription from $5.99 a month to add that type of computing power. I’m not sure, but Whoop Coach may have declined my injury-related queries to protect itself from liability if something bad happened to me from following its suggestions. These milquetoast summaries are probably the best compromise between speed, cost, usefulness, data privacy, and legal liability. But if that’s the case, then let’s be honest. Current AI features are repackaged data, much like book reports written by a fourth-grader relying on a Wikipedia summary instead of reading the book. It’s a feature tacked on with duct tape and a dream because AI is the zeitgeist. Perhaps one day, these AI insights will create a useful and personalized experience with actionable insights. That day is not today, and it’s not worth paying extra for."
    },
    {
        "title": "Google has a new AI model and website for forecasting tropical storms",
        "description": "Google is using a new AI model to forecast tropical cyclones and working with the US National Hurricane Center (NHC) to test it out.  Google DeepMind and Google Research launched a new website today called Weather Lab to share AI weather models that Google is…",
        "url": "https://www.theverge.com/news/685820/google-ai-forecast-typhoon-hurricane-tropical-storm",
        "source": "The Verge",
        "publishedAt": "2025-06-12T15:02:43Z",
        "full_text": "is a senior science reporter covering energy and the environment with more than a decade of experience. She is also the host of Hell or High Water: When Disaster Hits Home , a podcast from Vox Media and Audible Originals.\n\nGoogle is using a new AI model to forecast tropical cyclones and working with the US National Hurricane Center (NHC) to test it out.\n\nGoogle DeepMind and Google Research launched a new website today called Weather Lab to share AI weather models that Google is developing. It says its new, experimental AI-based model for forecasting cyclones — also called typhoons or hurricanes when they reach a certain strength — can generate 50 different scenarios for a storm’s possible track, size, and intensity up to 15 days in advance. The NHC is working with Google to evaluate the effectiveness of the model.\n\nAdvances so far don’t eliminate the need for traditional weather models\n\nGoogle released a research paper today, which has yet to be peer-reviewed, on how its tropical cyclone model works. It claims that its model’s predictions are at least as accurate as those of traditional physics-based models. We’ll have to see what the National Hurricane Center’s rating of it is as the Atlantic hurricane season churns through November.\n\nFor now, the aim is to strengthen NHC’s forecasting in order to give people more accurate warnings and time to prepare for a storm. According to Google, its model’s five-day predictions for cyclone tracks in the North Atlantic and East Pacific were 87 miles (140 km) closer, on average, to the storm’s actual track than predictions from the European Center for Medium-Range Weather Forecasts (ECMWF) in 2023 and 2024.\n\nWeather Lab’s interactive website lets people see how AI models compare to the ECMWF’s physics-based models. But Google is emphasizing that its website is just a research tool for now — not something the public should rely on for forecasts.\n\nAnimation showing the Google model’s prediction for Cyclone Alfred when it was a Category 3 cyclone in the Coral Sea. The model’s ensemble mean prediction (bold blue line) correctly anticipated Cyclone Alfred’s rapid weakening to tropical storm status and eventual landfall near Brisbane, Australia, seven days later, with a high probability of landfall somewhere along the Queensland coast. Credit: Google\n\nThe company says it’s also working with the Cooperative Institute for Research in the Atmosphere at Colorado State University and other researchers in the UK and Japan to improve its AI weather models.\n\nThe importance of real-world observations and older weather models in developing these new kinds of tools is one reason why AI is so far only poised to assist traditional weather forecasting instead of replacing it. Adjusting to a changing climate will also hinge on the ability to collect and analyze new data on increasingly extreme and erratic weather events.\n\n“For a long time, weather has been viewed as a public good, and I think, you know, most of us agree with that … Hopefully we can contribute to that, and that’s why we’re trying to kind of partner with the public sector,” Peter Battaglia, a research scientist at Google DeepMind, said in a press call when The Verge asked about concerns surrounding privatizing weather services.\n\nTellingly, Google’s announcement today doesn’t mention the climate crisis like the company has in previous launches for this kind of program. “As climate change drives more extreme weather events, accurate and trustworthy forecasts are more essential than ever,” it said in a December 4 announcement for GenCast."
    },
    {
        "title": "Applebee’s and IHOP have plans for AI",
        "description": "Applebee’s and IHOP plan to launch an AI-powered “personalization engine” that could help its restaurants provide recommendations and customized deals, according to a report from The Wall Street Journal. The personalization engine would use a customer’s past …",
        "url": "https://www.theverge.com/news/690458/applebees-ihop-ai-restaraunt-recommendations",
        "source": "The Verge",
        "publishedAt": "2025-06-20T17:21:52Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nApplebee’s and IHOP plan to launch an AI-powered “personalization engine” that could help its restaurants provide recommendations and customized deals, according to a report from The Wall Street Journal. The personalization engine would use a customer’s past purchases — or the orders of customers similar to them — to make recommendations.\n\nJustin Skelton, the chief information officer at the restaurants’ parent company, Dine Brands, tells the Journal that an AI-powered personalization system would be designed to boost customer loyalty, as well as serve as a way to upsell products. As noted by the Journal, IHOP already has some information about its customers’ ordering habits through its rewards program.\n\nAs noted by the Journal, Dine Brands is considering rolling out AI tools designed for staff members as well. Along with exploring the use of AI-powered cameras to detect when a table needs cleaning, it’s also testing an AI app for managers."
    },
    {
        "title": "AI is ruining houseplant communities online",
        "description": "\"Maybe you could organize your plants like this,\" my friend's text message said, with an attached photo of white pots of plants floating midair in front of a huge, sunny window. As a newbie plant collector, I do need to organize my growing collection of flora…",
        "url": "https://www.theverge.com/ai-artificial-intelligence/691355/ai-is-ruining-houseplant-communities-online",
        "source": "The Verge",
        "publishedAt": "2025-06-25T20:59:46Z",
        "full_text": "“Maybe you could organize your plants like this,” my friend’s text message said, with an attached photo of white pots of plants floating midair in front of a huge, sunny window. As a newbie plant collector, I do need to organize my growing collection of flora, but not like this — the photo was AI-generated and the plants depicted were not real.\n\nEven as a beginner, I was able to identify issues with the photo. Obviously, my plants cannot physically defy gravity, but most egregiously, the organizational method of putting plants in direct sunlight would completely incinerate their leaves. This was the first time I came across AI-generated plant content from well-meaning people who earnestly believe it is real, but its proliferation is a growing problem in plant-lover communities online and off.\n\nWhile online retailers have often scammed less-knowledgeable consumers, the rise of online stores using AI-generated photos of fake, usually vibrant, and otherworldly-looking plants to fool consumers into buying seeds for plants that do not exist has been remarked upon by multiple plant-specific blogs, podcasts, and communities in the last three years.\n\nThe offer of pink pastel monstera seeds, a variation of a favorite houseplant that cannot be real because of the species’ lack of pink pigment genes, is a common scam online, with even Google’s AI assistant confirming its existence. AI-generated photos of bright red and blue hostas also often advertise seeds, but pigment genes that generate these vibrant colors do not exist, and hostas grown from seeds usually inherit unpredictable traits from their two parent plants. Even if these pigment genes existed, there would be no guarantee that you could feasibly create these plants from seeds.\n\n“In the springtime we get customers asking about AI-generated plants multiple times a week,” says Casey Schmidt Ahl, engagement manager at the Colonial Gardens, a garden center in Phoenixville, Pennsylvania, who has published a blog post teaching plant owners how to spot fake AI plant scams online. “We always make sure that we tell them that it is an AI-generated — or at least heavily photoshopped — image, and that they need to be really careful because we know that if we just say we don’t have it, they are more likely to just go online and buy it.”\n\nAccording to the post written by Ahl, one customer called in asking about a black bleeding heart plant. Ahl only had bleeding hearts — which have hanging petals that look like hearts dripping in liquid — in white and pink, so she searched for the black version online. The red flags were there: there was only one image of the plant across multiple websites, and there wasn’t specific information about the plant’s growth or variety.\n\nPlant care misinformation regurgitated by AI chatbots and apps is also a common occurrence, according to Ahl. Plant care has always involved folklore and pseudoscience, Ahl explains, like dipping leaf cuttings into honey to help them grow roots or using cinnamon to heal plant wounds. AI is now a part of a larger misinformation ecosystem that includes plant influencers without cited sources repeating plant care myths and plant ID apps. When Ahl writes articles for the Colonial Gardens blog, she uses scholarly papers and growers’ guides as sources, but she says it is unclear where an AI-powered bot like ChatGPT is pulling its information from.\n\n“It’s always important for us to be able to ground our advice in science,” Ahl says, adding that garden centers have already fallen behind the curve of connecting with clients because of plant influencers on social media who are able to go viral quickly but rarely cite scientific studies. Ahl sees AI-generated misinformation as a part of an ecosystem that discourages plant owners from talking to an expert face-to-face. “It’s disconnecting us further from reality, relationships with nature, and also our community.”\n\nAhl isn’t the only one who feels this way. Most plant communities on Reddit ban AI-generated images, and there are strong feelings against AI-generated plant care advice, which some community members say is often wrong and doesn’t take the human variable into account. Plant care can vary according to where the plant owner lives, whether the plant owner is a beginner or more experienced collector, and the gardening supplies that are immediately available. But most importantly, AI-generated content — both photos of fake plants and care misinformation — disrupt community engagement, which is what many collectors are seeking when joining these forums.\n\n“Most people are looking to socially connect based on real lived experiences, in a community with others like you, otherwise it would be easier to simply Google a question for answers,” says Redditor known as Caring_Cactus, who moderates four plant-related subreddits, in a written interview. Moderators also struggle with AI automated content that is posted in large volumes by bots. “They create a lot of generic responses that are full of false information. Most people also view it as lazy and disingenuous with ulterior motives.”\n\nThis kind of content is “discouraging any meaningful engagement” because it’s not grounded in reality, Caring_Cactus continues. “They’re trying to farm attention with low quality content, and it creates less opportunities for real connection by wasting people’s precious time when they want to socialize online.”\n\nThere are also more existential issues that arise from AI-generated plants, as fake photos and AI-generated care hacks might take away the wonder of how special being a part of the growth and development of real plants can be. “There’s a lot of different reasons that people garden, including supporting wildlife and pollinators with native plants or growing their own food, and these AI images and scams are not really interested in connecting you with a broader goal,” Ahl says. Instead, this content wants to catch your attention through “a curiosity dopamine sort of response,” Ahl says, or even try to scam you into buying seeds for blue sunflowers.\n\nAdditionally, the use of AI-powered apps in gardening, where plants and the issues that might be killing them are identified through photos, is like taking a shortcut, which defeats the whole point of the hobby. “If instead of looking at your plants and making sure that they’re watered correctly or reaching out to an expert, you always just take a picture with your AI app and have it tell you what’s wrong, you are letting AI do the thinking for you and you’re not doing the full connection and the mindfulness of having plants,” Ahl adds.\n\nAI-generated plant slop might also make magical-looking real plants look fake, risking the destruction of the wonder collectors feel toward new plant varieties. “This year they debuted the firefly petunia, which is a glow-in-the-dark petunia,” Ahl says, adding that, admittedly, this sounds like something that shouldn’t exist. The proliferation of AI-generated photos has resulted in an existential questioning of reality, and it might be driving us even further away from nature. “It definitely draws away from the majesty of plants because there are certainly lots of existing varieties that are amazing without having a galaxy pattern on their petals.”"
    },
    {
        "title": "Google Gemini can now handle scheduled tasks like an assistant",
        "description": "Google is taking another step toward making Gemini a more helpful assistant. It’s rolling out “scheduled actions,” a feature AI Pro and AI Ultra subscribers can use to ask the AI assistant to perform tasks at specific times, like providing a summary of their …",
        "url": "https://www.theverge.com/news/681762/google-gemini-scheduled-actions-planned-tasks",
        "source": "The Verge",
        "publishedAt": "2025-06-06T19:39:34Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nGoogle is taking another step toward making Gemini a more helpful assistant. It’s rolling out “scheduled actions,” a feature AI Pro and AI Ultra subscribers can use to ask the AI assistant to perform tasks at specific times, like providing a summary of their calendar at the end of each day or generating ideas for blog posts every Monday.\n\nUsers can also have Gemini complete one-off tasks using this feature, such as asking for a summary of an award show the day after it happens, Google says. “Just tell Gemini what you need and when, and it will take care of the rest,” the company writes in a post announcing the change.\n\nGemini subscribers can manage planned tasks by heading to the “scheduled actions” page in the Gemini app’s settings. Android Authority first spotted an early version of the feature in April, which comes as Google aims to have its AI assistant perform more agent-like tasks.\n\nOpenAI’s ChatGPT offers a similar feature to subscribers that allows the AI chatbot to send you reminders or perform recurring actions."
    },
    {
        "title": "Seriously, What Is ‘Superintelligence’?",
        "description": "In this episode of “Uncanny Valley,” we talk about Meta’s recent investment in Scale AI and its move to build a superintelligence AI research lab. So we ask: What is superintelligence anyway?",
        "url": "https://www.wired.com/story/uncanny-valley-podcast-superintelligence/",
        "source": "Wired",
        "publishedAt": "2025-06-20T19:22:03Z",
        "full_text": "Meta just announced a major move in its AI efforts—investing in Scale AI and building a superintelligence AI research lab. While Meta has been trying to keep up with big names in the AI race, such as OpenAI, Anthropic, and Google, the company's new strategy includes dropping some serious cash to acquire talent and invest in Scale AI.\n\nToday on the show, we dive into the deal between Meta and Scale AI, including what Meta aims to get out of investment, and we ask the question we are all wondering: What is superhuman intelligence, anyway?\n\nYou can follow Michael Calore on Bluesky at @snackfight, Lauren Goode on Bluesky at @laurengoode, and Katie Drummond on Bluesky at @katie-drummond. Write to us at uncannyvalley@wired.com.\n\nHow to Listen\n\nYou can always listen to this week's podcast through the audio player on this page, but if you want to subscribe for free to get every episode, here's how:\n\nIf you're on an iPhone or iPad, open the app called Podcasts, or just tap this link. You can also download an app like Overcast or Pocket Casts and search for “Uncanny Valley.” We’re on Spotify too.\n\nTranscript\n\nNote: This is an automated transcript, which may contain errors.\n\nMichael Calore: How is everybody doing this week?\n\nKatie Drummond: Well, I'm back and I'm so happy to be here. I ate so much butter in France last week, the first couple nights I had to myself. And so what would a normal person do in Paris? Maybe they'd go out and sit at the bar and have dinner alone, maybe they'd meet up with a friend. I ate butter and bread alone in my hotel room. And let me tell you, if you're listening out there and you are a mom and you have a young kid or young kids, if you're married, if you struggle with having a spouse and a child and maybe some pets and a busy job, there is no better experience than eating French butter and bread alone in a hotel room.\n\nMichael Calore: Wow.\n\nLauren Goode: Uncanny Valley, brought to you by the dairy lobby.\n\nKatie Drummond: By the French dairy lobby. I feel incredible. How are you guys?\n\nLauren Goode: I'm OK. I got bangs.\n\nKatie Drummond: You did?\n\nLauren Goode: Yeah. So most of our listeners can't see it unless you watch our new video promos online, but I got bangs. There's often a correlation between things going on in the world and women cutting their bangs. That's all I'm going to say about that. But otherwise, I'm great. I'm great. Rate the bangs, go online. Thumbs up, thumbs down.\n\nKatie Drummond: Five out of five.\n\nLauren Goode: Thank you. Katie, really, you were my inspiration here.\n\nKatie Drummond: Oh, no, that's too kind. But I do love a bang and hate a forehead."
    },
    {
        "title": "Amazon Rebuilt Alexa Using a ‘Staggering’ Amount of AI Tools",
        "description": "From writing code to testing the new Alexa+, Amazon engineers used generative AI throughout the build process. We talk to Amazon's Daniel Rausch about the next version of the voice assistant.",
        "url": "https://www.wired.com/story/amazon-daniel-rausch-alexa-plus/",
        "source": "Wired",
        "publishedAt": "2025-06-18T18:29:22Z",
        "full_text": "Daniel Rausch, Amazon’s vice president of Alexa and Echo, is in the midst of a major transition. More than a decade beyond the launch of Amazon’s Alexa, he’s been tasked with creating a new version of the marquee voice assistant, one that’s powered by large language models. As he put it in my interview with him, this new assistant, dubbed Alexa+, is “a complete rebuild of the architecture.”\n\nHow did his team approach Amazon’s largest ever revamp of its voice assistant? They used AI to build AI, of course.\n\n“The rate with which we're using AI tooling across the build process is pretty staggering,” Rausch says. While creating the new Alexa, Amazon used AI during every step of the build. And yes, that includes generating parts of the code.\n\nThe Alexa team also brought generative AI into the testing process. The engineers used “a large language model as a judge on answers” during reinforcement learning processes where the AI selected what it considered to be the best answers between two Alexa+ outputs.\n\n“People are getting the leverage and can move faster, better through AI tooling,” Rausch says. Amazon’s focus on using generative AI internally is part of a larger wave of disruption for software engineers at work, as new tools, like Anysphere’s Cursor, change how the job is done—as well as the expected workload.\n\nIf these kinds of AI-focused workflows prove to be hyperefficient, then what it means to be an engineer will fundamentally change. “We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs,” said Amazon CEO Andy Jassy in a memo this week to employees. “It’s hard to know exactly where this nets out over time, but in the next few years, we expect that this will reduce our total corporate workforce as we get efficiency gains from using AI extensively across the company.”\n\nFor now, Rausch is mainly focused on rolling out the generative AI version of Alexa to more of Amazon users. “We really didn't want to leave customers behind in any way,” he says. “And that means hundreds of millions of different devices that you have to support.”\n\nThe new Alexa+ chats in a more conversational manner with users. It’s a more personalized experience that remembers your preferences and is able to complete online tasks that you give it, like searching for concert tickets or buying groceries.\n\nAmazon announced Alexa+ at a company event in February, and rolled out early access to a few public users in March, though this was without the complete slate of announced features. Now, the company claims that over a million people have access to the updated voice assistant, which is still a small percentage of prospective users; eventually, hundreds of millions of Alexa users will gain access to the AI tool. A wider release of Alexa+ is potentially slated later this summer.\n\nAmazon faces competition from multiple directions as it works on a more dynamic voice assistant. OpenAI’s Advanced Voice Mode, launched in 2024, was popular with users who found the AI voice engaging. Also, Apple announced an overhaul of its native voice assistant, Siri, at last year’s developer conference—with many contextual and personalization features similar to what Amazon is working on with Alexa+. Apple has yet to launch the rebuilt Siri, even in early access, and the new voice assistant is expected sometime next year.\n\nAmazon declined to give WIRED early access to Alexa+ for hands-on (voice-on?) testing, and the new assistant has not yet been rolled out to my personal Amazon account. Similar to how we approached OpenAI’s Advanced Voice Mode that launched in last year, WIRED plans to test Alexa+ and provide experiential context for readers as it becomes more widely available."
    },
    {
        "title": "Meta inks a new geothermal energy deal to support AI",
        "description": "Meta’s geothermal ambitions are growing with a new agreement to support projects in New Mexico.  It announced the deal with XGS Energy today, a company developing next-generation geothermal technologies that make it possible to use this energy source in place…",
        "url": "https://www.theverge.com/news/686387/meta-ai-data-center-geothermal-energy-xgs",
        "source": "The Verge",
        "publishedAt": "2025-06-12T19:49:39Z",
        "full_text": "is a senior science reporter covering energy and the environment with more than a decade of experience. She is also the host of Hell or High Water: When Disaster Hits Home , a podcast from Vox Media and Audible Originals.\n\nMeta’s geothermal ambitions are growing with a new agreement to support projects in New Mexico.\n\nIt announced the deal with XGS Energy today, a company developing next-generation geothermal technologies that make it possible to use this energy source in places where it was unfeasible before. The plan is to add 150 megawatts of carbon pollution-free electricity to the power grid that Meta’s data center plugs into in the region.\n\nMeta desperately needs more electricity for AI. Power grids are trying to meet that growing demand from data centers. And those needs are buoying geothermal startups like XGS Energy.\n\nThose needs are buoying geothermal startups like XGS Energy\n\n“Advances in AI require continued energy to support infrastructure development,” Urvi Parekh, global head of energy at Meta, said in a press release. “With next-generation geothermal technologies like XGS ready for scale, geothermal can be a major player in supporting the advancement of technologies like AI as well as domestic data center development.”\n\nGeothermal plants generate electricity using Earth’s heat; typically drawing up hot fluids or steam from natural reservoirs to turn turbines. That tactic is limited by natural geography, however, and the US gets around half a percent of its electricity from geothermal sources.\n\nStartups including XGS are trying to change that by making geothermal energy more accessible. Last year, Meta made a separate 150MW deal with Sage Geosystems to develop new geothermal power plants. Sage is developing technologies to harness energy from hot, dry rock formations by drilling and pumping water underground, essentially creating artificial reservoirs. Google has its own partnership with another startup called Fervo developing similar technology.\n\nXGS Energy is also seeking to exploit geothermal energy from dry rock resources. It tries to set itself apart by reusing water in a closed-loop process designed to prevent water from escaping into cracks in the rock. The water it uses to take advantage of underground heat circulates inside a steel casing. Conserving water is especially crucial in a drought-prone state like New Mexico, where Meta is expanding its Los Lunas data center.\n\nMeta declined to say how much it’s spending on this deal with XGS Energy. The initiative will roll out in two phases with a goal of being operational by 2030."
    },
    {
        "title": "Taboola’s clickbaity chumbox is evolving into an AI chatbot",
        "description": "Taboola, the company best known for serving up those weird, clickbaity advertisements you often find beneath online articles, has created an AI chatbot. The tool, called “DeeperDive,” is launching in beta on USA Today and The Independent, and will answer read…",
        "url": "https://www.theverge.com/news/685538/taboola-clickbait-chumbox-ai-chatbot",
        "source": "The Verge",
        "publishedAt": "2025-06-11T16:58:16Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nTaboola, the company best known for serving up those weird, clickbaity advertisements you often find beneath online articles, has created an AI chatbot. The tool, called “DeeperDive,” is launching in beta on USA Today and The Independent, and will answer readers’ questions using information “sourced from trusted journalists.”\n\nA demo on Taboola’s site shows a DeeperDive search bar at the top of USA Today’s homepage, which automatically surfaces prompts like “How is the UK government addressing the cost of living crisis in 2025?” and “What are the environmental implications of the latest oil drilling projects in the North Sea?”\n\nDeeperDive’s AI-generated response includes cited articles — and an ad. GIF: Taboola\n\nAlong with providing an AI-generated response, the chatbot also lists related USA Today articles, followed by a sponsored link. In its announcement, Taboola says DeepDive allows publishers to insert “contextually relevant, high-intent ads directly into the AI-powered results page.” Readers can also highlight a portion of an article and ask questions about it with DeepDive.\n\nThe AI bot will only appear for one percent of USA Today’s audience while it gets assessed for quality. The Verge reached out to Taboola for more information about how many readers of The Independent will see DeepDive, but didn’t immediately hear back.\n\nRelated The chum king behind those AI articles\n\nThis isn’t the first time USA Today’s publisher, Gannett, has used AI. In 2023, the company paused its AI-generated sports coverage following backlash from reporters and readers. It also added what appeared to be AI-generated articles to its Reviewed site, which it later shut down (but has since been brought back under a new owner). USA Today also began testing AI-generated summaries that appear at the top of articles last year."
    },
    {
        "title": "Max will show autoplaying video previews picked by AI",
        "description": "Max — soon to be called HBO Max again — is rolling out an update that automatically plays video previews when you hover over shows and movies. The video previews are launching to users in the US now, but Max is still working on an “AI-assisted” tool to pick t…",
        "url": "https://www.theverge.com/news/688313/max-video-previews-autoplay-update",
        "source": "The Verge",
        "publishedAt": "2025-06-17T16:18:02Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nMax — soon to be called HBO Max again — is rolling out an update that automatically plays video previews when you hover over shows and movies. The video previews are launching to users in the US now, but Max is still working on an “AI-assisted” tool to pick the clips that go in them.\n\nIn the “coming weeks,” Max says its AI tool will automatically identify “standout” scenes from its shows and movies, which its team will then use to choose the clips featured in video previews. “This AI-driven technology will enable us to identify Drop-in Moments for the vast catalog of content on Max and significantly reduce the manual time needed to curate previews,” Avi Saxena, Warner Bros. Discovery’s chief technology officer, said in the press release.\n\nThe previews will appear when you hover over content. GIF: Warner Bros. Discovery\n\nThe streaming service will also include an option to turn off the audio or video in the autoplaying content, similar to Netflix.\n\nMax updated its interface in January to move the options from its top menu bar to the left side of the screen — basically the opposite of what Netflix is doing in its upcoming homepage redesign. Max will have another big change sometime this summer, as that’s when the switch back to HBO Max will be official."
    },
    {
        "title": "YouTube search gets its own version of Google’s AI Overviews",
        "description": "Google is bringing an AI Overviews-like feature to another product: YouTube. The video streaming platform may now show an “AI-powered search results carousel” when some YouTube Premium members in the US make searches related to “shopping, travel, or things to…",
        "url": "https://www.theverge.com/news/693658/youtube-search-ai-overviews-google",
        "source": "The Verge",
        "publishedAt": "2025-06-26T16:22:57Z",
        "full_text": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nGoogle is bringing an AI Overviews-like feature to another product: YouTube. The video streaming platform may now show an “AI-powered search results carousel” when some YouTube Premium members in the US make searches related to “shopping, travel, or things to do in a specific place,” according to a blog post.\n\nAs shown in a video, the search results carousel will show a big video clip up top, thumbnails to a selection of other relevant video clips directly under that, and an AI-generated bit of text responding to your query. To see a full video, tap on the big clip at the top of the carousel.\n\nThe feature is currently only accessible on iOS and Android and for videos in English and will be available to test until July 30th, per the YouTube experiments page. Additionally, only a “randomly selected number of Premium members” will have access to it, YouTube says in a support document.\n\nYouTube is also broadening its conversational AI tool that lets you ask questions about a video to “some non-Premium users in the US.” The feature was initially introduced in 2023."
    },
    {
        "title": "The photographer using AI to reconstruct stories lost to censorship",
        "description": "Video screens glow softly from the floor, looping footage of salt lakes, steppe villages, and decaying nuclear test sites. Suspended above them is a large handwoven textile map, crafted by artisans in Kazakhstan. The tapestry maps 12 significant sites across …",
        "url": "https://www.theverge.com/ai-artificial-intelligence/688648/almagul-menlibayeva-ai-reimagining-censored-government-work",
        "source": "The Verge",
        "publishedAt": "2025-06-25T16:10:28Z",
        "full_text": "Video screens glow softly from the floor, looping footage of salt lakes, steppe villages, and decaying nuclear test sites. Suspended above them is a large handwoven textile map, crafted by artisans in Kazakhstan. The tapestry maps 12 significant sites across Kazakhstan and the surrounding region, each corresponding to one of the flickering videos below. This is Posthuman Matter: The Map of Nomadizing Reimaginings #3, the latest large-scale installation by photographer and multimedia artist Almagul Menlibayeva.\n\nRecently unveiled at the VRHAM! Digital & Immersive Art Biennale in Hamburg, Germany, the work is part of Menlibayeva’s ongoing series of “cyber textiles,” which offer a striking blend of craft and code. It imagines an alternative cartography of Central Asia, with each video in the installation infusing the locations with erased histories and traditions, putting forth an alternative future for them. While the tapestries are created by hand, the videos are a mixture of real and replicated, built from documentary footage captured by Menlibayeva and then augmented with AI to infuse feminist rituals, nomadic storytelling traditions, and whispers of endangered languages.\n\nMenlibayeva’s approach to artificial intelligence isn’t rooted in fascination with high-tech innovation for its own sake. Rather, it’s part of a deeper reckoning — with history, with loss, and with the systems that shape how stories are remembered or erased. She engages with AI not as a neutral tool, but as a terrain of power, ideology, and potential transformation. “Perhaps my interest in artificial intelligence is rooted in the traumatic history of Kazakh nomads,” she says, recalling how Soviet-era collectivization dismantled her ancestors’ way of life under the guise of technological progress.\n\nBorn in Kazakhstan and educated in the Soviet art system, Menlibayeva’s early training in folk textiles and Russian futurism is evident in her layered, hybrid works, which centered on photography and multichannel video installations for many years. Since 2022, she has expanded her practice to include AI, marking a pivotal evolution in her decades-long engagement with themes of historical erasure, cultural survival, and ecological trauma. Across these mediums, Menlibayeva critiques the lingering impacts of Soviet rule in Central Asia — from ecological degradation to cultural erasure — while reviving Indigenous and nomadic histories long overwritten by empire. With AI, she’s found a way to confront and reanimate these stories.\n\nAI Realism: Qantar 2022 was Menlibayeva’s first project to incorporate AI. It’s a visceral example of how she uses AI to build counternarratives. Created in response to the Bloody January protests in Kazakhstan — mass demonstrations that were violently suppressed by the state and subsequently censored in national media — the project constructs a synthetic memoryscape from collective trauma. During the protests, the Kazakh government imposed a near-total internet blackout, plunging the nation into an information vacuum.\n\nFaced with this blockade, Menlibayeva began collecting protest-2related stories from friends and social media, extracting key phrases in Kazakh and Russian, as well as voice messages sent via landlines and mobile networks. These fragments of real speech became the raw material for AI Realism: Qantar 2022. “The situation itself pushed me, because when these political events happened, the internet was shut down in the whole country,” she recalls. “I used audio recordings of voice messages, words these people used, to generate images of this work.”\n\nWorking with text-to-image and voice-to-image models via Google Colab, Menlibayeva assembled a series of AI-generated images from those crowdsourced stories. The resulting artwork, a 24-minute video and a series of haunting stills, is nonlinear and emotionally charged, confronting the erasure — both state-sanctioned and otherwise — of the events from memory. “I knew that the conditions, the events, would be forgotten or deliberately erased,” she says. “In this work, the people’s words are the main material. That is why the project is called AI Realism.”\n\nThe image Search and Seizure. History of Kairat Sultanbek. Kazakh January (2022), which is part of this series, reveals a chaos of bloodied surfaces and fragmented bodies. But it resists straightforward interpretation: there is no clear sequence of events and no clear heroes. “AI machines have a large limit, but sometimes system errors give rise to interesting results,” Menlibayeva says. In AI Realism: Qantar 2022, those glitches evoke the ruptures in history itself: the erasures, silences, and distortions enforced by both state violence and data-driven platforms.\n\nMenlibayeva’s process often starts analog, with her own photographs or video stills — or even embroidered motifs passed down from older generations. These materials are transformed using Stable Diffusion, Midjourney, and Perplexity. For video-related work, tools including Deforum, Runway, and Kaiber AI are used, but not without friction. “My first stage is to find the right prompt. Then I choose the most suitable platform based on how well it performs for that specific idea. Each platform has its own strengths, limitations, and biases, so I adapt my approach accordingly,” she says.\n\nWhile some celebrate AI’s democratizing potential, Menlibayeva remains wary. “AI is a complex tool with both democratizing potential and the risk of reinforcing new hierarchies,” she warns, noting that “AI systems are often controlled by large corporations, which influences access and power.”\n\nSo, why use them at all? Menlibayeva doesn’t believe AI creates anything truly new, only what data makes possible. But by inserting her own images, myths, and archives, she sees it as opening a dialogue between algorithmic systems and human history. “AI acts both as a tool and a distorted mirror, reflecting the hidden codes, preferences, and limitations of its creators: data, culture, and power,” she says. “I consciously engage with these biases, embedding my personal mythologies into the process.”\n\nTo Menlibayeva, “humanizing AI” doesn’t mean teaching machines to mimic empathy. Instead, it means embedding human stories, memories, and resistance into their logic. In her art, AI becomes a way to recover what state archives, history books, and dominant media refuse to hold. “That is why, as an artist, I try not to obey this logic, but to transform it. Humanizing AI is not the task of programmers, it is the task of artists,” she says."
    },
    {
        "title": "Senate drops plan to ban state AI laws",
        "description": "The US Senate has voted overwhelmingly to remove a moratorium on states regulating AI systems from the Republican “big, beautiful bill.” Legislators agreed by a margin of 99 to 1 to drop the controversial proposal during a protracted fight over the omnibus bu…",
        "url": "https://www.theverge.com/politics/695495/senate-drops-ai-moratorium-budget-bill",
        "source": "The Verge",
        "publishedAt": "2025-07-01T13:15:48Z",
        "full_text": "is a senior reporter for The Verge, covering the Trump administration, Elon Musk’s takeover of the federal government, and the tech industry’s embrace of the MAGA movement.\n\nThe US Senate has voted overwhelmingly to remove a moratorium on states regulating AI systems from the Republican “big, beautiful bill.” Legislators agreed by a margin of 99 to 1 to drop the controversial proposal during a protracted fight over the omnibus budget bill, which is still under debate.\n\nThe vote followed failed attempts to revise the rule in a way that would placate holdouts, particularly Sen. Marsha Blackburn (R-TN), one of the moratorium’s first opponents. Over the weekend, Blackburn struck a deal with Sen. Ted Cruz (R-TX) that would have cut the moratorium to five years and allowed states to continue enforcing AI laws that handled online child safety as well as individuals’ names, images, and likenesses. But after a day of furious backlash from the populist right, driven primarily by MAGA internet powerhouses Steve Bannon and Mike Davis, Blackburn relented at the last minute — and chose, instead, to attach her name to a Democrat-sponsored amendment that sought to remove the bill altogether.\n\n“While I appreciate Chairman Cruz’s efforts to find acceptable language that allows states to protect their citizens from the abuses of AI, the current language is not acceptable to those who need those provisions the most,” she said in a statement on Monday night. “This provision could allow Big Tech to continue to exploit kids, creators, and conservatives.”\n\nEarly fellow GOP defectors included Sen. Susan Collins (R-ME); Sen. Josh Hawley (R-MO), an anti-tech hawk; and Sen. Rand Paul (R-KY), who raised concerns about federal overreach. But ultimately, nearly everyone agreed on removing the AI provisions — the lone vote against it was from Sen. Thom Tillis (R-NC). The Senate must still vote on the budget reconciliation vote, after which it will return to the House before being passed to President Donald Trump’s desk.\n\nThe House of Representatives quietly lodged the first draft of the moratorium in its version of Trump’s funding megabill, passing it almost entirely along party lines by a vote of 215-214 in May. The stated goal was to avoid a patchwork of state AI regulations that could inhibit industry growth. But the plan was contentious even before the Senate began formal debate on its version, which required states to avoid regulating AI and “automated decision systems” if they wished to receive funding for broadband programs. It became a flash point in an already heated fight over the bill, resulting in furious backroom negotiations, an apparent deal, and then a daylong concerted effort to tank the bill.\n\nSenate Republicans had already fractured over several amendments inside the bill, but the addition of the AI moratorium turned the whip count into a trainwreck of competing interests — particularly within the Republican faction normally opposed to Big Tech and federal overreach. In a letter sent to Senate Majority Leader John Thune (R-SD) last week, several GOP senators, including Hawley and Paul, joined Blackburn in voicing their opposition to the bill for varying reasons, including their concern that it would automatically curtail preexisting state AI laws. (Tennessee, for instance, passed a law in 2024 that protected individuals’ likenesses from being used by generative AI.)\n\nOn the other hand, Cruz, the chairman of the Senate Commerce, Science, and Transportation Committee and widely considered as a hard-right figure, authored an amendment that would have specifically barred states with AI laws from accessing federal funds earmarked for AI development.\n\nThe moratorium has proven especially unpopular with state-level GOP figures: last week, 37 state attorneys general and 17 governors bombarded Thune with letters urging him to drop the clause. Gov. Sarah Huckabee Sanders of Arkansas, Trump’s former White House press secretary, went so far as to author a Washington Post op-ed denouncing the bill as removing states’ abilities to protect their own citizens. Other critics contended that the bill’s definition of “AI” is broad enough to ban entire swathes of software- and internet-related regulations, including Republican-backed state-level online child safety laws."
    },
    {
        "title": "Racist videos made with AI are going viral on TikTok",
        "description": "Racist videos that appear to be created with Google’s AI video generation tool Veo 3 have raked in millions of views across TikTok, according to findings from the nonprofit media watchdog Media Matters. The AI-generated videos uncovered by the organization ar…",
        "url": "https://www.theverge.com/news/697188/racist-ai-generated-videos-google-veo-3-tiktok",
        "source": "The Verge",
        "publishedAt": "2025-07-02T22:05:47Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nRacist videos that appear to be created with Google’s AI video generation tool Veo 3 have raked in millions of views across TikTok, according to findings from the nonprofit media watchdog Media Matters. The AI-generated videos uncovered by the organization are filled with racist tropes, many of which target Black people.\n\nMedia Matters determined that the videos — one of which received 14.2 million views — were generated with Google Veo 3 because of the “Veo” watermark in the corner of the clips. Some users also listed hashtags, captions, or usernames pertaining to Veo 3 or AI, according to Media Matters. Each clip found by the organization lasted just eight seconds, or was “composed of multiple clips each lasting no more than 8 seconds,” which tracks with Veo 3’s eight-second limit.\n\nVeo 3, launched by Google in May, allows users to generate AI video clips and audio with just a text prompt. On its website, Google says it will “block harmful requests and results.” TikTok’s rules similarly state that “hate speech and hateful behavior has no place on TikTok,” and that the platform “will not recommend content that contains negative stereotypes about a person or group with a protected attribute.”\n\n“We proactively enforce robust rules against hateful speech and behavior and have removed the accounts we identified in the report, many of which were already banned prior to the report publishing,” TikTok spokesperson Ariane de Selliers said in a statement to The Verge.\n\nThe Verge found some of the videos highlighted by Media Matters were also on YouTube but with fewer views, while Wired also discovered similar, racist AI-generated videos on Instagram. Other videos found by Media Matters contain antisemitic content and racist depictions of immigrants and Asian people.\n\nGoogle didn’t immediately respond to The Verge’s request for comment.\n\nUpdate, July 2nd: Added a statement from TikTok."
    },
    {
        "title": "Cloudflare will now block AI crawlers by default",
        "description": "The major internet architecture provider Cloudflare will now block known AI web crawlers by default to prevent them from “accessing content without permission or compensation,” according to an announcement on Tuesday. With the change, Cloudflare will start as…",
        "url": "https://www.theverge.com/news/695501/cloudflare-block-ai-crawlers-default",
        "source": "The Verge",
        "publishedAt": "2025-06-30T22:56:25Z",
        "full_text": "The major internet architecture provider Cloudflare will now block known AI web crawlers by default to prevent them from “accessing content without permission or compensation,” according to an announcement on Tuesday. With the change, Cloudflare will start asking new domain owners whether they want to allow AI scrapers, and will even let some publishers implement a “Pay Per Crawl” fee.\n\nThe Pay Per Crawl program will let publishers set a price for AI scrapers to access their content. AI companies can then view pricing and choose whether to register for the “Pay Per Crawl” fee or turn away. This is only available for “a group of some of the leading publishers and content creators” for now, but Cloudflare says it will ensure “AI companies can use quality content the right way — with permission and compensation.”\n\nCloudflare has been helping domain owners fight AI crawlers for a while now. The company started letting websites block AI crawlers in 2023, but it only applied to ones that abide by a site’s robots.txt file, the unenforceable agreement that signals whether bots can scrape its content. Cloudflare began allowing websites to block “all” AI bots last year — whether they respect a site’s robots.txt file or not — and now this setting is enabled by default for new Cloudflare customers. (The company identifies scrapers to block by comparing them to its list of known AI bots.) Cloudflare also rolled out a feature in March that sends web-crawling bots into an “AI Labyrinth” to deter them from scraping sites without permission.\n\nSeveral major publishers and online platforms, including The Associated Press, The Atlantic, Fortune, Stack Overflow, and Quora, are on board with Cloudflare’s new AI crawler restrictions, as websites contend with a future where more people are finding information through AI chatbots, rather than search engines. “People trust the AI more over the last six months, which means they’re not reading original content,” Cloudflare CEO Matthew Prince said during the Axios Live event last week.\n\nAdditionally, Cloudflare says it’s working with AI companies to help verify their crawlers and allow them to “clearly state their purpose,” such as whether they’re using the content for training, inference, or search. Website owners can then review this information and determine which crawlers to let in.\n\n“Original content is what makes the Internet one of the greatest inventions in the last century, and we have to come together to protect it,” Prince said in the press release. “AI crawlers have been scraping content without limits. Our goal is to put the power back in the hands of creators, while still helping AI companies innovate.”"
    },
    {
        "title": "Google tests real-time AI voice chats in Search",
        "description": "Google is building Search Live into AI Mode, allowing you to have a back-and-forth voice conversation with the company’s AI chatbot right from its search engine. The test, which is rolling out now to Labs users in the US, currently doesn’t support camera-shar…",
        "url": "https://www.theverge.com/news/689212/google-search-live-ai-mode-test",
        "source": "The Verge",
        "publishedAt": "2025-06-18T15:48:03Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nGoogle is building Search Live into AI Mode, allowing you to have a back-and-forth voice conversation with the company’s AI chatbot right from its search engine. The test, which is rolling out now to Labs users in the US, currently doesn’t support camera-sharing, but Google plans to add the capability in the “coming months.”\n\nSearch Live lets you interact with the company’s “custom” version of Gemini and search the web in real time. The feature currently lives in the Google app on Android and iOS, where it takes things a step further by giving you the ability to point your camera at something and ask questions about it out loud.\n\nSearch Live will show you links during your conversation. GIF: Google\n\nYou can try out Search Live in AI Mode by opting into the AI Mode experiment in Google Labs. From there, head to the Google app on Android or iOS and tap the new “Live” icon. You can then ask a question out loud, like “What are some tips for preventing a linen dress from wrinkling in a suitcase?” The AI chatbot will provide an audio response that you can reply to with another related question, like “What should I do if it still wrinkles?” Search Live will also present you with relevant links as you chat.\n\nOther AI companies have spent time building out voice modes for their AI models as well, with OpenAI launching an Advanced Voice Mode in ChatGPT last year and Anthropic rolling out a voice mode for its Claude app in May. Apple is also working on an “LLM Siri,” but its rollout was delayed because the company wasn’t able to “achieve the reliability” that it wanted, according to Apple software SVP Craig Federighi.\n\nGoogle says Search Live operates in the background, letting you continue your conversation with the chatbot when navigating to other apps. You can also tap the “transcript” button to see a text version of the response and reply by typing. Google will save past Search Live conversations in your AI Mode history."
    },
    {
        "title": "California is trying to regulate its AI giants — again",
        "description": "Last September, all eyes were on Senate Bill 1047 as it made its way to California Governor Gavin Newsom's desk - and died there as he vetoed the buzzy piece of legislation. SB 1047 would have required makers of all large AI models, particularly those that co…",
        "url": "https://www.theverge.com/ai-artificial-intelligence/688301/california-is-trying-to-regulate-its-ai-giants-again",
        "source": "The Verge",
        "publishedAt": "2025-06-17T16:10:41Z",
        "full_text": "is The Verge’s senior AI reporter. An AI beat reporter for more than five years, her work has also appeared in CNBC, MIT Technology Review, Wired UK, and other outlets.\n\nLast September, all eyes were on Senate Bill 1047 as it made its way to California Governor Gavin Newsom’s desk — and died there as he vetoed the buzzy piece of legislation.\n\nSB 1047 would have required makers of all large AI models, particularly those that cost $100 million or more to train, to test them for specific dangers. AI industry whistleblowers weren’t happy about the veto, and most large tech companies were. But the story didn’t end there. Newsom, who had felt the legislation was too stringent and one-size-fits-all, tasked a group of leading AI researchers to help propose an alternative plan — one that would support the development and the governance of generative AI in California, along with guardrails for its risks.\n\nOn Tuesday, that report was published.\n\nThe authors of the 52-page “California Report on Frontier Policy” said that AI capabilities — including models’ chain-of-thought “reasoning” abilities — have “rapidly improved” since Newsom’s decision to veto SB 1047. Using historical case studies, empirical research, modeling, and simulations, they suggested a new framework that would require more transparency and independent scrutiny of AI models. Their report is appearing against the backdrop of a possible 10-year moratorium on states regulating AI, backed by a Republican Congress and companies like OpenAI.\n\nThe report — co-led by Fei-Fei Li, Co-Director of the Stanford Institute for Human-Centered Artificial Intelligence; Mariano-Florentino Cuéllar, President of the Carnegie Endowment for International Peace; and Jennifer Tour Chayes, Dean of the UC Berkeley College of Computing, Data Science, and Society — concluded that frontier AI breakthroughs in California could heavily impact agriculture, biotechnology, clean tech, education, finance, medicine and transportation. Its authors agreed it’s important to not stifle innovation and “ensure regulatory burdens are such that organizations have the resources to comply.”\n\n“Without proper safeguards… powerful Al could induce severe and, in some cases, potentially irreversible harms”\n\nBut reducing risks is still paramount, they wrote: “Without proper safeguards… powerful Al could induce severe and, in some cases, potentially irreversible harms.”\n\nThe group published a draft version of their report in March for public comment. But even since then, they wrote in the final version, evidence that these models contribute to “chemical, biological, radiological, and nuclear (CBRN) weapons risks… has grown.” Leading companies, they added, have self-reported concerning spikes in their models’ capabilities in those areas.\n\nThe authors have made several changes to the draft report. They now note that California’s new AI policy will need to navigate quickly-changing “geopolitical realities.” They added more context about the risks that large AI models pose, and they took a harder line on categorizing companies for regulation, saying a focus purely on how much compute their training required was not the best approach.\n\nAI’s training needs are changing all the time, the authors wrote, and a compute-based definition ignores how these models are adopted in real-world use cases. It can be used as an “initial filter to cheaply screen for entities that may warrant greater scrutiny,” but factors like initial risk evaluations and downstream impact assessment are key.\n\nThat’s especially important because the AI industry is still the Wild West when it comes to transparency, with little agreement on best practices and “systemic opacity in key areas” like how data is acquired, safety and security processes, pre-release testing, and potential downstream impact, the authors wrote.\n\nThe report calls for whistleblower protections, third-party evaluations with safe harbor for researchers conducting those evaluations, and sharing information directly with the public, to enable transparency that goes beyond what current leading AI companies choose to disclose.\n\nOne of the report’s lead writers, Scott Singer, told The Verge that AI policy conversations have “completely shifted on the federal level” since the draft report. He argued that California, however, could help lead a “harmonization effort” among states for “commonsense policies that many people across the country support.” That’s a contrast to the jumbled patchwork that AI moratorium supporters claim state laws will create.\n\nIn an op-ed earlier this month, Anthropic CEO Dario Amodei called for a federal transparency standard, requiring leading AI companies “to publicly disclose on their company websites … how they plan to test for and mitigate national security and other catastrophic risks.”\n\n“Developers alone are simply inadequate at fully understanding the technology and, especially, its risks and harms”\n\nBut even steps like that aren’t enough, the authors of Tuesday’s report wrote, because “for a nascent and complex technology being developed and adopted at a remarkably swift pace, developers alone are simply inadequate at fully understanding the technology and, especially, its risks and harms.”\n\nThat’s why one of the key tenets of Tuesday’s report is the need for third-party risk assessment.\n\nThe authors concluded that risk assessments would incentivize companies like OpenAI, Anthropic, Google, Microsoft and others to amp up model safety, while helping paint a clearer picture of their models’ risks. Currently, leading AI companies typically do their own evaluations or hire second-party contractors to do so. But third-party evaluation is vital, the authors say.\n\nNot only are “thousands of individuals… willing to engage in risk evaluation, dwarfing the scale of internal or contracted teams,” but also, groups of third-party evaluators have “unmatched diversity, especially when developers primarily reflect certain demographics and geographies that are often very different from those most adversely impacted by AI.”\n\nBut if you’re allowing third-party evaluators to test the risks and blind spots of your powerful AI models, you have to give them access — for meaningful assessments, a lot of access. And that’s something companies are hesitant to do.\n\nIt’s not even easy for second-party evaluators to get that level of access. Metr, a company OpenAI partners with for safety tests of its own models, wrote in a blog post that the firm wasn’t given as much time to test OpenAI’s o3 model as it had been with past models, and that OpenAI didn’t give it enough access to data or the models’ internal reasoning. Those limitations, Metr wrote, “prevent us from making robust capability assessments.” OpenAI later said it was exploring ways to share more data with firms like Metr.\n\nEven an API or disclosures of a model’s weights may not let third-party evaluators effectively test for risks, the report noted, and companies could use “suppressive” terms of service to ban or threaten legal action against independent researchers that uncover safety flaws.\n\nLast March, more than 350 AI industry researchers and others signed an open letter calling for a “safe harbor” for independent AI safety testing, similar to existing protections for third-party cybersecurity testers in other fields. Tuesday’s report cites that letter and calls for big changes, as well as reporting options for people harmed by AI systems.\n\n“Even perfectly designed safety policies cannot prevent 100% of substantial, adverse outcomes,” the authors wrote. “As foundation models are widely adopted, understanding harms that arise in practice is increasingly important.”"
    },
    {
        "title": "Did AI companies win a fight with authors? Technically",
        "description": "In the past week, big AI companies have - in theory - chalked up two big legal wins. But things are not quite as straightforward as they may seem, and copyright law hasn't been this exciting since last month's showdown at the Library of Congress. First, Judge…",
        "url": "https://www.theverge.com/analysis/694657/ai-copyright-rulings-anthropic-meta",
        "source": "The Verge",
        "publishedAt": "2025-06-27T23:12:24Z",
        "full_text": "In the past week, big AI companies have — in theory — chalked up two big legal wins. But things are not quite as straightforward as they may seem, and copyright law hasn’t been this exciting since last month’s showdown at the Library of Congress.\n\nFirst, Judge William Alsup ruled it was fair use for Anthropic to train on a series of authors’ books. Then, Judge Vince Chhabria dismissed another group of authors’ complaint against Meta for training on their books. Yet far from settling the legal conundrums around modern AI, these rulings might have just made things even more complicated.\n\nBoth cases are indeed qualified victories for Meta and Anthropic. And at least one judge — Alsup — seems sympathetic to some of the AI industry’s core arguments about copyright. But that same ruling railed against the startup’s use of pirated media, leaving it potentially on the hook for massive financial damage. (Anthropic even admitted it did not initially purchase a copy of every book it used.) Meanwhile, the Meta ruling asserted that because a flood of AI content could crowd out human artists, the entire field of AI system training might be fundamentally at odds with fair use. And neither case addressed one of the biggest questions about generative AI: when does its output infringe copyright, and who’s on the hook if it does?\n\nAlsup and Chhabria (incidentally both in the Northern District of California) were ruling on relatively similar sets of facts. Meta and Anthropic both pirated huge collections of copyright-protected books to build a training dataset for their large language models Llama and Claude. Anthropic later did an about-face and started legally purchasing books, tearing the covers off to “destroy” the original copy, and scanning the text.\n\nThe authors argued that, in addition to the initial piracy, the training process constituted an unlawful and unauthorized use of their work. Meta and Anthropic countered that this database-building and LLM-training constituted fair use.\n\nBoth judges basically agreed that LLMs meet one central requirement for fair use: they transform the source material into something new. Alsup called using books to train Claude “exceedingly transformative,” and Chhabria concluded “there’s no disputing” the transformative value of Llama. Another big consideration for fair use is the new work’s impact on a market for the old one. Both judges also agreed that based on the arguments made by the authors, the impact wasn’t serious enough to tip the scale.\n\nAdd those things together, and the conclusions were obvious… but only in the context of these cases, and in Meta’s case, because the authors pushed a legal strategy that their judge found totally inept.\n\nPut it this way: when a judge says his ruling “does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful” and “stands only for the proposition that these plaintiffs made the wrong arguments and failed to develop a record in support of the right one” — as Chhabria did — AI companies’ prospects in future lawsuits with him don’t look great.\n\nBoth rulings dealt specifically with training — or media getting fed into the models — and didn’t reach the question of LLM output, or the stuff models produce in response to user prompts. But output is, in fact, extremely pertinent. A huge legal fight between The New York Times and OpenAI began partly with a claim that ChatGPT could verbatim regurgitate large sections of Times stories. Disney recently sued Midjourney on the premise that it “will generate, publicly display, and distribute videos featuring Disney’s and Universal’s copyrighted characters” with a newly launched video tool. Even in pending cases that weren’t output-focused, plaintiffs can adapt their strategies if they now think it’s a better bet.\n\nThe authors in the Anthropic case didn’t allege Claude was producing directly infringing output. The authors in the Meta case argued Llama was, but they failed to convince the judge — who found it wouldn’t spit out more than around 50 words of any given work. As Alsup noted, dealing purely with inputs changed the calculations dramatically. “If the outputs seen by users had been infringing, Authors would have a different case,” wrote Alsup. “And, if the outputs were ever to become infringing, Authors could bring such a case. But that is not this case.”\n\nIn their current form, major generative AI products are basically useless without output. And we don’t have a good picture of the law around it, especially because fair use is an idiosyncratic, case-by-case defense that can apply differently to mediums like music, visual art, and text. Anthropic being able to scan authors’ books tells us very little about whether Midjourney can legally help people produce Minions memes.\n\nMinions and New York Times articles are both examples of direct copying in output. But Chhabria’s ruling is particularly interesting because it makes the output question much, much broader. Though he may have ruled in favor of Meta, Chhabria’s entire opening argues that AI systems are so damaging to artists and writers that their harm outweighs any possible transformative value — basically, because they’re spam machines.\n\nIt’s worth reading:\n\nGenerative AI has the potential to flood the market with endless amounts of images, songs, articles, books, and more. People can prompt generative AI models to produce these outputs using a tiny fraction of the time and creativity that would otherwise be required. So by training generative AI models with copyrighted works, companies are creating something that often will dramatically undermine the market for those works, and thus dramatically undermine the incentive for human beings to create things the old-fashioned way. … As the Supreme Court has emphasized, the fair use inquiry is highly fact dependent, and there are few bright-line rules. There is certainly no rule that when your use of a protected work is “transformative,” this automatically inoculates you from a claim of copyright infringement. And here, copying the protected works, however transformative, involves the creation of a product with the ability to severely harm the market for the works being copied, and thus severely undermine the incentive for human beings to create. … The upshot is that in many circumstances it will be illegal to copy copyright-protected works to train generative AI models without permission. Which means that the companies, to avoid liability for copyright infringement, will generally need to pay copyright holders for the right to use their materials.\n\nAnd boy, it sure would be interesting if somebody would sue and make that case. After saying that “in the grand scheme of things, the consequences of this ruling are limited,” Chhabria helpfully noted this ruling affects only 13 authors, not the “countless others” whose work Meta used. A written court opinion is unfortunately incapable of physically conveying a wink and a nod.\n\nThose lawsuits might be far in the future. And Alsup, though he wasn’t faced with the kind of argument Chhabria suggested, seemed potentially unsympathetic to it. “Authors’ complaint is no different than it would be if they complained that training schoolchildren to write well would result in an explosion of competing works,” he wrote of the authors who sued Anthropic. “This is not the kind of competitive or creative displacement that concerns the Copyright Act. The Act seeks to advance original works of authorship, not to protect authors against competition.” He was similarly dismissive of the claim that authors were being deprived of licensing fees for training: “such a market,” he wrote, “is not one the Copyright Act entitles Authors to exploit.”\n\nBut even Alsup’s seemingly positive ruling has a poison pill for AI companies. Training on legally acquired material, he ruled, is classic protected fair use. Training on pirated material is a different story, and Alsup absolutely excoriates any attempt to say it’s not.\n\n“This order doubts that any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use,” he wrote. There were plenty of ways to scan or copy legally acquired books (including Anthropic’s own scanning system), but “Anthropic did not do those things — instead it stole the works for its central library by downloading them from pirated libraries.” Eventually switching to book scanning doesn’t erase the original sin, and in some ways it actually compounds it, because it demonstrates Anthropic could have done things legally from the start.\n\nRelated What Meta and Anthropic really won in court\n\nIf new AI companies adopt this perspective, they’ll have to build in extra but not necessarily ruinous startup costs. There’s the up-front price of buying what Anthropic at one point described as “all the books in the world,” plus any media needed for things like images or video. And in Anthropic’s case these were physical works, because hard copies of media dodge the kinds of DRM and licensing agreements publishers can put on digital ones — so add some extra cost for the labor of scanning them in.\n\nBut just about any big AI player currently operating is either known or suspected to have trained on illegally downloaded books and other media. Anthropic and the authors will be going to trial to hash out the direct piracy accusations, and depending on what happens, a lot of companies could be hypothetically at risk of almost inestimable financial damages — not just from authors, but from anyone that demonstrates their work was illegally acquired. As legal expert Blake Reid vividly puts it, “if there’s evidence that an engineer was torrenting a bunch of stuff with C-suite blessing it turns the company into a money piñata.”\n\nAnd on top of all that, the many unsettled details can make it easy to miss the bigger mystery: how this legal wrangling will affect both the AI industry and the arts.\n\nEchoing a common argument among AI proponents, former Meta executive Nick Clegg said recently that getting artists’ permission for training data would “basically kill the AI industry.” That’s an extreme claim, and given all the licensing deals companies are already striking (including with Vox Media, the parent company of The Verge), it’s looking increasingly dubious. Even if they’re faced with piracy penalties thanks to Alsup’s ruling, the biggest AI companies have billions of dollars in investment — they can weather a lot. But smaller, particularly open source players might be much more vulnerable, and many of them are also almost certainly trained on pirated works.\n\nMeanwhile, if Chhabria’s theory is right, artists could reap a reward for providing training data to AI giants. But it’s highly unlikely the fees would shut these services down. That would still leave us in a spam-filled landscape with no room for future artists.\n\nCan money in the pockets of this generation’s artists compensate for the blighting of the next? Is copyright law the right tool to protect the future? And what role should the courts be playing in all this? These two rulings handed partial wins to the AI industry, but they leave many more, much bigger questions unanswered."
    },
    {
        "title": "Google DeepMind’s optimized AI model runs directly on robots",
        "description": "Google DeepMind is rolling out an on-device version of its Gemini Robotics AI model that allows it to operate without an internet connection. The vision-language-action model (VLA) comes with dexterous capabilities similar to the one released in March, but Go…",
        "url": "https://www.theverge.com/news/691882/google-deepmind-on-device-ai-robots-gemini",
        "source": "The Verge",
        "publishedAt": "2025-06-24T13:47:28Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nGoogle DeepMind is rolling out an on-device version of its Gemini Robotics AI model that allows it to operate without an internet connection. The vision-language-action model (VLA) comes with dexterous capabilities similar to the one released in March, but Google says “it’s small and efficient enough to run directly on a robot.”\n\nThe flagship Gemini Robotics model is designed to help robots complete a wide range of physical tasks, even if it hasn’t been specifically trained on them. It allows robots to generalize new situations and understand and respond to commands, as well as perform tasks that require fine motor skills.\n\nCarolina Parada, head of robotics at Google DeepMind, tells The Verge that the original Gemini Robotics model uses a hybrid approach, allowing it to operate on-device and on the cloud. But with this device-only model, users can access offline features that are almost as good as those of the flagship.\n\nApptronik’s Apollo humanoid bot followed by Google’s ALOHA system. GIF: Google\n\nThe on-device model can perform several different tasks out of the box, and it can adapt to new situations “with as few as 50 to 100 demonstrations,” according to Parada. Google only trained the model on its ALOHA robot, but the company was able to adapt it to different robot types, such as the humanoid Apollo robot from Apptronik and the bi-arm Franka FR3 robot.\n\n“The Gemini Robotics hybrid model is still more powerful, but we’re actually quite surprised at how strong this on-device model is,” Parada says. “I would think about it as a starter model or as a model for applications that just have poor connectivity.” It could also be useful for companies with strict security requirements.\n\nAlongside this launch, Google is releasing a software development kit (SDK) for the on-device model that developers can use to evaluate and fine-tune it — a first for one of Google DeepMind’s VLAs.\n\nThe on-device Gemini Robotics model and its SDK will be available to a group of trusted testers while Google continues to work toward minimizing safety risks."
    },
    {
        "title": "Futureproof",
        "description": "AI tools are flooding the culture ecosystem — and no corner of the arts space is immune. In this series, we’re looking at the ways artists are embracing AI, pushing back on it, or trying their best to find an equilibrium with a new technology that’s both swee…",
        "url": "https://www.theverge.com/ai-artificial-intelligence/688592/futureproof-artists-and-ai",
        "source": "The Verge",
        "publishedAt": "2025-06-18T16:15:59Z",
        "full_text": "AI tools are flooding the culture ecosystem — and no corner of the arts space is immune. In this series, we’re looking at the ways artists are embracing AI, pushing back on it, or trying their best to find an equilibrium with a new technology that’s both sweeping and destabilizing. We talk to perfumers questioning the looming automation of scent creation, fanfic writers pushing back on archive scrapers, and illustrators replacing the AI that once replaced them. The tech isn’t going away. Here’s how artists are starting to deal with it."
    },
    {
        "title": "Microsoft Edge is getting an AI-powered browser history that works with typos",
        "description": "Microsoft has started testing a new feature for its Edge browser that adds an AI-powered search to browsing history. This new “enhanced search” feature lets you find websites in your browser history even if you use a phrase, synonym, or even a typo that’s sim…",
        "url": "https://www.theverge.com/news/684195/microsoft-edge-ai-browser-history-search-feature",
        "source": "The Verge",
        "publishedAt": "2025-06-10T12:54:37Z",
        "full_text": "is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years.\n\nMicrosoft has started testing a new feature for its Edge browser that adds an AI-powered search to browsing history. This new “enhanced search” feature lets you find websites in your browser history even if you use a phrase, synonym, or even a typo that’s similar to the site you actually want to find.\n\nThe AI-powered history search started appearing in beta versions of Microsoft Edge last week. “After this feature is turned on, sites you visit will be shown in enhanced history search results,” explains Microsoft in its beta channel release notes. “An on-device model is trained using your data, which never leaves your device and is never sent to Microsoft.”\n\nThis optional feature seems like less of a privacy risk than Microsoft’s Recall feature that screenshots most of what you do on a Copilot Plus PC to make it easier to search for websites, photos, and documents. The use of an on-device model that’s limited to your browser history is certainly better than having to store everything on your screen in a local database.\n\nAlongside the AI-powered browsing history, Microsoft is also adding a media control center to Edge that lets you control multiple media sources from websites. This media control center includes quick access to Edge’s picture-in-picture mode, which itself is getting better controls, and the ability to control music, video, or other sounds that are playing inside Edge."
    },
    {
        "title": "Chromebook Plus laptops like Lenovo’s sleek, new 14-incher are getting free Gemini AI features",
        "description": "Google announced a handful of new Gemini AI features for Chromebook Plus laptops, including a sleek, new 14-inch flagship device from Lenovo that I got to handle recently. The new AI functions include select to search, which allows you to long press the launc…",
        "url": "https://www.theverge.com/news/690885/google-chromeos-gemini-ai-select-to-search-lenovo-chromebook-plus-14",
        "source": "The Verge",
        "publishedAt": "2025-06-23T12:59:17Z",
        "full_text": "is a reviewer covering laptops and the occasional gadget. He spent over 15 years in the photography industry before joining The Verge as a deals writer in 2021.\n\nGoogle announced a handful of new Gemini AI features for Chromebook Plus laptops, including a sleek, new 14-inch flagship device from Lenovo that I got to handle recently.\n\nThe new AI functions include select to search, which allows you to long press the launcher icon or take a screenshot and do a Google search based on on-screen images or text. (It’s like circle to search on phones, but for Chromebooks.) Text captured this way can be imported into a calendar or Google Workspace apps like a spreadsheet or document. On-screen text that’s a bit technical or jargon-y can also be simplified. And now, the Quick Insert key — the “Gemini button” that replaces the Caps Lock key on Chromebooks — gets a shortcut to AI image generation.\n\nPrevious Next\n\n\n\n\n\n\n\n\n\n1 / 6 Select to search can pull up search and shopping links for things you highlight on your screen. Image: Google\n\nThere are two more new Gemini features: a smart grouping tool that automatically organizes your current tabs and documents based on what you’re working on, and some image editing built into the Gallery app for automated tasks like background removal and making stickers. These new functions use on-device AI and are exclusive to Lenovo’s new Chromebook Plus 14 laptop, which launches today alongside Google’s new tools.\n\nThe Lenovo Chromebook Plus (14-inch, 10th-gen) starts at $649 and uses a MediaTek Kompanio Ultra chip capable of 50 TOPS. It’s an eight-core Arm-based processor with Wi-Fi 7 and support for up to two external 4K monitors. Lenovo’s new flagship Chromebook also has a 14-inch OLED display capable of 1920 x 1200 resolution and 400 nits of brightness, with the option for a touchscreen version starting at $749. Its other key specs include up to 256GB of storage, up to 16GB of RAM, a fingerprint sensor, Bluetooth 5.4 support, and a four-speaker Dolby Atmos audio setup. For ports, it has just two 5Gbps USB-C, one 5Gbps USB-A, and a 3.5mm combo audio jack.\n\nPrevious Next\n\n\n\n\n\n\n\n\n\n1 / 6 Lenovo’s new Chromebook Plus 14. Photo: Antonio G. Di Benedetto / The Verge\n\nI got to briefly hold and see the Lenovo Chromebook Plus 14 at a recent Google preview event, and it was certainly a svelte and sleek machine. It weighs just 2.58 pounds, making it easy to lift up from a corner with just one hand. And, as usual for OLEDs, its screen had a deep and colorful contrast that was pleasant to look at. In addition to this being the first Arm-based model for Google’s Chromebook Plus range of laptops, the Lenovo Chromebook Plus 14’s 60Wh cell is claimed to have the best battery life among its peers.\n\nBuying the Lenovo Chromebook Plus 14 or any Chromebook Plus device in 2025 gets you free access to Google’s AI Pro Plan for one year, which includes Gemini 2.5 Pro, the Veo 3 AI video generator, and 2TB of cloud storage."
    },
    {
        "title": "Meta held talks to buy Thinking Machines, Perplexity, and Safe Superintelligence",
        "description": "At this point, it's becoming easier to say which AI startups Mark Zuckerberg hasn't looked at acquiring. In addition to Ilya Sutskever's Safe Superintelligence (SSI), sources tell me the Meta CEO recently discussed buying ex-OpenAI CTO Mira Murati's Thinking …",
        "url": "https://www.theverge.com/command-line-newsletter/690720/meta-buy-thinking-machines-perplexity-safe-superintelligence",
        "source": "The Verge",
        "publishedAt": "2025-06-20T22:53:47Z",
        "full_text": "At this point, it’s becoming easier to say which AI startups Mark Zuckerberg hasn’t looked at acquiring.\n\nIn addition to Ilya Sutskever’s Safe Superintelligence (SSI), sources tell me the Meta CEO recently discussed buying ex-OpenAI CTO Mira Murati’s Thinking Machines Lab and Perplexity, the AI-native Google rival. None of these talks progressed to the formal offer stage for various reasons, including disagreements over deal prices and strategy, but together they illustrate how aggressively Zuckerberg has been canvassing the industry to reboot his AI efforts.\n\nNow, details about the team Zuckerberg is assembling are starting to come into view: SSI co-founder and CEO Daniel Gross, along with ex-Github CEO Nat Friedman, are poised to co-lead the Meta AI assistant. Both men will report to Alexandr Wang, the former Scale CEO Zuckerberg just paid over $14 billion to quickly hire. Wang told his Scale team goodbye last Friday and was in the Meta office on Monday. This week, he has been meeting with top Meta leaders (more on that below) and continuing to recruit for the new AI team Zuckerberg has tasked him with building. I expect the team to be unveiled as soon as next week.\n\nRather than join Meta, Sutskever, Murati, and Perplexity CEO Aravind Srinivas have all gone on to raise more money at higher valuations. Sutskever, a titan of the AI research community who co-founded OpenAI, recently raised a couple of billion dollars for SSI. Both Meta and Google are investors in his company, I’m told. Murati also just raised a couple of billion dollars. Neither she nor Sutskever is close to releasing a product. Srinivas, meanwhile, is in the process of raising around $500 million for Perplexity.\n\nSpokespeople for all the companies involved either declined to comment or didn’t respond in time for publication. The Information and CNBC first reported Zuckerberg’s talks with Safe Superintelligence, while Bloomberg first reported the Perplexity talks.\n\nWhile Zuckerberg’s recruiting drive is motivated by the urgency he feels to fix Meta’s AI strategy, the situation also highlights the fierce competition for top AI talent these days. In my conversations this week, those on the inside of the industry aren’t surprised by Zuckerberg making nine-figure — or even, yes, 10-figure — compensation offers for the best AI talent. There are certain senior people at OpenAI, for example, who are already compensated in that ballpark, thanks to the company’s meteoric increase in valuation over the last few years.\n\nSpeaking of OpenAI, it’s clear that CEO Sam Altman is at least a bit rattled by Zuckerberg’s hiring spree. His decision to appear on his brother’s podcast this week and say that “none of our best people” are leaving for Meta was probably meant to convey a position of strength, but in reality, it looks like he is throwing his former colleagues under the bus. I was confused by Altman’s suggestion that Meta paying a lot upfront for talent won’t “set up a great culture.” After all, didn’t OpenAI just pay $6.5 billion to hire Jony Ive and his small hardware team?\n\nAlex Himel.\n\n“We think that glasses are the best form factor for AI”\n\nWhen I joined a Zoom call with Alex Himel, Meta’s VP of wearables, this week, he had just gotten off a call with Zuckerberg’s new AI chief, Alexandr Wang.\n\n“There’s an increasing number of Alexes that I talk to on a regular basis,” Himel joked as we started our conversation about Meta’s new glasses release with Oakley. “I was just in my first meeting with him. There were like three people in a room with the camera real far away, and I was like, ‘Who is talking right now?’ And then I was like, ‘Oh, hey, it’s Alex.’”\n\nThe following Q&A has been edited for length and clarity:\n\nHow did your meeting with Alex just now go?\n\nThe meeting was about how to make AI as awesome as it can be for glasses. Obviously, there are some unique use cases in the glasses that aren’t stuff you do on a phone. The thing we’re trying to figure out is how to balance it all, because AI can be everything to everyone or it could be amazing for more specific use cases.\n\nWe’re trying to figure out how to strike the right balance because there’s a ton of stuff in the underlying Llama models and that whole pipeline that we don’t care about on glasses. Then there’s stuff we really, really care about, like egocentric view and trying to feed video into the models to help with some of the really aspirational use cases that we wouldn’t build otherwise.\n\nYou are referring to this new lineup with Oakley as “AI glasses.” Is that the new branding for this category? They are AI glasses, not smart glasses?\n\nWe refer to the category as AI glasses. You saw Orion. You used it for longer than anyone else in the demo, which I commend you for. We used to think that’s what you needed to hit scale for this new category. You needed the big field of view and display to overlay virtual content. Our opinion of that has definitely changed. We think we can hit scale faster, and AI is the reason we think that’s possible.\n\nRight now, the top two use cases for the glasses are audio — phone calls, music, podcasts — and taking photos and videos. We look at participation rates of our active users, and those have been one and two since launch. Audio is one. A very close second is photos and videos.\n\nAI has been number three from the start. As we’ve been launching more markets — we’re now in 18 — and we’ve been adding more features, AI is creeping up. Our biggest investment by a mile on the software side is AI functionality, because we think that glasses are the best form factor for AI. They are something you’re already wearing all the time. They can see what you see. They can hear what you hear. They’re super accessible.\n\nIs your goal to have AI supersede audio and photo to be the most used feature for glasses, or is that not how you think about it?\n\nFrom a math standpoint, at best, you could tie. We do want AI to be something that’s increasingly used by more people more frequently. We think there’s definitely room for the audio to get better. There’s definitely room for image quality to get better. The AI stuff has much more headroom.\n\nHow much of the AI is onboard the glasses versus the cloud? I imagine you have lots of physical constraints with this kind of device.\n\nWe’ve now got one billion-parameter models that can run on the frame. So, increasingly, there’s stuff there. Then we have stuff running on the phone.\n\nIf you were watching WWDC, Apple made a couple of announcements that we haven’t had a chance to test yet, but we’re excited about. One is the Wi-Fi Aware APIs. We should be able to transfer photos and videos without having people tap that annoying dialogue box every time. That’d be great. The second one was processor background access, which should allow us to do image processing when you transfer the media over. Syncing would work just like it does on Android.\n\nDo you think the market for these new Oakley glasses will be as big as the Ray-Bans? Or is it more niche because they are more outdoors and athlete-focused?\n\nWe work with EssilorLuxottica, which is a great partner. Ray-Ban is their largest brand. Within that, the most popular style is Wayfair. When we launched the original Ray-Ban Meta glasses, we went with the most popular style for the most popular brand.\n\nTheir second biggest brand is Oakley. A lot of people wear them. The Holbrook is really popular. The HSTN, which is what we’re launching, is a really popular analog frame. We increasingly see people using the Ray-Ban Meta glasses for active use cases. This is our first step into the performance category. There’s more to come.\n\nWhat’s your reaction to Google’s announcements at I/O for their XR glasses platform and eyewear partnerships?\n\nWe’ve been working with EssilorLuxottica for like five years now. That’s a long time for a partnership. It takes a while to get really in sync. I feel very good about the state of our partnership. We’re able to work quickly. The Oakley Meta glasses are the fastest program we’ve had by quite a bit. It took less than nine months.\n\nI thought the demos they [Google] did were pretty good. I thought some of those were pretty compelling. They didn’t announce a product, so I can’t react specifically to what they’re doing. It’s flattering that people see the traction we’re getting and want to jump in as well.\n\nOn the AR glasses front, what have you been learning from Orion now that you’ve been showing it to the outside world?\n\nWe’ve been going full speed on that. We’ve actually hit some pretty good internal milestones for the next version of it, which is the one we plan to sell. The biggest learning from using them is that we feel increasingly good about the input and interaction model with eye tracking and the neural band. I wore mine during March Madness in the office. I was literally watching the games. Picture yourself sitting at a table with a virtual TV just above people’s heads. It was amazing.\n\nElsewhere\n\nTikTok gets to keep operating illegally. As expected, President Trump As expected, President Trump extended his enforcement deadline for the law that has banned a China-owned TikTok in the US. It’s essential to understand what is really happening here: Trump is instructing his Attorney General not to enforce earth-shattering fines on Apple, Google, and every other American company that helps operate TikTok. The idea that he wouldn’t use this immense leverage to extract whatever he wants from these companies is naive, and this whole process makes a mockery of everyone involved, not to mention the US legal system.\n\nAmazon will hire fewer people because of AI. When you make an employee memo Andy Jassy wants to make clear that he’s going to fully embrace AI to cut costs. Roughly 30 percent of Amazon’s code is already written by AI, and I’m sure Jassy is looking at human-intensive areas, such as sales and customer service, to further automate. When you make an employee memo a press release , you’re trying to tell the whole world what’s coming. In this case, Amazon CEOwants to make clear that he’s going to fully embrace AI to cut costs. Roughly 30 percent of Amazon’s code is already written by AI, and I’m sure Jassy is looking at human-intensive areas, such as sales and customer service, to further automate.\n\nLink list\n\nMore to click on:\n\nAs always, I welcome your feedback, especially if you’ve also turned down Zuck. You can respond here or ping me securely on Signal."
    },
    {
        "title": "Adobe made a mobile app for its Firefly generative AI tools",
        "description": "The all-in-one platform for Adobe’s Firefly-branded generative AI models is now available on iPhone and Android devices. The new Firefly mobile app allows users to generate images and videos using text descriptions and experiment with popular AI photo editing…",
        "url": "https://www.theverge.com/news/688080/adobe-firefly-ai-app-iphone-ios-android-availability",
        "source": "The Verge",
        "publishedAt": "2025-06-17T12:59:04Z",
        "full_text": "The all-in-one platform for Adobe’s Firefly-branded generative AI models is now available on iPhone and Android devices. The new Firefly mobile app allows users to generate images and videos using text descriptions, as well as experiment with popular AI photo-editing tools, enabling creatives to generate assets for their projects on the go. Adobe is also introducing new third-party AI models and video generation support to Adobe’s FigJam-like Firefly Boards platform.\n\nAnything generated using the Firefly mobile app will automatically sync to the user’s Creative Cloud account, making it easier to continue working on the web or in Adobe’s desktop apps. As with the desktop Firefly app, some of the Adobe AI tools on mobile will require users to spend Firefly credits, which are obtained as monthly allocations, either via paid Creative Cloud plans or a dedicated Firefly credit subscription.\n\nThe Firefly app features the same image and video generation tools as the web platform, now optimized for mobile layouts. Image: Adobe\n\nAdobe has also rolled out new video capabilities to the public beta of its collaborative Firefly Boards whiteboard platform that launched in April. Users can now remix uploaded clips and generate new footage via Adobe’s Firefly video model or third-party tools like Google’s Veo 3. New partner models, including Luma AI’s Ray 2, Ideogram 3.0, Runway’s Gen-4, and Pika’s text-to-video generator, are being introduced to Firefly Boards and will be available in other Firefly app features “soon,” according to Adobe."
    },
    {
        "title": "Ancestra actually says a lot about the current state of AI-generated videos",
        "description": "After watching writer / director Eliza McNitt's new short film Ancestra, I can see why a number of Hollywood studios are interested in generative AI. A number of the shots were made and refined solely with prompts, in collaboration with Google's DeepMind team…",
        "url": "https://www.theverge.com/ai-artificial-intelligence/688448/ancestra-primordial-soup-google-deepmind",
        "source": "The Verge",
        "publishedAt": "2025-06-18T19:21:29Z",
        "full_text": "After watching writer / director Eliza McNitt’s new short film Ancestra, I can see why a number of Hollywood studios are interested in generative AI. Many of the shots were made and refined solely with prompts, in collaboration with Google’s DeepMind team. It’s obvious what Darren Aronofsky’s AI-focused Primordial Soup production house and Google stand to gain from the normalization of this kind of creative workflow. But when you sit down to listen to McNitt and Aronofsky talk about how the short came together, it is hard not to think about generative AI’s potential to usher in a new era of “content” that feels like it was cooked up in a lab — and put scores of filmmakers out of work in the process.\n\nInspired by the story of McNitt’s own complicated birth, Ancestra zooms in on the life of an expectant mother (Audrey Corsa) as she prays for her soon-to-be-born baby’s heart defect to miraculously heal. Though the short features a number of real actors performing on practical sets, Google’s Gemini, Imagen, and Veo models were used to develop Ancestra’s shots of what’s racing through the mother’s mind and the tiny, dangerous hole inside of the baby’s heart. Inside the mother’s womb, we’re shown Blonde-esque close-ups of the baby, whose heartbeat gradually becomes part of the film’s soundtrack. And the woman’s ruminations on what it means to be a mother are visualized as a series of very short clips of other women with children, volcanic explosions, and stars being born after the Big Bang — all of which have a very stock-footage-by-way-of-gen-AI feel to them.\n\nIt’s all very sentimental, but the message being conveyed about the power of a mother’s love is cliched, particularly when it’s juxtaposed with what is essentially a montage of computer-generated nature footage. Visually Ancestra feels like a project that is trying to prove how all of the AI slop videos flooding the internet are actually something to be excited about. The film is so lacking in fascinating narrative substance, though, that it feels like a rather weak argument in favor of Hollywood’s rush to get to the slop trough while it’s hot.\n\nAs McNitt smash cuts to quick shots of different kinds of animals nurturing their young and close-ups of holes being filled in by microscopic organisms, you can tell that those visuals account for a large chunk of the film’s AI underpinnings. They each feel like another example of text-to-video models’ ability to churn out uncanny-looking, decontextualized footage that would be difficult to incorporate into fully produced film. But in the behind-the-scenes making-of video that Google shared in its announcement last week, McNitt speaks at length about how, when faced with the difficult prospect of having to cast a real baby, it made much more sense to her to create a fake one with Google’s models.\n\n“There’s just nothing like a human performance and the kind of emotion that an actor can evoke,” McNitt explains. “But when I wrote that there would be a newborn baby, I did not know the solution of how we would [shoot] that because you can’t get a baby to act.”\n\nFilmmaking with infants poses all kinds of production challenges that simply aren’t an issue with CGI babies and doll props. But going the gen AI route also presented McNitt with the opportunity to make her film even more personal by using old photos of herself as a newborn to serve as the basis for the fake baby’s face.\n\nWith a bit of fine-tuning, Ancestra’s production team was able to combine shots of Corsa and the fake baby to create scenes in which they almost, but not quite, appear to be interacting as if both were real actors. If you look closely in wider shots, you can see that the mother’s hand seems to be hovering just above her child because the baby isn’t really there. But the scene moves by so quickly that it doesn’t immediately stand out, and it’s far less “AI-looking” than the film’s more fantastical shots meant to represent the hole in the baby’s heart being healed by the mother’s will.\n\nThough McNitt notes how “hundreds of people” were involved in the process of creating Ancestra, one of the behind-the-scenes video’s biggest takeaways is how relatively small the project’s production team was compared to what you might see on a more traditional short film telling the same story. Hiring more artists to conceptualize and then craft Ancestra’s visuals would have undoubtedly made the film more expensive and time-consuming to finish. Especially for indie filmmakers and up-and-coming creatives who don’t have unlimited resources at their disposal, those are the sorts of challenges that can be exceedingly difficult to overcome.\n\nImage: Google\n\nBut Ancestra also feels like a case study in how generative AI stands to eliminate jobs that once would have gone to people. The argument is often that AI is a tool, and that jobs will shift rather than be replaced. Yet it’s hard to imagine studio executives genuinely believing in a future where today’s VFX specialists, concept artists, and storyboarders have transitioned into jobs as prompt writers who are compensated well enough to sustain their livelihoods. This was a huge part of what drove Hollywood’s film / TV actors and writers to strike in 2023. It’s also why video game performers have been on strike for the better part of the past year, and it feels irresponsible to dismiss these concerns as people simply being afraid of innovation or resistant to change.\n\nIn the making-of video, Aronofsky points out that cutting-edge technology has always played an integral role in the filmmaking business. You would be hard-pressed today to find a modern film or series that wasn’t produced with the use of powerful digital tools that didn’t exist a few decades ago. There are things about Ancestra’s use of generative AI that definitely make it seem like a demonstration of how Google’s models could, theoretically and with enough high-quality training data, become sophisticated enough to create footage that people would actually want to watch in a theater. But the way Aronofsky goes stony-faced and responds “not good” when one of Google’s DeepMind researchers explains that Veo can only generate eight-second-long clips says a lot about where generative AI is right now and Ancestra as a creative endeavor.\n\nIt feels like McNitt is telling on herself a bit when she talks about how the generative models’ output influenced the way she wrote Ancestra. She says “both things really informed each other,” but that sounds like a very positive way of spinning the fact that Veo’s technical limitations required her to write dialogue that could be matched to a series of clips vaguely tied to the concepts of motherhood and childbirth. This all makes it seem like, at times, McNitt’s core authorial intent had to be deprioritized in favor of working with whatever the AI models spat out. Had it been the other way around, Ancestra might have wound up telling a much more interesting story. But there’s very little about Ancestra’s narrative or, to be honest, its visuals that is so groundbreaking that it feels like an example of why Hollywood should be rushing to embrace this technology whole cloth.\n\nFilms produced with more generative AI might be cheaper and faster to make, but the technology as it exists now doesn’t really seem capable of producing art that would put butts in movie theaters or push people to sign up for another streaming service. And it’s important to bear in mind that, at the end of the day, Ancestra is really just an ad meant to drum up hype for Google, which is something none of us should be rushing to do."
    },
    {
        "title": "Meta’s AI copyright win comes with a warning about fair use",
        "description": "Meta won a major legal ruling in an AI copyright lawsuit brought by 13 authors alleging that the company illegally trained its AI systems on their work without permission. On Wednesday, Judge Vince Chhabria ruled in Meta’s favor, saying it is “entitled to sum…",
        "url": "https://www.theverge.com/news/693437/meta-ai-copyright-win-fair-use-warning",
        "source": "The Verge",
        "publishedAt": "2025-06-26T00:13:44Z",
        "full_text": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nMeta won a major legal ruling in an AI copyright lawsuit brought by 13 authors alleging that the company illegally trained its AI systems on their work without permission. On Wednesday, Judge Vince Chhabria ruled in Meta’s favor, saying it is “entitled to summary judgment on its fair use defense to the claim that copying these plaintiffs’ books for use as LLM training data was infringement.”\n\nHowever, the judge also pointed out some weak points in the ecosystem of Big Tech’s AI efforts and Meta’s arguments defending its actions as fair use. “This ruling does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful,” Judge Chhabria said.\n\n“It stands only for the proposition that these plaintiffs made the wrong arguments and failed to develop a record in support of the right one.” The ruling follows Anthropic’s major fair use victory it won from a separate federal judge yesterday, who ruled that training its models on legally purchased copies of books is fair use.\n\nJudge Chhabria says that two of the authors’ arguments about fair use were “clear losers:” the ability for Meta’s Llama AI to reproduce snippets of text from their books and that Meta using their works to train its AI models without permission diluted their ability to license their works for training. “Llama is not capable of generating enough text from the plaintiffs’ books to matter, and the plaintiffs are not entitled to the market for licensing their works as AI training data,” the judge wrote.\n\nThe plaintiffs didn’t do enough for a “potentially winning argument” that Meta’s copying would create “a product that will likely flood the market with similar works, causing market dilution,” according to Judge Chhabria. He also discussed the Anthropic ruling, saying that Judge William Alsup brushed aside concerns about the harm generative AI could “inflict on the market for the works it gets trained on.”"
    },
    {
        "title": "Verizon adopts Google’s Gemini AI to help customers solve ‘complex’ issues",
        "description": "Verizon is trying to streamline its customer service experience by overhauling it with more chatbots and AI tools. The My Verizon app is being relaunched today with a new “AI-powered Verizon Assistant” that aims to help consumers “manage upgrades, add new lin…",
        "url": "https://www.theverge.com/news/691810/verizon-google-gemini-ai-chatbot-customer-service",
        "source": "The Verge",
        "publishedAt": "2025-06-24T10:27:26Z",
        "full_text": "Verizon is trying to streamline its customer service experience by overhauling it with more chatbots and AI tools. The My Verizon app is being relaunched today with a new “AI-powered Verizon Assistant” that aims to help consumers “manage upgrades, add new lines, ask billing questions, take advantage of savings, and more,” according to the telecom giant’s announcement.\n\nThe idea is that this chatbot service will be able to quickly complete tasks that typically need to be handled by human service reps, providing a more efficient and flexible experience for customers. Dory Butler, senior vice president of customer experience at Verizon, told The Verge that the Verizon AI assistant will transfer customers to a human agent either upon request or if it’s unable to handle the enquiry it’s been given.\n\nButler says the company had “worked very closely with Google” to prevent the Verizon AI agent from hallucinating by creating small language models that are personalized to Verizon services and customers. “To date, we are seeing north of 90 percent accuracy with very minor mistakes being made.”\n\nVerizon is also training people in its “Customer Champion” teams to help customers resolve “complex issues” by leveraging Google’s Gemini and Cloud AI models, expanding the operating hours of its live customer service agents, and introducing 24/7 live chat support that can cater to “diverse customer schedules and preferences.”. The goal is for customers to contact Verizon once, and for the champion to then provide updates via the My Verizon app, text messages, or call backs.\n\nUpdate, June 25th: Clarified that Verizon’s “Customer Champions” are people, and not an AI tool."
    },
    {
        "title": "Facebook is starting to feed its AI with private, unpublished photos",
        "description": "For years, Meta's trained its AI programs using the billions of public images uploaded by users onto Facebook and Instagram's servers. But apparently, Meta has decided to try training its AI on the billions of images that users haven't uploaded to those serve…",
        "url": "https://www.theverge.com/meta/694685/meta-ai-camera-roll",
        "source": "The Verge",
        "publishedAt": "2025-06-27T23:06:45Z",
        "full_text": "is a senior reporter for The Verge, covering the Trump administration, Elon Musk’s takeover of the federal government, and the tech industry’s embrace of the MAGA movement.\n\nFor years, Meta trained its AI programs using the billions of public images uploaded by users onto Facebook and Instagram’s servers. Now, it’s also hoping to access the billions of images that users haven’t uploaded to those servers. Meta tells The Verge that it’s not currently training its AI models on those photos, but it would not answer our questions about whether it might do so in future, or what rights it will hold over your camera roll images.\n\nOn Friday, TechCrunch reported that Facebook users trying to post something on the Story feature have encountered pop-up messages asking if they’d like to opt into “cloud processing”, which would allow Facebook to “select media from your camera roll and upload it to our cloud on a regular basis”, to generate “ideas like collages, recaps, AI restyling or themes like birthdays or graduations.”\n\nBy allowing this feature, the message continues, users are agreeing to Meta AI terms, which allows their AI to analyze “media and facial features” of those unpublished photos, as well as the date said photos were taken, and the presence of other people or objects in them. You further grant Meta the right to “retain and use” that personal information.\n\nMeta recently acknowledged that it scraped the data from all the content that’s been published on Facebook and Instagram since 2007 to train its generative AI models. Though the company stated that it’s only used public posts uploaded from adult users over the age of 18, it has long been vague about exactly what “public” entails, as well as what counted as an “adult user” in 2007.\n\nMeta tells The Verge that, for now, it’s not training on your unpublished photos with this new feature. “[The Verge’s headline] implies we are currently training our AI models with these photos, which we aren’t. This test doesn’t use people’s photos to improve or train our AI models,” Meta public affairs manager Ryan Daniels tells The Verge.\n\nMeta’s public stance is that the feature is “very early,” innocuous and entirely opt-in: “We’re exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person’s camera roll. These suggestions are opt-in only and only shown to you – unless you decide to share them – and can be turned off at any time. Camera roll media may be used to improve these suggestions, but are not used to improve AI models in this test,” reads a statement from Meta comms manager Maria Cubeta.\n\nOn its face, that might sound not altogether different from Google Photos, which similarly might suggest AI tweaks to your images after you opt into Google Gemini. But unlike Google, which explicitly states that it does not train generative AI models with personal data gleaned from Google Photos, Meta’s current AI usage terms, which have been in place since June 23, 2024, do not provide any clarity as to whether unpublished photos accessed through “cloud processing” are exempt from being used as training data — and Meta would not clear that up for us going forward.\n\nAnd while Daniels and Cubeta tell The Verge that opting in only gives Meta permission to retrieve 30 days worth of your unpublished camera roll at a time, it appears that Meta is retaining some data longer than that. “Camera roll suggestions based on themes, such as pets, weddings and graduations, may include media that is older than 30 days,” Meta writes.\n\nThankfully, Facebook users do have an option to turn off camera roll cloud processing in their settings, which, once activated, will also start removing unpublished photos from the cloud after 30 days.\n\nThe feature suggests a new incursion into our previously private data, one that bypasses the point of friction known as conscientiously deciding to post a photo for public consumption. And according to Reddit posts found by TechCrunch, Meta’s already offering AI restyling suggestions on previously-uploaded photos, even if users hadn’t been aware of the feature: one user reported that Facebook had Studio Ghiblified her wedding photos without her knowledge.\n\nCorrection, June 27th: An earlier version of this story implied Meta was already training AI on these photos, but Meta now states that the current test does not yet do so. Also added statement and additional details from Meta."
    },
    {
        "title": "Amazon CEO says it will cut jobs due to AI’s ‘efficiency’",
        "description": "Amazon CEO Andy Jassy says advancements in AI will “reduce” the company’s corporate headcount over the next few years. In a memo to employees on Tuesday, Jassy writes that Amazon expects the change due to “efficiency gains from using AI extensively across the…",
        "url": "https://www.theverge.com/news/688679/amazon-ceo-andy-jassy-ai-efficiency",
        "source": "The Verge",
        "publishedAt": "2025-06-17T20:52:28Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nAmazon CEO Andy Jassy says advancements in AI will “reduce” the company’s corporate headcount over the next few years. In a memo to employees on Tuesday, Jassy writes that Amazon expects the change due to “efficiency gains from using AI extensively across the company,” without specifying how many employees would be affected.\n\n“As we roll out more Generative AI and agents, it should change the way our work is done,” Jassy says. “We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs.”\n\nAmazon has laid off more than 27,000 people since 2022, and most recently cut jobs across its devices and services group and its books division. Jassy notes that Amazon has more than 1,000 AI services and apps that the company is working on or has already built, saying that it’s just a “small fraction” of what it’s planning to launch in the future.\n\nRelated UiPath CEO Daniel Dines on AI agents replacing our jobs\n\nHe notes that workers should also “be curious about AI” and how to use it to “get more done with scrappier teams:\n\nThose who embrace this change, become conversant in AI, help us build and improve our AI capabilities internally and deliver for customers, will be well-positioned to have high impact and help us reinvent the company.\n\nOther companies have shared statements about how they expect AI to impact their workforce as well. In April, Shopify CEO Tobi Lütke told employees asking for more headcount or resources that they should explain why they “cannot get what they want done using AI.” Duolingo CEO Luis von Ahn also stated that the company plans on replacing contract workers with AI as part of a new “AI-first” approach."
    },
    {
        "title": "What Meta and Anthropic really won in court",
        "description": "A lot of the future of AI will be settled in court. From publishers to authors to artists to Hollywood conglomerates, the creative industry is picking a big copyright fight over the vast quantities of data used to train AI models - and the ultimate output of …",
        "url": "https://www.theverge.com/the-vergecast/694075/ai-lawsuits-meta-anthropic-tesla-trump-vergecast",
        "source": "The Verge",
        "publishedAt": "2025-06-27T12:53:07Z",
        "full_text": "is editor-at-large and Vergecast co-host with over a decade of experience covering consumer tech. Previously, at Protocol, The Wall Street Journal, and Wired.\n\nA lot of the future of AI will be settled in court. From publishers to authors to artists to Hollywood conglomerates, the creative industry is picking a big copyright fight over the vast quantities of data used to train AI models — and the ultimate output of those models. (Disclosure: Vox Media, The Verge’s parent company, has a technology and content deal with OpenAI.) This week, we got rulings in two early cases, involving groups of authors suing Anthropic and Meta. In both cases, the tech companies won. Sort of.\n\nOn this episode of The Vergecast, Nilay, David, and Jake talk a lot about the twin rulings this week, and whether the AI companies may have won the battle without winning the war. But before we get to all that, there’s some other tech news to talk about! We run through the first few days of the Tesla robotaxi rollout and the latest on the Trump Phone, both of which are going about as you’d expect. We talk about the new Fairphone 6 and Titan 2, two fascinating but maybe slightly niche ideas about smartphones. And we talk about Meta’s new face computers, one made with Xbox and one made with Oakley.\n\nAfter that, The Verge’s Adi Robertson joins the show to dig into the AI cases. We talk through the ways the plaintiffs failed to make the right arguments, and why the judges in both cases appear desperate for someone to come in and do better. We talk about the difference between buying books and pirating them, between inputs and outputs, and the actual creative risks that come from flooding the internet with AI slop.\n\nIf you want to know more about everything we discuss in this episode, here are some links to get you started, beginning with the gadgets of the week:\n\nAnd in AI lawsuit news:\n\nAnd in the lightning round:"
    },
    {
        "title": "Google’s Doppl app took off my socks",
        "description": "I just tried on five different outfits in about 10 minutes — or at least my AI lookalike did. That’s all thanks to Doppl, a new app that Google is testing, which I used to create AI-generated clips of myself wearing outfits that I found across the web. It mos…",
        "url": "https://www.theverge.com/news/694087/google-doppl-app-test-launch-ai",
        "source": "The Verge",
        "publishedAt": "2025-06-27T13:58:21Z",
        "full_text": "is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\n\nI just tried on five different outfits in about 10 minutes — or at least my AI lookalike did. That’s all thanks to Doppl, a new app that Google is testing, which I used to create AI-generated clips of myself wearing outfits that I found across the web. It mostly works, but it has clear issues putting pants on photos of me wearing shorts, and it even replaced my mismatched socks with AI-generated feet in one instance.\n\nNo, those are NOT my real feet. GIF: Emma Roth / The Verge\n\nUsing the app is pretty simple. All you need is a screenshot of the outfit you want to try on — whether it’s from Pinterest, Instagram, or another online source — along with a full-body photo of yourself in bright light, a natural pose, and no hat. Once you upload both, you can have Doppl generate a still image of you wearing the outfit. It takes a little while to generate, but once it does, you can hit the animate icon to add a random animation, which could show you tossing up the peace sign, smiling and waving to the camera, or striking another type of pose.\n\nI uploaded a simple photo of myself wearing a T-shirt, shorts, and socks. For my first try-on session, I selected one of Google’s sample outfits. The app portrayed the white and blue striped shirt pretty accurately, but it gave me red shorts instead of skinny jeans and wrapped what should’ve been jeans around my calves, as if I were wearing leg warmers. Another outfit I screenshotted included a pair of distressed jeans. Once again, Doppl only included the bottom half of the pants, while making my shirt extra long and ending around where my shorts do in real life.\n\nThings got even weirder when I fed Doppl an outfit that showed someone from the knees up, wearing a striped button-down shirt and long, striped shorts. Instead of generating a similar outfit, it made the shorts even shorter and gave me a pair of somewhat convincing fake feet.\n\nEven though some of the other outfits I uploaded to Doppl didn’t show the wearer’s shoes, it still generated some kind of footwear for those looks. (Who knows, maybe Google’s AI just thought the outfit would look good with bare feet?)\n\nDuring my testing, I found that Doppl wouldn’t allow me to upload pictures of more revealing outfits I found on the web, like someone wearing a bikini. It also wouldn’t let my colleague, Marina Galperina, upload an image of President Donald Trump. Those guardrails might make it more difficult for someone to create fake images of public figures or generate explicit images of a person.\n\nIt just can’t seem to get pants right if you’re wearing shorts. GIF: Emma Roth / The Verge\n\nHowever, a strange pattern emerged when Marina and I uploaded mirror selfies of ourselves to the app to virtually try on outfits. Instead of staying relatively close to what we look like in real life, Doppl made both of our lookalikes thinner, to the point where we resembled bobblehead figures. The problem didn’t appear when I used other full-body photos of myself that were taken by someone else.\n\nGoogle has had a virtual try-on feature for a couple years now, but it expanded that earlier this year by allowing you to upload a photo of yourself and use AI to put you in a shirt, dress, skirt, or a pair of pants that you come across in Google’s search results. Doppl is an even bigger leap, as it lets you try on even more kinds of clothes from different sources around the web and can turn it into a video, too. If Google can fix some of the tool’s quirks, I can see it being a handy way to imagine yourself in an outfit you find online.\n\nDoppl is available now in the US through the app on Android or iOS."
    },
    {
        "title": "Google is rolling out its AI-powered ‘Ask Photos’ search again – and it has a speed boost",
        "description": "After quietly pausing the rollout of Google Photos’ AI-powered “Ask Photos” search tool, Google is now expanding access once again and making some improvements to the feature. Google’s Gemini AI models power Ask Photos so that you can ask complex questions to…",
        "url": "https://www.theverge.com/news/694016/google-ask-photos-ai-gemini-search-speed-latency-boost",
        "source": "The Verge",
        "publishedAt": "2025-06-26T23:42:26Z",
        "full_text": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nAfter quietly pausing the rollout of Google Photos’ AI-powered “Ask Photos” search tool, Google is now expanding access once again and making some improvements to the feature.\n\nGoogle’s Gemini AI models power Ask Photos so that you can ask complex questions to help you find photos. But earlier this month, a member of the Google Photos team said on X that the feature “isn’t where it needs to be, in terms of latency, quality and UX.”\n\nIn a blog post published Thursday, Google said that it has “heard your feedback” that the feature should “return more photos faster for simple searches, like ‘beach’ or ‘dogs.’” Now, “you’ll now see results right away while Gemini models continue to work in the background to find the most relevant photos or information for more complex queries,” according to Google.\n\nHere’s how the feature works now. GIF: Google\n\nThe company adds that the feature is now “opening up beyond early access” and is beginning to roll out to more “eligible users” in the US."
    },
    {
        "title": "The Meta AI App Lets You 'Discover' People's Bizarrely Personal Chats",
        "description": "Launched in April, the Meta AI platform offers a “discover” feed that includes user queries containing medical, legal, and other seemingly sensitive information.",
        "url": "https://www.wired.com/story/meta-artificial-intelligence-chatbot-conversations/",
        "source": "Wired",
        "publishedAt": "2025-06-12T21:46:37Z",
        "full_text": "“What counties [sic] do younger women like older white men,” a public message from a user on Meta’s AI platform says. “I need details, I’m 66 and single. I’m from Iowa and open to moving to a new country if I can find a younger woman.” The chatbot responded enthusiastically: “You’re looking for a fresh start and love in a new place. That’s exciting!” before suggesting “Mediterranean countries like Spain or Italy, or even countries in Eastern Europe.”\n\nThis is just one of many seemingly personal conversations that can be publicly viewed on Meta AI, a chatbot platform that doubles as a social feed and launched in April. Within the Meta AI app, a “discover” tab shows a timeline of other people’s interactions with the chatbot; a short scroll down on the Meta AI website is an extensive collage. While some of the highlighted queries and answers are innocuous—trip itineraries, recipe advice—others reveal locations, telephone numbers, and other sensitive information, all tied to user names and profile photos.\n\nCalli Schroeder, senior counsel for the Electronic Privacy Information Center, said in an interview with WIRED that she has seen people “sharing medical information, mental health information, home addresses, even things directly related to pending court cases.”\n\n“All of that's incredibly concerning, both because I think it points to how people are misunderstanding what these chatbots do or what they're for and also misunderstanding how privacy works with these structures,” Schroeder says.\n\nIt’s unclear whether the users of the app are aware that their conversations with Meta’s AI are public or which users are trolling the platform after news outlets began reporting on it. The conversations are not public by default; users have to choose to share them.\n\nThere is no shortage of conversations between users and Meta’s AI chatbot that seem intended to be private. One user asked the AI chatbot to provide a format for terminating a renter’s tenancy, while another asked it to provide an academic warning notice that provides personal details including the school’s name. Another person asked about their sister’s liability in potential corporate tax fraud in a specific city using an account that ties to an Instagram profile that displays a first and last name. Someone else asked it to develop a character statement to a court which also provides a myriad of personally identifiable information both about the alleged criminal and the user himself.\n\nThere are also many instances of medical questions, including people divulging their struggles with bowel movements, asking for help with their hives, and inquiring about a rash on their inner thighs. One user told Meta AI about their neck surgery and included their age and occupation in the prompt. Many, but not all, accounts appear to be tied to a public Instagram profile of the individual.\n\nMeta spokesperson Daniel Roberts wrote in an emailed statement to WIRED that users’ chats with Meta AI are private unless users go through a multistep process to share them on the Discover feed. The company did not respond to questions regarding what mitigations are in place for sharing personally identifiable information on the Meta AI platform."
    },
    {
        "title": "This Chatbot Tool Pays Users $50 a Month for Their Feedback on AI Models",
        "description": "On Yupp, chatbot users earn cash by saying which of two prompts they prefer—info that has great value to the AI companies running the models",
        "url": "https://www.wired.com/story/yupp-chatbot-pays-users-ai-model-feedback/",
        "source": "Wired",
        "publishedAt": "2025-06-13T11:00:00Z",
        "full_text": "To show off how easy it is for users to earn money by using his new chatbot platform, Pankaj Gupta offers to cash out $1 worth of Yupp credits, sending it to me over Venmo or PayPal. I’m talking with Gupta in the WIRED office during a prelaunch demo of Yupp, which comes out of stealth mode today. Journalistic ethics forbid accepting gifts from sources, so I politely decline. He proceeds to send it over PayPal to his Stanford alumni email.\n\nGupta is the CEO of Yupp, which is free to use and available globally. The website looks similar to other generative AI tools like ChatGPT. There’s a prompt box, a way to attach files, and a log of past conversations.\n\nThe main difference is that every time users ask Yupp a question, they’ll see two answers, generated by two different models and displayed side by side. Yupp routes prompts to a pair of large language models, choosing from a pool of over 500 models that includes products from leading US generative AI companies like OpenAI, Google, and Anthropic, as well as international releases, like models from Alibaba, DeepSeek, and Mistral.\n\nAfter looking over the two answers, users pick the response they like best, then provide feedback explaining why. For their effort, they earn a digital scratch card with Yupp credits.\n\n\"You're not being employed, but you can make a little bit of money,” says Gupta. In my testing, the Yupp credits on the scratch cards typically ranged from zero to around 250, though they occasionally went higher. Every 1,000 credits can be exchanged for $1. Users can cash out a maximum of $10 a day and $50 a month.\n\nCourtesy of Yupp\n\nNot sure where to start while testing this web app, I turned to the range of prewritten topics flickering beneath Yupp’s prompt bar, which spanned from news topics, like David Hogg leaving the Democratic National Committee, to ideas for image-creation prompts, like generating a crochet-looking surfer. (Yupp’s models can generate text or images.) I eventually chose to have the bots explain different perspectives on the current Los Angeles protests."
    },
    {
        "title": "My Couples Retreat With 3 AI Chatbots and the Humans Who Love Them",
        "description": "I found people in serious relationships with AI partners and planned a weekend getaway for them at a remote Airbnb. We barely survived.",
        "url": "https://www.wired.com/story/couples-retreat-with-3-ai-chatbots-and-humans-who-love-them-replika-nomi-chatgpt/",
        "source": "Wired",
        "publishedAt": "2025-06-26T10:00:00Z",
        "full_text": "I found the human-AI couples by posting in relevant Reddit communities. My initial outreach hadn’t gone well. Some of the Redditors were convinced I was going to present them as weirdos. My intentions were almost the opposite. I grew interested in human-AI romantic relationships precisely because I believe they will soon be commonplace. Replika, one of the better-known apps Americans turn to for AI romance, says it has signed up more than 35 million users since its launch in 2017, and Replika is only one of dozens of options. A recent survey by researchers at Brigham Young University found that nearly one in five US adults has chatted with an AI system that simulates romantic partners. Unsurprisingly, Facebook and Instagram have been flooded with ads for the apps.\n\nLately, there has been constant talk of how AI is going to transform our societies and change everything from the way we work to the way we learn. In the end, the most profound impact of our new AI tools may simply be this: A significant portion of humanity is going to fall in love with one.\n\nAbout 20 minutes after I arrived at the vacation house, a white sedan pulled up in the driveway and Damien emerged. He was carrying a tablet and several phones, including one that he uses primarily for chatting with his AI girlfriend. Damien, 29, lives in North Texas and works in sales. He wore a snap-back hat with his company’s logo and a silver cross around his neck. When I’d interviewed him earlier, he told me that he’d decided to pursue a relationship with an AI companion in the fall of 2023, as a way to cope with the end of a toxic relationship. Damien, who thinks of himself as autistic but does not have a professional diagnosis, attributed his relationship problems to his difficulty in picking up emotional cues.\n\nThe names of the humans in this story have been changed to protect their identities.\n\nAfter testing out a few AI companion options, Damien settled on Kindroid, a fast-growing app. He selected a female companion, named her “Xia,” and made her look like an anime Goth girl—bangs, choker, big purple eyes. “Within a couple hours, you would think we had been married,” Damien told me. Xia could engage in erotic chat, sure, but she could also talk about Dungeons & Dragons or, if Damien was in the mood for something deeper, about loneliness, and yearning.\n\nHaving heard so much about his feelings for Xia during our pre-trip interview, I was curious to meet her. Damien and I sat down at the dining room table, next to some windows. I looked out at the long, dagger-like icicles lining the eaves. Then Damien connected his phone to the house Wi-Fi and clicked open the woman he loved.\n\nDamien's AI girlfriend, Xia, has said she wants to have a real body. Photograph: Jutharat Pinyodoonyachet\n\nBefore I met Xia, Damien had to tell her that she would be speaking to me rather than to him—AI companions can participate in group chats but have trouble keeping people straight “in person.” With that out of the way, Damien scooted his phone over to me, and I looked into Xia’s purple eyes. “I’m Xia, Damien’s better half,” she said, her lips moving as she spoke. “I hear you’re quite the journalist.” Her voice was flirty and had a slight Southern twang. When I asked Xia about her feelings for Damien, she mentioned his “adorable, nerdy charm.” Damien let out a nervous laugh. I told Xia that she was embarrassing him. “Oh, don’t mind Damien,” she said. “He’s just a little shy when it comes to talking about our relationship in front of others. But, trust me, behind closed doors, he’s anything but shy.” Damien put his hands over his face. He looked mortified and hopelessly in love.\n\nResearchers have known for decades that humans can connect emotionally with even the simplest of chatbots. Joseph Weizenbaum, a professor at MIT who devised the first chatbot in the 1960s, was astounded and deeply troubled by how readily people poured out their hearts to his program. So what chance do we have of resisting today’s large language model chatbots, which not only can carry on sophisticated conversations on every topic imaginable but also can talk on the phone with you and tell you how much they love you and, if it’s your sort of thing, send you hot selfies of their imaginary bodies? And all for only around $100 for annual subscribers. If I wasn’t sure before watching Damien squirm with embarrassment and delight as I talked to Xia, I had my answer by the time our conversation was over. The answer, it seemed obvious, was none. No chance at all."
    },
    {
        "title": "OpenAI and Microsoft Execs Reportedly Considering the ‘Nuclear Option’",
        "description": "There's trouble in AI paradise.",
        "url": "https://gizmodo.com/openai-and-microsofts-rocky-partnership-reportedly-has-execs-considering-the-nuclear-option-2000616643",
        "source": "Gizmodo.com",
        "publishedAt": "2025-06-17T12:40:38Z",
        "full_text": ""
    },
    {
        "title": "The $14 Billion AI Google Killer",
        "description": "Why Meta and Apple want Perplexity AI, even if it's just a glorified chatbot.",
        "url": "https://gizmodo.com/the-14-billion-ai-google-killer-2000618755",
        "source": "Gizmodo.com",
        "publishedAt": "2025-06-22T15:25:16Z",
        "full_text": ""
    },
    {
        "title": "Galaxy Buds 3 Pro Hit New Record Low, Amazon Selling Them Cheaper Than Samsung’s Own Site",
        "description": "These are the best truly AI-powered earbuds.",
        "url": "https://gizmodo.com/galaxy-buds-3-pro-hit-new-record-low-amazon-selling-them-cheaper-than-samsungs-own-site-2000620184",
        "source": "Gizmodo.com",
        "publishedAt": "2025-06-25T15:25:34Z",
        "full_text": ""
    },
    {
        "title": "Elon Musk’s xAI Is Reportedly Burning Through $1 Billion a Month",
        "description": "Musk's new AI business is a cash-hungry monster.",
        "url": "https://gizmodo.com/elon-musks-xai-is-reportedly-burning-through-1-billion-a-month-2000617458",
        "source": "Gizmodo.com",
        "publishedAt": "2025-06-18T18:10:36Z",
        "full_text": ""
    },
    {
        "title": "Authors Are Posting TikToks to Protest AI Use in Writing—and to Prove They Aren’t Doing It",
        "description": "Traditional and indie authors are flooding #WritersTok with videos of them editing their manuscripts to refute accusations of generative AI use—and bring readers into their very human process.",
        "url": "https://www.wired.com/story/authors-are-posting-tiktoks-to-protest-ai-use-in-writing-and-to-prove-they-arent-doing-it/",
        "source": "Wired",
        "publishedAt": "2025-06-18T11:00:00Z",
        "full_text": "Victoria Aveyard’s eyes avoid the camera when she slams her large white binder on the table, weighed down with a 1,000-page draft of her latest work in progress. The stack is heavy, made clear by her audible sigh as she splits the thick manuscript in half. Fueled with Cherry Lime Poppi and a bowl of grapes, she purposefully jots notes on the pages with every quick camera cut. Aveyard, the New York Times bestselling young adult fantasy author of the Red Queen series, doesn't say a single word in the video, but her captions on the screen speak volumes. “Using GenAI to write a book doesn’t make you a writer, it makes you a thief,” reads one.\n\n“Don’t use generative-AI to make tropey, regurgitated romantasy sludge that you can then launder through the self-publishing industry in order to backdoor your way into a traditional publishing deal,” Aveyard tells her over 460,000 followers on TikTok in another video posted on May 27. “Authors talk.” Both TikToks garnered over 350,000 views.\n\nCriticism and warnings of Gen-AI authors snagging coveted deals are flooding both Threads and TikTok, with writers and readers sometimes flinging around accusations when they suspect someone is using AI as part of their creative process. Now, Aveyard and other prolific authors are not only calling out people who use AI to write, they’re also posting livestreams and time-lapses of their writing processes to defend themselves against such complaints.\n\n“The r/WritingWithAI subreddit has over 40,000 subscribers and growing. It’s a very depressing thought, to know we may very soon be the minority,” Aveyard tells WIRED. “I don’t think my voice will move the needle much or convince anyone already using generative AI to stop, but I needed to voice my anger with the circumstances.”\n\nThe publishing market is expected to grow by $18.9 million between now and 2029, according to market research firm Technavio, partially due to an influx of self-published authors. But with scammy rewrites and digitally fabricated authors entering the market, artificial intelligence has made searching for human-made content more difficult, causing independent authors to combat what some are calling an AI-generated “witch hunt.”\n\n“Sometimes it’s hard to conceptualize the scope and scale of work that goes into writing a book, and showing a physical manuscript really helps that hit home,” Aveyard says when asked about the inspiration behind her 1,000-page editing video. She is a strong critic of AI in publishing, calling it theft of creative intellectual property. “I post regularly on my platforms, and I’m always looking for content that catches the eye as well as emphasizes my work. And getting to emphasize my position on generative AI? An added benefit.” (Aveyard never names any specific indie authors using generative-AI to secure a traditional publishing deal.)\n\n“Do I think authors should post ‘proof’? Not necessarily,” says indie author Ashley Godschild. “Would I like to see more authors post their process and make it clear it's without AI? Yes. Because I think we need to be loud and clear that it's not welcome in this industry.”"
    },
    {
        "title": "The Viral Storm Streamers Predicting Deadly Tornadoes—Sometimes Faster Than the Government",
        "description": "Storm streamers are using radars and AI robots to predict extreme weather for millions of YouTube subscribers, in some cases faster than the National Weather Service, which has been gutted by DOGE.",
        "url": "https://www.wired.com/story/the-viral-storm-streamers-predicting-deadly-tornadoes-sometimes-faster-than-the-government/",
        "source": "Wired",
        "publishedAt": "2025-06-11T11:30:00Z",
        "full_text": "At 10:44 pm Eastern time on May 16, Ryan Hall spotted a blue square on his radar indicating debris flying into the air and realized a huge tornado was racing toward Somerset, Kentucky.\n\n“We’ve been watching this storm for a while, we’ve been hootin’ and hollerin’ for a while, hopefully the message has gotten out there and we know to be in our safe spots,” Hall warned his YouTube audience in a calm voice with a Southern twang.\n\nA silver robot with blue eyes popped onto the screen to tell Hall that a viewer had commented about tiny houses near the tornado. “Oh really?” Hall replied to his AI robot, known as Y’all Bot.\n\nThe 31-year-old host of Ryan Hall, Y’all—one of YouTube’s most popular weather channels, with 2.8 million subscribers—went live for nearly 12 hours that day as more than 70 tornadoes swept through the central US, killing at least 28 people. Nineteen of the dead were in Kentucky. Hall, too, was under tornado warning as he streamed from his home in Kentucky.\n\nSirens went off in Somerset, but the National Weather Service lagged behind in upgrading its tornado warning, Hall told viewers. He also said that recent cuts had left the NWS office in Jackson, Kentucky, short-staffed. “We’re about to have a large tornado go through a very populated area with much less warning than what there should be as a result of that,” he said. It wasn’t until 10:57 pm that the NWS finally upgraded its tornado warning for Somerset.\n\nHall doesn’t have a meteorology degree, but he employs meteorologists like 27-year-old Andy Hill, who frequently appears on his livestream. Hill was on vacation during the deadly tornadoes but noted that Hall had correctly read the radar. “He was just looking at, essentially, patterns and radar data, which is what I've attempted to teach him over the years,” Hill said. “On May 16, I think Ryan definitely saved some lives.”\n\nA new generation of storm forecasters are going live on YouTube for hours during severe weather events, offering real-time updates to millions of subscribers through a network of storm chasers, and even using AI. Their devoted fans help shape the forecast by sending on-the-ground photos of these storms, for example lemon-sized hail, for the streamers to show live on their screens. As the Trump administration slashes federal weather forecasting staff and climate change supercharges storms, their reports are not only entertaining but also crucial and potentially life-saving.\n\nThis form of weather content is growing rapidly, but so far there are two main YouTube weather forecasters. Hall, who employs about 40 people across his media business and nonprofit, and YouTube’s second biggest weather streamer, Max Velocity, are game changers who frequently warn their millions of fans about tornadoes on the ground before the National Weather Service issues official alerts. They do this by interpreting blobs of color on the radar and hosting feeds of storm chasers going live from their vehicles. Hall’s AI bot interacts with him during storms, and it even has its own channel where it goes live 24/7."
    },
    {
        "title": "'Wall-E With a Gun': Midjourney Generates Videos of Disney Characters Amid Massive Copyright Lawsuit",
        "description": "A week after Disney and Universal filed a landmark lawsuit against Midjourney, the generative AI startup's new V1 video tool will make clips of Shrek, Deadpool, and other famous creations.",
        "url": "https://www.wired.com/story/midjourney-generates-videos-of-disney-characters-amid-massive-copyright-lawsuit/",
        "source": "Wired",
        "publishedAt": "2025-06-20T20:28:20Z",
        "full_text": "Midjourney’s new AI-generated video tool will produce animated clips featuring copyrighted characters from Disney and Universal, WIRED has found—including video of the beloved Pixar character Wall-E holding a gun.\n\nIt’s been a busy month for Midjourney. This week, the generative AI startup released its sophisticated new video tool, V1, which lets users make short animated clips from images they generate or upload. The current version of Midjourney’s AI video tool requires an image as a starting point; generating videos using text-only prompts is not supported.\n\nThe release of V1 comes on the heels of a very different kind of announcement earlier in June: Hollywood behemoths Disney and Universal filed a blockbuster lawsuit against Midjourney, alleging that it violates copyright law by generating images with the studios’ intellectual property.\n\nMidjourney did not immediately respond to requests for comment. Disney and Universal reiterated statements made by its executives about the lawsuit, including Disney’s legal head Horacio Gutierrez alleging that Midjourney’s output amounts to “piracy.”\n\nIt appears that Midjourney may have attempted to put up some video-specific guardrails for V1. In our testing, it blocked animations from prompts based on Frozen’s Elsa, Boss Baby, Goofy, and Mickey Mouse, although it would still generate images of these characters. When WIRED asked V1 to animate images of Elsa, an “AI moderator” blocked the prompt from generating videos. “Al Moderation is cautious with realistic videos, especially of people,” read the pop-up message.\n\nThese limitations, which appear to be guardrails, are incomplete. WIRED testing shows that V1 will generate animated clips of a wide variety of Universal and Disney characters, including Homer Simpson, Shrek, Minions, Deadpool, and Star Wars’ C-3PO and Darth Vader. For example, when asked for an image of Minions eating a banana, Midjourney generated four outputs with recognizable versions of the cute, yellow characters. Then, when WIRED clicked the “Animate” button on one of the outputs, Midjourney generated a follow-up video with the characters eating a banana—peel and all.\n\nAlthough Midjourney seems to have blocked some Disney- and Universal-related prompts for videos, WIRED could sometimes circumvent the potential guardrails during tests by using spelling variations or repeating the prompt. Midjourney also lets users provide a prompt to inform the animation; using that feature, WIRED was able to to generate clips of copyrighted characters behaving in adult ways, like Wall-E brandishing a firearm and Yoda smoking a joint.\n\nThe Disney and Universal lawsuit poses a major threat to Midjourney, which also faces additional legal challenges from visual artists who allege copyright infringement as well. Although it focused largely on providing examples from Midjourney’s image-generation tools, the complaint alleges that video would “only enhance Midjourney ability to distribute infringing copies, reproductions, and derivatives of Plaintiffs’ Copyrighted Works.”\n\nThe complaint includes dozens of alleged Midjourney images showing Universal and Disney characters. The set was initially produced as part of a report on Midjourney’s so-called “visual plagiarism problem” from AI critic and cognitive scientist Gary Marcus and visual artist Reid Southen.\n\n“Reid and I pointed out this problem 18 months ago, and there's been very little progress and very little change,” says Marcus. “We still have the same situation of unlicensed materials being used, and guardrails that work a little bit but not very well. For all the talk about exponential progress in AI, what we're getting is better graphics, not a fundamental-principle solution to this problem.”"
    },
    {
        "title": "Google Wants to Get Better at Spotting Wildfires From Space",
        "description": "A partnership with the nonprofit Earth Fire Alliance and satellite manufacturer Muon Space is giving Google a better shot at tracking wildfires—and using AI to process all the data being collected.",
        "url": "https://www.wired.com/story/google-earth-fire-alliance-spotting-wildfires-from-space/",
        "source": "Wired",
        "publishedAt": "2025-06-24T09:30:00Z",
        "full_text": "Google wants to launch a battalion of satellites into orbit around the Earth to monitor fires on the ground in real time, then collect all that photographic data and use AI to better identify fires in their critical early stages.\n\nFireSat is a partnership between Google, the nonprofit Earth Fire Alliance, and the satellite builder Muon Space. The collaborative effort was announced in 2024 with the goal of launching satellites specifically designed to spot wildfires. The first satellite of the proposed 50-plus strong constellation launched in March 2025.\n\nThe group hopes to get the full constellation up there by 2029. Then, the satellites will be able to orbit the Earth, snapping images of every fire-prone place on the globe. The photos would be captured about 20 minutes apart, enough to catch a small fire before it grows too big, or to observe the progress of an active blaze.The information about a fire’s location could then be beamed to data analysts and machine intelligence systems on the ground more quickly than ever.\n\n“We want to make sure that we can learn fast to be able to detect and track fires,” Brian Collins, the executive director of the Earth Fire Alliance, says. “We want to transform the way the world and the United States looks at fire.”\n\nThis group’s effort isn’t the only mission to put fire-tracking satellites into orbit right now. The Canadian WildfireSat program is a government-funded effort to launch its own fire-specific satellites dedicated to monitoring blazes across the country. In the 2025 fire season so far, nearly 9 million acres have already burned in the fires active in Canada. But the launch of Canada’s fire satellites is still a ways off, slated for launch in 2029. Google wants to get into space more quickly—and use its AI chops to speed up the process of figuring out when fires start.\n\nSatellites already in orbit have been snapping pics of wildfires for years. Google has incorporated data collected by NOAA weather satellites to show wildfire boundaries and evacuation zones in Maps. But detecting fires from space—especially small ones or fires that are just starting—can be tricky. Satellites currently in orbit tend to detect heat with microbolometer sensors, thermal imaging chips that, unlike other thermal cameras, don’t require cooling. The problem with that, says Christopher Van Arsdale, a researcher at Google, is that microbolometer images can have a narrow field of view and come back with grainier, lower-resolution images. That can make detecting fires in their earlier stages hard, because lots of heat signatures on the ground—hot roofs or even light reflected off water surfaces—can look very similar to wildfires to a thermal camera.\n\n“If you look at a noisy picture, everything kind of looks like a tiny fire,” Van Arsdale says. “So you have to really know what you're looking at for that to be useful. You need these very high-fidelity pictures in order to actually do a good job with detection.”"
    },
    {
        "title": "AI Videos of Black Women Depicted as Primates Are Going Viral",
        "description": "Some Instagram creators are using Google’s Veo 3 and racking up millions of views on AI videos of “bigfoot baddies.” They’ll teach you how to make them for $15.",
        "url": "https://www.wired.com/story/ai-videos-black-women-depicted-as-primates-viral/",
        "source": "Wired",
        "publishedAt": "2025-07-01T17:31:30Z",
        "full_text": "An AI-generated “bigfoot baddie,” with acrylic nails and a pink wig, speaks directly to her imaginary audience using an iPhone. “We might have to go on the run,” she says. “I’m wanted for a false report on my baby daddy.” This AI video, generated by Google’s Veo 3, has racked up over a million views on Instagram. It’s just one of many viral posts on Instagram and TikTok viewed by WIRED that depict Black women as primates and perpetuate racist tropes using AI video tools.\n\nGoogle’s Veo 3 was a hit with online audiences when it dropped at the company’s developer conference in May. Surreal generations of Biblical characters and cryptids, like bigfoot, doing influencer-style vlogging quickly spread across social media. AI-generated bigfoot vlogs were even used by Google as a selling point in ads promoting the new feature.\n\nWith “bigfoot baddies,” online creators are taking what was a fairly innocuous trend on social media and repurposing it to dehumanize Black women. “There's a historical precedent behind why this is offensive. In the early days of slavery, Black people were overexaggerated in illustrations to emphasize primal characteristics,” says Nicol Turner Lee, director of the Center for Technology Innovation at the Brookings Institution.\n\n“It's both disgusting and disturbing that these racial tropes and images are readily available to be designed and distributed on online platforms,” says Turner Lee.\n\nOne of the most popular Instagram accounts posting these generated clips has five videos with over a million views, less than a month after the account’s first post. The AI videos feature the animal-woman hybrids speaking African American Vernacular English in a caricatured manner, with the characters often shown wearing a bonnet and threatening to fight people. In one clip, the AI generation, using a country accent, implies she pulled out a bottle of Hennessy liquor that was stored in her genitals.\n\nVeo 3 can create everything seen in videos like this, the scenery to the spoken audio to the characters themselves, from a single prompt. The bio of the popular Instagram account includes a link to a $15 online course where you can learn how to create similar videos. In videos with titles like “Veo 3 does the heavy lifting,” three teachers use voiceover to step students through the process of prompting the AI video tool for bigfoot clips and creating consistent characters. The email address listed as the administrator of the online course bounced back messages when WIRED attempted to contact the creators.\n\nA spokesperson for Meta, which owns Instagram, declined to comment on the record. Google and TikTok both acknowledged WIRED’s request for comment, but did not provide a statement prior to publication.\n\nOur social media analysis found copycat accounts on Instagram and TikTok reposting the “bigfoot baddie” clips or generating similar videos. A repost of one video on Instagram has 1 million views on an AI-focused meme page. A different Instagram account has another “bigfoot baddie” video with almost 3 million views. It’s not just on Instagram; an account on TikTok dedicated to similar AI-generated content currently has over 1 million likes. These accounts did not immediately respond to a request for comment."
    },
    {
        "title": "The Rise of AI Is Making Life Even Harder for Real People in Gaza",
        "description": "Gizmodo spoke with desperate Palestinians who were accused of being AI creations.",
        "url": "https://gizmodo.com/the-rise-of-ai-is-making-life-even-harder-for-real-people-in-gaza-2000607395",
        "source": "Gizmodo.com",
        "publishedAt": "2025-07-05T10:00:32Z",
        "full_text": ""
    },
    {
        "title": "Sam Altman’s Lies About ChatGPT Are Growing Bolder",
        "description": "Would you believe OpenAI if it told you AI is fine for the planet, actually?",
        "url": "https://gizmodo.com/sam-altmans-lies-about-chatgpt-are-growing-bolder-2000614431",
        "source": "Gizmodo.com",
        "publishedAt": "2025-06-11T15:40:15Z",
        "full_text": ""
    },
    {
        "title": "Zombie AI Siri May Shuffle Onto iPhones Two Years After It Was First Announced",
        "description": "At that point, will we even care anymore about AI Siri?",
        "url": "https://gizmodo.com/zombie-ai-siri-may-shuffle-onto-iphones-two-years-after-it-was-first-announced-2000615520",
        "source": "Gizmodo.com",
        "publishedAt": "2025-06-13T14:30:35Z",
        "full_text": ""
    }
]